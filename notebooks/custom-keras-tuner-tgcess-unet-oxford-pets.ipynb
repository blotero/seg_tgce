{"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":183561927,"sourceType":"kernelVersion"}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":5295.356001,"end_time":"2024-06-14T23:49:21.644977","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-06-14T22:21:06.288976","version":"2.5.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"005606ee5a494faeaf1bf8e0270db84a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"006ae15aaf3143648cdf15c646650fae":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_acde00af91a24ff5b4b0abc1edf43430","placeholder":"​","style":"IPY_MODEL_e59da96fd42b4a20b6db25749b73dc0d","value":" 2401/3669 [00:01&lt;00:00, 2400.80 examples/s]"}},"0159f244200848f6945e7dfacaaa1abe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b4c2712b60304a4e8a799e7e003f6f8e","IPY_MODEL_532336458080415cbcbe5393aaf3f4f1","IPY_MODEL_1bda1de335764e94a0f0e42ebb7f6152"],"layout":"IPY_MODEL_77171776686f4194b7f84c19cec7978d"}},"026a366582bf42f8922bcaf9d30a99bb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_502da723a63b45d2b19039f5d5054758","placeholder":"​","style":"IPY_MODEL_09587696dcab4455ad055af851ad49d3","value":"Extraction completed...: 100%"}},"09587696dcab4455ad055af851ad49d3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0a1425bfe4be4562bc3df43d6cfded6d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0bd46eea31e04dcba872dcddc7920808":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0f72964af9cb4c6ea53afd0260720ad3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"11f665dcd5b64e05aceee079c8dbd026":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_43ccee349b2747a0be87d5d3248737a0","placeholder":"​","style":"IPY_MODEL_80947d765a5044b2a6927e92677e3569","value":" 18473/18473 [01:00&lt;00:00, 407.24 file/s]"}},"12d94750ffaf40cc8ada82d4333a934a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"155bec38b5774dabb1357e5bcc0f9970":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"1bda1de335764e94a0f0e42ebb7f6152":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3d19a4862c614f9c99c3357094e6ce20","placeholder":"​","style":"IPY_MODEL_b3ae542f45bb43e99aadf046f46eef3d","value":" 2/2 [00:04&lt;00:00,  2.03s/ splits]"}},"1e92482d195d497292dce5842c3fc0df":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_7549a9593dfc4724a5bfb9d71f304b1b","max":3680,"min":0,"orientation":"horizontal","style":"IPY_MODEL_be259e16a5e84b3c8877a3389fcb5084","value":3680}},"2647dc8f567e4dbc889335ac4aa89cc0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"271f273764e0489dba9b12e88ffef6df":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b2688aa677649fe91b961c6c156253d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"314ea283a51a4806bce462043b6d02f0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_410ffd7887b840a88f6d2dbf88d86829","placeholder":"​","style":"IPY_MODEL_fd26d482d0e142e393a7862fb4f6a2f9","value":" 3504/3669 [00:00&lt;00:00, 8937.57 examples/s]"}},"32ff97353bc94f0683a8cd59dadc326b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ddba3ee548e84591add61139cf4436d0","placeholder":"​","style":"IPY_MODEL_5baf563a35134c279cf058fb4f3ae36d","value":" 2177/3680 [00:01&lt;00:00, 2176.22 examples/s]"}},"367954210a384b9fa0e663268a973e44":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3839b7535fa54db48b6f782cbbbde60d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eef64ca4183e432e84723664c5b5b801","placeholder":"​","style":"IPY_MODEL_2b2688aa677649fe91b961c6c156253d","value":" 2/2 [01:00&lt;00:00, 24.58s/ url]"}},"3d19a4862c614f9c99c3357094e6ce20":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3dd19a09389d48e796980ea459462daa":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"40f8dc9830cd41a381294f36eeefb185":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_417050a8affd426eab336aa2cafee330","IPY_MODEL_b117e14474d54b3b89834543ab4a4727","IPY_MODEL_3839b7535fa54db48b6f782cbbbde60d"],"layout":"IPY_MODEL_84b547c8372c41618abe50d88cfaabc2"}},"410ffd7887b840a88f6d2dbf88d86829":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"417050a8affd426eab336aa2cafee330":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8560465023024714ac3e3adaf1adcef4","placeholder":"​","style":"IPY_MODEL_4b1daed1cc5943c785c918de008b7bd9","value":"Dl Completed...: 100%"}},"43ccee349b2747a0be87d5d3248737a0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ac60e4cc1f5495ea711eda916a6d616":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0a1425bfe4be4562bc3df43d6cfded6d","placeholder":"​","style":"IPY_MODEL_80bd8aee73214d429eb039b4e90dd793","value":"Shuffling /root/tensorflow_datasets/oxford_iiit_pet/3.2.0.incompleteQ40T4C/oxford_iiit_pet-train.tfrecord*...:  81%"}},"4b1daed1cc5943c785c918de008b7bd9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4cb400ec99f4475a9aa90275377a5265":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_cb0fd86a7337486fa68ee270b4e8c446","max":3669,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7f248c5edfd24242aee15bc14f756eff","value":3669}},"4d03ba4a13f0464080a10f3b6eef99d7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4d4506c10d024b3ab1cb98403e340eef":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_271f273764e0489dba9b12e88ffef6df","max":3680,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c1a8ea16efed4556b46186ec9fd6b9d4","value":3680}},"502da723a63b45d2b19039f5d5054758":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"51434e7b15b04e69821d084a0f37d41e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eda3c04c93f44780bb7cb01c3af371d0","placeholder":"​","style":"IPY_MODEL_0bd46eea31e04dcba872dcddc7920808","value":" 2977/3680 [00:00&lt;00:00, 9851.02 examples/s]"}},"523c897f21dc4db588171682b3d12767":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"532336458080415cbcbe5393aaf3f4f1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_8feb82cf26d743bd91c9e65db16f5b38","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_813ff9371f644e59b19451f36ea7b831","value":2}},"5609ded263824dd99e3e2ff5b1db27ac":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"59442a843bce4a9daf22bfc08d77f652":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_026a366582bf42f8922bcaf9d30a99bb","IPY_MODEL_bc1a5c556793481eaf77cce089e24e39","IPY_MODEL_11f665dcd5b64e05aceee079c8dbd026"],"layout":"IPY_MODEL_5609ded263824dd99e3e2ff5b1db27ac"}},"5baf563a35134c279cf058fb4f3ae36d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6632b0052b4b430bab478c7e08c86646":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"6736a0ea3f1144c5ac0f31668159a6dc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_905ac081406c4587b578de7f44279963","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_992e1e90ca24475a8a3c981757e2d746","value":1}},"6f0cce1c5b7a42159e77f84974d9c71a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"71ca5c556b89433386c286823ea6d2ba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7549a9593dfc4724a5bfb9d71f304b1b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75bc54ba000941908926c94a6e8cdc1f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_005606ee5a494faeaf1bf8e0270db84a","placeholder":"​","style":"IPY_MODEL_4d03ba4a13f0464080a10f3b6eef99d7","value":"Generating test examples...:  65%"}},"77171776686f4194b7f84c19cec7978d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"7db39eeb1ff04228b84445623ae48160":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c261ff47f40a4e0ab43234791694096e","placeholder":"​","style":"IPY_MODEL_12d94750ffaf40cc8ada82d4333a934a","value":"Generating train examples...:  59%"}},"7df1eaac899b4587ae4e1ebcc9c233cb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f248c5edfd24242aee15bc14f756eff":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"80947d765a5044b2a6927e92677e3569":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"80bd8aee73214d429eb039b4e90dd793":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"813ff9371f644e59b19451f36ea7b831":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"84b547c8372c41618abe50d88cfaabc2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8560465023024714ac3e3adaf1adcef4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8dcd8137666d4909b45d8b2bca7b8860":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f2fd12ee0e314d99a04aba7041f173b9","IPY_MODEL_6736a0ea3f1144c5ac0f31668159a6dc","IPY_MODEL_b92f4640810a40ff821d709a52070f66"],"layout":"IPY_MODEL_367954210a384b9fa0e663268a973e44"}},"8feb82cf26d743bd91c9e65db16f5b38":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"905ac081406c4587b578de7f44279963":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"992e1e90ca24475a8a3c981757e2d746":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9b93289fae1b458f86d95b87f0c82e16":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_7df1eaac899b4587ae4e1ebcc9c233cb","max":3669,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b6f6b369d39c4f82b8c89d7514114a80","value":3669}},"9c0270044a6049e584dbfe7ac7adf2fd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a82e6e4a018e4521a2d9ae5aa66148b0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fb8906e78d2040a79d431e5910544f5c","placeholder":"​","style":"IPY_MODEL_f5985967219f4b9db62fd2d4702575f0","value":"Shuffling /root/tensorflow_datasets/oxford_iiit_pet/3.2.0.incompleteQ40T4C/oxford_iiit_pet-test.tfrecord*...:  96%"}},"acde00af91a24ff5b4b0abc1edf43430":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad8f0215ae704367b94f87ab75cf5260":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"b07e215ad3d645a986f44189665a3efb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b117e14474d54b3b89834543ab4a4727":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c9cde6a23ad04c5bbfe96906eed8c579","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6f0cce1c5b7a42159e77f84974d9c71a","value":1}},"b3ae542f45bb43e99aadf046f46eef3d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b49d966249ac479384ecb48cb9907ba5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4ac60e4cc1f5495ea711eda916a6d616","IPY_MODEL_4d4506c10d024b3ab1cb98403e340eef","IPY_MODEL_51434e7b15b04e69821d084a0f37d41e"],"layout":"IPY_MODEL_155bec38b5774dabb1357e5bcc0f9970"}},"b4c2712b60304a4e8a799e7e003f6f8e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3dd19a09389d48e796980ea459462daa","placeholder":"​","style":"IPY_MODEL_dac63a82bf6d4a7d85e246c9f74014f6","value":"Generating splits...: 100%"}},"b6f6b369d39c4f82b8c89d7514114a80":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b7bd35f0329b40b39685a66ec47ee8be":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7db39eeb1ff04228b84445623ae48160","IPY_MODEL_1e92482d195d497292dce5842c3fc0df","IPY_MODEL_32ff97353bc94f0683a8cd59dadc326b"],"layout":"IPY_MODEL_523c897f21dc4db588171682b3d12767"}},"b8d5f72b658d419db088d4c631c55982":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"b92f4640810a40ff821d709a52070f66":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2647dc8f567e4dbc889335ac4aa89cc0","placeholder":"​","style":"IPY_MODEL_71ca5c556b89433386c286823ea6d2ba","value":" 773/773 [01:00&lt;00:00, 33.04 MiB/s]"}},"bb0dbf5efbde44c6ae6a4159b08b5377":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_75bc54ba000941908926c94a6e8cdc1f","IPY_MODEL_9b93289fae1b458f86d95b87f0c82e16","IPY_MODEL_006ae15aaf3143648cdf15c646650fae"],"layout":"IPY_MODEL_6632b0052b4b430bab478c7e08c86646"}},"bc1a5c556793481eaf77cce089e24e39":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b8d5f72b658d419db088d4c631c55982","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b07e215ad3d645a986f44189665a3efb","value":1}},"be259e16a5e84b3c8877a3389fcb5084":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c1a8ea16efed4556b46186ec9fd6b9d4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c261ff47f40a4e0ab43234791694096e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9cde6a23ad04c5bbfe96906eed8c579":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"cb0fd86a7337486fa68ee270b4e8c446":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dac63a82bf6d4a7d85e246c9f74014f6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ddb2baaab771402f91802870ee53c68f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a82e6e4a018e4521a2d9ae5aa66148b0","IPY_MODEL_4cb400ec99f4475a9aa90275377a5265","IPY_MODEL_314ea283a51a4806bce462043b6d02f0"],"layout":"IPY_MODEL_ad8f0215ae704367b94f87ab75cf5260"}},"ddba3ee548e84591add61139cf4436d0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e59da96fd42b4a20b6db25749b73dc0d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eda3c04c93f44780bb7cb01c3af371d0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eef64ca4183e432e84723664c5b5b801":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f2fd12ee0e314d99a04aba7041f173b9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9c0270044a6049e584dbfe7ac7adf2fd","placeholder":"​","style":"IPY_MODEL_0f72964af9cb4c6ea53afd0260720ad3","value":"Dl Size...: 100%"}},"f5985967219f4b9db62fd2d4702575f0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fb8906e78d2040a79d431e5910544f5c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fd26d482d0e142e393a7862fb4f6a2f9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}},"version_major":2,"version_minor":0}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Chained deep learning using generalized cross entropy for multiple annotators segmentation\n\n\n\n## Loss functions for segmentation in deep learning\n\n\nGiven a $k$ class multiple annotators segmentation problem with a dataset like the following'\n\n$$\\mathbf X \\in \\mathbb{R}^{W \\times H}, \\{ \\mathbf Y_r \\in \\{0,1\\}^{W \\times H \\times K} \\}_{r=1}^R; \\;\\; \\mathbf {\\hat Y} \\in [0,1]^{W\\times H \\times K} = f(\\mathbf X)$$\n\nThe segmentation mask function will map input output as follows:\n\n$$f: \\mathbb  R ^{W\\times H} \\to [0,1]^{W\\times H\\times K}$$\n\n$\\mathbf Y$ will satisfy the following condition for being a softmax-like representation:\n\n$$\\mathbf Y_r[w,h,:] \\mathbf{1} ^ \\top _ k = 1; \\;\\; w \\in W, h \\in H$$\n\nNow, let's suppose the existence of an annotators reliability map estimation $\\Lambda_r; \\; r \\in R$;\n\n\n$$\\bigg\\{ \\Lambda_r (\\mathbf X; \\theta ) \\in [0,1] ^{W\\times H} \\bigg\\}_{r=1}^R $$\n\n\nThen, our $TGCE_{SS}$:\n\n\n$$TGCE_{SS}(\\mathbf{Y}_r,f(\\mathbf X;\\theta) | \\mathbf{\\Lambda}_r (\\mathbf X;\\theta)) =\\mathbb E_{r} \\left\\{ \\mathbb E_{w,h}  \\left\\{ \\Lambda_r (\\mathbf X; \\theta) \\circ \\mathbb E_k \\bigg\\{    \\mathbf Y_r \\circ \\bigg( \\frac{\\mathbf 1 _{W\\times H \\times K} - f(\\mathbf X;\\theta) ^{\\circ q }}{q} \\bigg); k \\in K  \\bigg\\}  + \\\\ \\left(\\mathbf 1 _{W \\times H } - \\Lambda _r (\\mathbf X;\\theta)\\right) \\circ \\bigg(   \\frac{\\mathbf 1_{W\\times H} - (\\frac {1}{k} \\mathbf 1_{W\\times H})^{\\circ q}}{q} \\bigg); w \\in W, h \\in H \\right\\};r\\in R\\right\\} $$\n\n\nWhere $q \\in (0,1)$\n\nTotal Loss for a given batch holding $N$ samples:\n\n$$\\mathscr{L}\\left(\\mathbf{Y}_r[n],f(\\mathbf X[n];\\theta) | \\mathbf{\\Lambda}_r (\\mathbf X[n];\\theta)\\right)  = \\frac{1}{N} \\sum_{n}^NTGCE_{SS}(\\mathbf{Y}_r[n],f(\\mathbf X[n];\\theta) | \\mathbf{\\Lambda}_r (\\mathbf X[n];\\theta))$$","metadata":{"id":"P5QjkmUx8m7x","papermill":{"duration":0.052687,"end_time":"2024-06-14T22:21:09.122133","exception":false,"start_time":"2024-06-14T22:21:09.069446","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Modification of Keras Tuner bayesian optimization with a Matern kernel with Nu ($\\nu$) value 0.5","metadata":{}},{"cell_type":"code","source":"import os\nnew_content=\"\"\"\n# Copyright 2019 The KerasTuner Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\n\ntry:\n    import sklearn\n    import sklearn.exceptions\n    import sklearn.gaussian_process\nexcept ImportError:  # pragma: no cover\n    sklearn = None  # pragma: no cover\n\ntry:\n    import scipy\n    import scipy.optimize\nexcept ImportError:  # pragma: no cover\n    scipy = None  # pragma: no cover\n\nfrom keras_tuner.src.api_export import keras_tuner_export\nfrom keras_tuner.src.engine import hyperparameters as hp_module\nfrom keras_tuner.src.engine import oracle as oracle_module\nfrom keras_tuner.src.engine import trial as trial_module\nfrom keras_tuner.src.engine import tuner as tuner_module\n\n\n@keras_tuner_export(\"keras_tuner.oracles.BayesianOptimizationOracle\")\nclass BayesianOptimizationOracle(oracle_module.Oracle):\n    \\\"\\\"\\\"Bayesian optimization oracle.\n\n    It uses Bayesian optimization with a underlying Gaussian process model.\n    The acquisition function used is upper confidence bound (UCB), which can\n    be found [here](\n    https://www.cse.wustl.edu/~garnett/cse515t/spring_2015/files/lecture_notes/12.pdf).\n\n    Args:\n        objective: A string, `keras_tuner.Objective` instance, or a list of\n            `keras_tuner.Objective`s and strings. If a string, the direction of\n            the optimization (min or max) will be inferred. If a list of\n            `keras_tuner.Objective`, we will minimize the sum of all the\n            objectives to minimize subtracting the sum of all the objectives to\n            maximize. The `objective` argument is optional when\n            `Tuner.run_trial()` or `HyperModel.fit()` returns a single float as\n            the objective to minimize.\n        max_trials: Integer, the total number of trials (model configurations)\n            to test at most. Note that the oracle may interrupt the search\n            before `max_trial` models have been tested if the search space has\n            been exhausted. Defaults to 10.\n        num_initial_points: Optional number of randomly generated samples as\n            initial training data for Bayesian optimization. If left\n            unspecified, a value of 3 times the dimensionality of the\n            hyperparameter space is used.\n        alpha: Float, the value added to the diagonal of the kernel matrix\n            during fitting. It represents the expected amount of noise in the\n            observed performances in Bayesian optimization. Defaults to 1e-4.\n        beta: Float, the balancing factor of exploration and exploitation. The\n            larger it is, the more explorative it is. Defaults to 2.6.\n        seed: Optional integer, the random seed.\n        hyperparameters: Optional `HyperParameters` instance. Can be used to\n            override (or register in advance) hyperparameters in the search\n            space.\n        tune_new_entries: Boolean, whether hyperparameter entries that are\n            requested by the hypermodel but that were not specified in\n            `hyperparameters` should be added to the search space, or not. If\n            not, then the default value for these parameters will be used.\n            Defaults to True.\n        allow_new_entries: Boolean, whether the hypermodel is allowed to\n            request hyperparameter entries not listed in `hyperparameters`.\n            Defaults to True.\n        max_retries_per_trial: Integer. Defaults to 0. The maximum number of\n            times to retry a `Trial` if the trial crashed or the results are\n            invalid.\n        max_consecutive_failed_trials: Integer. Defaults to 3. The maximum\n            number of consecutive failed `Trial`s. When this number is reached,\n            the search will be stopped. A `Trial` is marked as failed when none\n            of the retries succeeded.\n    \\\"\\\"\\\"\n\n    def __init__(\n        self,\n        objective=None,\n        max_trials=10,\n        num_initial_points=None,\n        alpha=1e-4,\n        beta=2.6,\n        seed=None,\n        hyperparameters=None,\n        allow_new_entries=True,\n        tune_new_entries=True,\n        max_retries_per_trial=0,\n        max_consecutive_failed_trials=3,\n    ):\n        if scipy is None:\n            raise ImportError(\n                \"Please install scipy before using the `BayesianOptimization` \"\n                \"with `pip install keras-tuner[bayesian]`.\"\n            )\n\n        if sklearn is None:\n            raise ImportError(\n                \"Please install scikit-learn (sklearn) before using the \"\n                \"`BayesianOptimization` with \"\n                \"`pip install keras-tuner[bayesian]`.\"\n            )\n        super().__init__(\n            objective=objective,\n            max_trials=max_trials,\n            hyperparameters=hyperparameters,\n            tune_new_entries=tune_new_entries,\n            allow_new_entries=allow_new_entries,\n            seed=seed,\n            max_retries_per_trial=max_retries_per_trial,\n            max_consecutive_failed_trials=max_consecutive_failed_trials,\n        )\n        self.num_initial_points = num_initial_points\n        self.alpha = alpha\n        self.beta = beta\n        self._random_state = np.random.RandomState(self.seed)\n        self.gpr = self._make_gpr()\n\n    def _make_gpr(self):\n        return sklearn.gaussian_process.GaussianProcessRegressor(\n            kernel=sklearn.gaussian_process.kernels.Matern(nu=0.5),\n            n_restarts_optimizer=20,\n            normalize_y=True,\n            alpha=self.alpha,\n            random_state=self.seed,\n        )\n\n    def populate_space(self, trial_id):\n        \\\"\\\"\\\"Fill the hyperparameter space with values.\n\n        Args:\n            trial_id: A string, the ID for this Trial.\n\n        Returns:\n            A dictionary with keys \"values\" and \"status\", where \"values\" is\n            a mapping of parameter names to suggested values, and \"status\"\n            should be one of \"RUNNING\" (the trial can start normally), \"IDLE\"\n            (the oracle is waiting on something and cannot create a trial), or\n            \"STOPPED\" (the oracle has finished searching and no new trial should\n            be created).\n        \\\"\\\"\\\"\n        # Generate enough samples before training Gaussian process.\n        completed_trials = [\n            t for t in self.trials.values() if t.status == \"COMPLETED\"\n        ]\n\n        # Use 3 times the dimensionality of the space as the default number of\n        # random points.\n        dimensions = len(self.hyperparameters.space)\n        num_initial_points = self.num_initial_points or max(3 * dimensions, 3)\n        if len(completed_trials) < num_initial_points:\n            return self._random_populate_space()\n\n        # Fit a GPR to the completed trials and return the predicted optimum\n        # values.\n        x, y = self._vectorize_trials()\n\n        # Ensure no nan, inf in x, y. GPR cannot process nan or inf.\n        x = np.nan_to_num(x, posinf=0, neginf=0)\n        y = np.nan_to_num(y, posinf=0, neginf=0)\n\n        self.gpr.fit(x, y)\n\n        def _upper_confidence_bound(x):\n            x = x.reshape(1, -1)\n            mu, sigma = self.gpr.predict(x, return_std=True)\n            return mu - self.beta * sigma\n\n        optimal_val = float(\"inf\")\n        optimal_x = None\n        num_restarts = 50\n        bounds = self._get_hp_bounds()\n        x_seeds = self._random_state.uniform(\n            bounds[:, 0], bounds[:, 1], size=(num_restarts, bounds.shape[0])\n        )\n        for x_try in x_seeds:\n            # Sign of score is flipped when maximizing.\n            result = scipy.optimize.minimize(\n                _upper_confidence_bound,\n                x0=x_try,\n                bounds=bounds,\n                method=\"L-BFGS-B\",\n            )\n            result_fun = (\n                result.fun if np.isscalar(result.fun) else result.fun[0]\n            )\n            if result_fun < optimal_val:\n                optimal_val = result_fun\n                optimal_x = result.x\n\n        values = self._vector_to_values(optimal_x)\n        return {\"status\": trial_module.TrialStatus.RUNNING, \"values\": values}\n    \n    def _gpr_trained(self):\n        if self.gpr is None:\n            print(\"There is no GPR model trained yet.\")\n            return None\n        else:\n            return self.gpr\n\n    def _random_populate_space(self):\n        values = self._random_values()\n        if values is None:\n            return {\"status\": trial_module.TrialStatus.STOPPED, \"values\": None}\n        return {\"status\": trial_module.TrialStatus.RUNNING, \"values\": values}\n\n    def get_state(self):\n        state = super().get_state()\n        state.update(\n            {\n                \"num_initial_points\": self.num_initial_points,\n                \"alpha\": self.alpha,\n                \"beta\": self.beta,\n            }\n        )\n        return state\n\n    def set_state(self, state):\n        super().set_state(state)\n        self.num_initial_points = state[\"num_initial_points\"]\n        self.alpha = state[\"alpha\"]\n        self.beta = state[\"beta\"]\n        self.gpr = self._make_gpr()\n\n    def _vectorize_trials(self):\n        x = []\n        y = []\n        ongoing_trials = set(self.ongoing_trials.values())\n        for trial in self.trials.values():\n            # Create a vector representation of each Trial's hyperparameters.\n            trial_hps = trial.hyperparameters\n            vector = []\n            for hp in self._nonfixed_space():\n                # For hyperparameters not present in the trial (either added\n                # after the trial or inactive in the trial), set to default\n                # value.\n                if (\n                    trial_hps.is_active(hp)  # inactive\n                    and hp.name in trial_hps.values  # added after the trial\n                ):\n                    trial_value = trial_hps.values[hp.name]\n                else:\n                    trial_value = hp.default\n\n                # Embed an HP value into the continuous space [0, 1].\n                prob = hp.value_to_prob(trial_value)\n                vector.append(prob)\n\n            if trial in ongoing_trials:\n                # \"Hallucinate\" the results of ongoing trials. This ensures that\n                # repeat trials are not selected when running distributed.\n                x_h = np.array(vector).reshape((1, -1))\n                y_h_mean, y_h_std = self.gpr.predict(x_h, return_std=True)\n                # Give a pessimistic estimate of the ongoing trial.\n                y_h_mean = np.array(y_h_mean).flatten()\n                score = y_h_mean[0] + y_h_std[0]\n            elif trial.status == \"COMPLETED\":\n                score = trial.score\n                # Always frame the optimization as a minimization for\n                # scipy.minimize.\n                if self.objective.direction == \"max\":\n                    score = -1 * score\n            elif trial.status in [\"FAILED\", \"INVALID\"]:\n                # Skip the failed and invalid trials.\n                continue\n\n            x.append(vector)\n            y.append(score)\n\n        x = np.array(x)\n        y = np.array(y)\n        return x, y\n\n    def _vector_to_values(self, vector):\n        hps = hp_module.HyperParameters()\n        vector_index = 0\n        for hp in self.hyperparameters.space:\n            hps.merge([hp])\n            if isinstance(hp, hp_module.Fixed):\n                value = hp.value\n            else:\n                prob = vector[vector_index]\n                vector_index += 1\n                value = hp.prob_to_value(prob)\n\n            if hps.is_active(hp):\n                hps.values[hp.name] = value\n        return hps.values\n\n    def _nonfixed_space(self):\n        return [\n            hp\n            for hp in self.hyperparameters.space\n            if not isinstance(hp, hp_module.Fixed)\n        ]\n\n    def _get_hp_bounds(self):\n        bounds = [[0, 1] for _ in self._nonfixed_space()]\n        return np.array(bounds)\n\n\n@keras_tuner_export(\n    [\n        \"keras_tuner.BayesianOptimization\",\n        \"keras_tuner.tuners.BayesianOptimization\",\n    ]\n)\nclass BayesianOptimization(tuner_module.Tuner):\n    \\\"\\\"\\\"BayesianOptimization tuning with Gaussian process.\n\n    Args:\n        hypermodel: Instance of `HyperModel` class (or callable that takes\n            hyperparameters and returns a `Model` instance). It is optional\n            when `Tuner.run_trial()` is overridden and does not use\n            `self.hypermodel`.\n        objective: A string, `keras_tuner.Objective` instance, or a list of\n            `keras_tuner.Objective`s and strings. If a string, the direction of\n            the optimization (min or max) will be inferred. If a list of\n            `keras_tuner.Objective`, we will minimize the sum of all the\n            objectives to minimize subtracting the sum of all the objectives to\n            maximize. The `objective` argument is optional when\n            `Tuner.run_trial()` or `HyperModel.fit()` returns a single float as\n            the objective to minimize.\n        max_trials: Integer, the total number of trials (model configurations)\n            to test at most. Note that the oracle may interrupt the search\n            before `max_trial` models have been tested if the search space has\n            been exhausted. Defaults to 10.\n        num_initial_points: Optional number of randomly generated samples as\n            initial training data for Bayesian optimization. If left\n            unspecified, a value of 3 times the dimensionality of the\n            hyperparameter space is used.\n        alpha: Float, the value added to the diagonal of the kernel matrix\n            during fitting. It represents the expected amount of noise in the\n            observed performances in Bayesian optimization. Defaults to 1e-4.\n        beta: Float, the balancing factor of exploration and exploitation. The\n            larger it is, the more explorative it is. Defaults to 2.6.\n        seed: Optional integer, the random seed.\n        hyperparameters: Optional `HyperParameters` instance. Can be used to\n            override (or register in advance) hyperparameters in the search\n            space.\n        tune_new_entries: Boolean, whether hyperparameter entries that are\n            requested by the hypermodel but that were not specified in\n            `hyperparameters` should be added to the search space, or not. If\n            not, then the default value for these parameters will be used.\n            Defaults to True.\n        allow_new_entries: Boolean, whether the hypermodel is allowed to\n            request hyperparameter entries not listed in `hyperparameters`.\n            Defaults to True.\n        max_retries_per_trial: Integer. Defaults to 0. The maximum number of\n            times to retry a `Trial` if the trial crashed or the results are\n            invalid.\n        max_consecutive_failed_trials: Integer. Defaults to 3. The maximum\n            number of consecutive failed `Trial`s. When this number is reached,\n            the search will be stopped. A `Trial` is marked as failed when none\n            of the retries succeeded.\n        **kwargs: Keyword arguments relevant to all `Tuner` subclasses. Please\n            see the docstring for `Tuner`.\n    \\\"\\\"\\\"\n\n    def __init__(\n        self,\n        hypermodel=None,\n        objective=None,\n        max_trials=10,\n        num_initial_points=None,\n        alpha=1e-4,\n        beta=2.6,\n        seed=None,\n        hyperparameters=None,\n        tune_new_entries=True,\n        allow_new_entries=True,\n        max_retries_per_trial=0,\n        max_consecutive_failed_trials=3,\n        **kwargs\n    ):\n        oracle = BayesianOptimizationOracle(\n            objective=objective,\n            max_trials=max_trials,\n            num_initial_points=num_initial_points,\n            alpha=alpha,\n            beta=beta,\n            seed=seed,\n            hyperparameters=hyperparameters,\n            tune_new_entries=tune_new_entries,\n            allow_new_entries=allow_new_entries,\n            max_retries_per_trial=max_retries_per_trial,\n            max_consecutive_failed_trials=max_consecutive_failed_trials,\n        )\n        super().__init__(oracle=oracle, hypermodel=hypermodel, **kwargs)\n\"\"\"\n\nfile_path = \"/opt/conda/lib/python3.10/site-packages/keras_tuner/src/tuners/bayesian.py\"\n\nwith open(file_path, 'w') as file:\n    file.write(new_content)","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Download and importing of libraries","metadata":{"id":"XxDpkCxi0jLy","papermill":{"duration":0.049804,"end_time":"2024-06-14T22:21:09.222219","exception":false,"start_time":"2024-06-14T22:21:09.172415","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%capture\n!pip install --upgrade keras==2.15.0\n!pip install -U git+https://github.com/UN-GCPDS/python-gcpds.image_segmentation.git >> /tmp/null","metadata":{"id":"MwRAu5cpNQMr","papermill":{"duration":32.608581,"end_time":"2024-06-14T22:21:41.877358","exception":false,"start_time":"2024-06-14T22:21:09.268777","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# General Libraries\nimport time\nimport shutil\nimport random\nimport warnings\nimport pandas as pd\nimport seaborn as sns\nfrom enum import auto, Enum\nfrom functools import partial\nfrom datetime import datetime\n\n# Image Processing Libraries\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\nfrom matplotlib.cm import coolwarm\n\n# Deep Learning Libraries\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras_tuner import Objective\nfrom keras_tuner import HyperModel\nimport tensorflow.keras.backend as K\nfrom keras.layers import Layer, Activation\nfrom keras_tuner import BayesianOptimization\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import Callback\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.utils import get_custom_objects\nfrom keras_tuner.engine.hyperparameters import HyperParameters\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Machine Learning Libraries - Sklearn\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel as C, DotProduct, ExpSineSquared, Matern\n\n# Deep Learning Libraries - GCPDS\nfrom gcpds.image_segmentation.datasets.segmentation import OxfordIiitPet\n\n# Deep Learning Libraries - TensorFlow specific\nfrom tensorflow.keras.losses import Loss\nfrom tensorflow.keras.metrics import Metric\nfrom tensorflow.keras import Model, layers, regularizers\n\n# Other Libraries\nimport gc\nimport json\nimport gdown\nimport itertools\nfrom PIL import ImageFont\nfrom dataclasses import dataclass\nfrom matplotlib.style import available\nfrom tensorflow.python.framework.ops import EagerTensor\n\nwarnings.filterwarnings(\"ignore\") # Disable warnings","metadata":{"id":"MwRAu5cpNQMr","papermill":{"duration":15.097782,"end_time":"2024-06-14T22:21:57.022679","exception":false,"start_time":"2024-06-14T22:21:41.924897","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Download OxfordPet dataset and generation of synthetic annotators with the introduction of different signal-to-noise ratio values","metadata":{"id":"JpW1HZlY0jL1","papermill":{"duration":0.046107,"end_time":"2024-06-14T22:21:57.116593","exception":false,"start_time":"2024-06-14T22:21:57.070486","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Download OxfordPet dataset\n\ndataset = OxfordIiitPet()\ntrain_dataset, val_dataset, test_dataset = dataset()","metadata":{"id":"DkiXYUlq0jL1","outputId":"1d24cdd3-24c9-4bb1-d5a7-3381b15a3beb","papermill":{"duration":66.97641,"end_time":"2024-06-14T22:23:04.138756","exception":false,"start_time":"2024-06-14T22:21:57.162346","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Original Masks","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 128\nTARGET_SHAPE = 128, 128\n\ndef fussion_mask(mask: EagerTensor) -> EagerTensor:\n    \"\"\"Fuses the object and border masks into a single mask.\n\n    This function takes a mask tensor containing separate channels for the object, \n    background, and border, and fuses the object and border channels into a single \n    mask channel.\n\n    Parameters:\n        mask (EagerTensor): A tensor representing the segmentation mask with \n            object, background, and border channels.\n\n    Returns:\n        EagerTensor: A tensor representing the fused mask containing the sum \n            of the object and border channels.\n\n    \"\"\"\n    obj, bg, border = tf.unstack(mask, axis=2)\n    orig_shape = mask.shape\n    new_shape = list(orig_shape)\n    new_shape[-1] = 1\n    return tf.reshape(tf.stack([obj + border]), new_shape)\n\n\ndef map_dataset(dataset, target_shape, batch_size):\n    \"\"\"Preprocesses and batches a dataset for training or evaluation.\n\n    This function applies a series of transformations to each sample in the dataset \n    to prepare it for training or evaluation. It resizes images and masks to the \n    specified target shape, fuses mask channels if needed, and batches the dataset.\n\n    Args:\n        dataset (tf.data.Dataset): The input dataset containing images, masks, labels, and IDs.\n        target_shape (tuple): A tuple specifying the desired shape of images and masks.\n        batch_size (int): The batch size to use for training or evaluation.\n\n    Returns:\n        A preprocessed and batched dataset ready for training or evaluation.\n\n    \"\"\"\n    # Resize images and masks to the target shape\n    dataset_ = dataset.map(lambda img, mask, label, id_img: (img, mask),\n                           num_parallel_calls=tf.data.AUTOTUNE)\n    dataset_ = dataset_.map(lambda img, mask: (tf.image.resize(img, target_shape), \n                                                tf.image.resize(mask, target_shape)),\n                            num_parallel_calls=tf.data.AUTOTUNE)\n    \n    # Fuse mask channels if needed\n    dataset_ = dataset_.map(lambda img, mask: (img, fussion_mask(mask)),\n                            num_parallel_calls=tf.data.AUTOTUNE)\n\n    # Batch the dataset\n    dataset_ = dataset_.batch(batch_size)\n    return dataset_\n\noriginal_train = map_dataset(train_dataset, TARGET_SHAPE, BATCH_SIZE)\noriginal_val = map_dataset(val_dataset, TARGET_SHAPE, BATCH_SIZE)\noriginal_test = map_dataset(test_dataset, TARGET_SHAPE, BATCH_SIZE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for img,mask in original_train.take(1):\n    print(mask.shape)\n    fig, axes = plt.subplots(1, 2 , figsize=(10,5))\n    axes[0].imshow(img[0])\n    axes[0].set_title('Image')\n    axes[0].axis('off')\n    axes[1].imshow(mask[0][:,:,0])\n    axes[1].set_title('Original mask')\n    axes[1].axis('off')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Loading of the different parts of the dataset","metadata":{}},{"cell_type":"code","source":"# Loading of the training part of the database in a tensor manner\n\nX_train = []\noriginal_y_train = []\n\nfor img, mask in original_train.take(8):\n    X_train.append(img)\n    original_y_train.append(mask)\n\nX_train, original_y_train = tf.concat(X_train, axis=0), tf.concat(original_y_train, axis=0)\nprint(f'Tensor dimensions with training images: {X_train.shape} \\nTensor dimensions with training original masks: {original_y_train.shape}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading of the validation part of the database in a tensor manner\n\nX_val = []\noriginal_y_val = []\n\nfor img, mask in original_val.take(2):\n    X_val.append(img)\n    original_y_val.append(mask)\n\nX_val, original_y_val = tf.concat(X_val, axis=0), tf.concat(original_y_val, axis=0)\nprint(f'Tensor dimensions with validation images: {X_val.shape}\\nTensor dimensions with validation original masks: {original_y_val.shape}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading of the testing part of the database in a tensor manner\n\nX_test = []\noriginal_y_test = []\n\nfor img, mask in original_test.take(2):\n    X_test.append(img)\n    original_y_test.append(mask)\n\nX_test, original_y_test = tf.concat(X_test, axis=0), tf.concat(original_y_test, axis=0)\nprint(f'Tensor dimensions with test images: {X_test.shape}\\nTensor dimensions with test original masks: {original_y_test.shape}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Synthetics masks","metadata":{}},{"cell_type":"code","source":"# Download trained Unet network for OxfordPet segmentation task from Drive\n\nmodel_url = \"https://drive.google.com/file/d/1x39L3QNDMye1SJhKh1gf4YS-HRFLTs6G/view?usp=drive_link\"\nmodel_uri = model_url.split(\"/\")[5]\n!gdown $model_uri\n\nmodel_extension = \"keras\"\npaths = []\n\nfor file in os.listdir(\".\"):\n  if file.endswith(model_extension):\n    paths.append(file)\n\nmodel_path = paths[0]\nprint(f\"Loading {model_path}...\")\nmodel_ann  = tf.keras.models.load_model(model_path, compile = False)","metadata":{"id":"Dq2aLCBetK2c","outputId":"e7742e0c-4ba9-415d-9f00-2d3b7ec2d5fb","papermill":{"duration":5.064584,"end_time":"2024-06-14T22:23:09.255815","exception":false,"start_time":"2024-06-14T22:23:04.191231","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find last encoder convolution layer\n\ndef find_last_encoder_conv_layer(model):\n    '''\n    Finds the index of the last convolutional layer in the encoder part of the model.\n\n    Parameters:\n    model (keras.Model): The Keras model to search for the last encoder convolutional layer.\n\n    Returns:\n    int: Index of the last convolutional layer in the encoder part of the model.\n    '''\n\n    last_conv_encoder_layer = 0\n    for i,layer in enumerate(model.layers):\n        if (isinstance(layer, keras.layers.Conv2D)):\n          last_conv_encoder_layer = i\n        if (isinstance(layer, keras.layers.UpSampling2D)):\n          break\n    return last_conv_encoder_layer\n\nlast_conv_encoder_layer = find_last_encoder_conv_layer(model_ann)\nlast_conv_encoder_layer","metadata":{"id":"nuj7o-lNtUI7","outputId":"e2f3cb25-f04b-43fc-865e-8b0cb2346130","papermill":{"duration":0.062229,"end_time":"2024-06-14T22:23:09.367378","exception":false,"start_time":"2024-06-14T22:23:09.305149","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute and add noise to the target layer\n\ndef compute_snr(signal: float, noise_std: float) -> float:\n    \"\"\"Compute the Signal-to-Noise Ratio (SNR) in decibels.\n\n    The Signal-to-Noise Ratio (SNR) measures the ratio of the power of a signal to the\n    power of background noise. Higher SNR values indicate a stronger signal relative to\n    the noise.\n\n    Parameters:\n        signal (float): The signal power.\n        noise_std (float): The standard deviation of the background noise.\n\n    Returns:\n        float: The Signal-to-Noise Ratio (SNR) in decibels.\n\n    \"\"\"\n    return 10 * np.log10(np.mean(signal ** 2) / noise_std ** 2)\n\nclass SnrType(Enum):\n    \"\"\"Enumeration representing different types of Signal-to-Noise Ratio (SNR) scales.\n\n    This enumeration defines two types of SNR scales: 'log' and 'linear'. These types\n    indicate whether the SNR values are represented in logarithmic or linear scale.\n\n    Attributes:\n        log (int): Represents the logarithmic scale for SNR values.\n        linear (int): Represents the linear scale for SNR values.\n\n    \"\"\"\n    log = 0\n    linear = 1\n\ndef add_noise_to_layer_weights(model, layer, noise_snr, snr_type: SnrType = SnrType.log, verbose=0):\n    \"\"\"Adds noise to the weights of a specified layer in the model.\n\n    This function adds noise to the weights of a specified layer in the model,\n    simulating a certain signal-to-noise ratio (SNR) either in linear or logarithmic scale.\n\n    Parameters:\n        model (tf.keras.Model): The model to modify.\n        layer (int): Index of the layer whose weights will be modified.\n        noise_snr (float): Desired signal-to-noise ratio (SNR) for the added noise.\n        snr_type (SnrType): Type of SNR scale to use, either 'log' (logarithmic) or 'linear'.\n            Defaults to SnrType.log.\n        verbose (int): Verbosity mode. If greater than 0, prints information about the noise\n            and signal powers. Defaults to 0.\n\n    Returns:\n        float: The computed signal-to-noise ratio (SNR) after adding noise to the layer weights.\n\n    \"\"\"\n    layer_weights = model.layers[layer].get_weights()\n\n    sig_power = np.mean(layer_weights[0] ** 2)\n\n    if snr_type == SnrType.log:\n        noise_power = sig_power / (10 ** (noise_snr / 10))\n    elif snr_type == SnrType.linear:\n        noise_power = sig_power / noise_snr\n\n    noise_std = noise_power ** (1 / 2)\n\n    snr = compute_snr(layer_weights[0], noise_std)\n\n    if verbose > 0:\n        print(f\"Adding noise for SNR: {noise_snr}\\n\\n\")\n        print(f\"Signal power: {sig_power}\")\n        print(f\"Noise power: {noise_power}\\n\\n\")\n\n    for i in range(layer_weights[0].shape[0]):\n        for j in range(layer_weights[0].shape[1]):\n            layer_weights[0][i][j] += np.random.randn(128, 128) * noise_std\n\n    model.layers[last_conv_encoder_layer].set_weights(layer_weights)\n    return snr","metadata":{"id":"_MILTfMrtWeg","papermill":{"duration":0.067872,"end_time":"2024-06-14T22:23:09.486854","exception":false,"start_time":"2024-06-14T22:23:09.418982","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the signal-to-noise ratio values for each synthetic annotator\nvalues_to_test = [20,0,-15]\n\n# Creation of the different models and their perturbations starting from the base model\ndef produce_disturbed_models(values_to_test, base_model_path):\n    \"\"\"Produces a list of disturbed models by adding noise to layer weights.\n\n    This function loads a base model from the specified path and creates disturbed\n    versions of it by adding noise to the weights of a specified layer. The noise\n    level is controlled by the values provided in the `values_to_test` list.\n\n    Parameters:\n        values_to_test (list): A list of values representing the noise levels to test.\n        base_model_path (str): The file path to the base model to load.\n\n    Returns:\n        Tuple containing two lists:\n            - List of disturbed models, each with noise added to layer weights.\n            - List of Signal-to-Noise Ratio (SNR) values corresponding to each disturbed model.\n\n    \"\"\"\n    snr_values = []\n    models = []\n\n    for value in values_to_test:\n        model_ = tf.keras.models.load_model(base_model_path, compile=False)\n        snr = add_noise_to_layer_weights(model_, last_conv_encoder_layer, value)\n        snr_values.append(snr)\n        models.append(model_)\n\n    return models, snr_values\n\n\ndisturbance_models, snr_values = produce_disturbed_models(values_to_test, model_path)","metadata":{"id":"U6JNeh5VtYaa","papermill":{"duration":1.935141,"end_time":"2024-06-14T22:23:11.474759","exception":false,"start_time":"2024-06-14T22:23:09.539618","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Disturbance processing with different SNR ratios values for each database partition using the modified networks\n\nBATCH_SIZE = 128\nTARGET_SHAPE = (128, 128)\nORIGINAL_MODEL_SHAPE = 256, 256\nNUM_ANNOTATORS = 3\n\ndef disturb_mask(model, image, model_shape, target_shape):\n    \"\"\"Disturbs a segmentation mask using a neural network model.\n\n    This function takes an input image and passes it through the given neural network model\n    to generate a disturbed segmentation mask. The input image is resized to fit the model's\n    input shape, and the output mask is resized to match the target shape.\n\n    Parameters:\n        model (tf.keras.Model): A neural network model used to disturb the segmentation mask.\n        image (tf.Tensor): Input image tensor.\n        model_shape (tuple): Shape of the input expected by the model.\n        target_shape (tuple): Target shape for the disturbed segmentation mask.\n\n    Returns:\n        A disturbed segmentation mask tensor.\n\n    \"\"\"\n    return tf.image.resize(model(tf.image.resize(image, model_shape)), target_shape)\n\n\ndef mix_channels(mask, num_annotators):\n    \"\"\"Mixes the channels of a segmentation mask.\n\n    This function creates a new tensor by mixing the channels of the input segmentation mask.\n    It is commonly used in scenarios where binary segmentation masks are represented with\n    multiple channels, each indicating the annotation of a different annotator.\n\n    Parameters:\n        mask (tensor): Input segmentation mask tensor with shape (batch_size, height, width, channels).\n        num_annotators (int): Number of annotators whose annotations are included in the mask.\n\n    Returns:\n        A tensor representing the mixed channels segmentation mask with shape\n        (batch_size, height, width, num_annotators).\n\n    \"\"\"\n    return tf.stack([mask, 1 - mask], axis=-2)\n\n\ndef add_noisy_annotators(img: EagerTensor, models, model_shape, target_shape) -> EagerTensor:\n    \"\"\"Adds noise from multiple annotators to an input image.\n\n    This function applies noise to an input image from multiple annotator models,\n    creating a set of noisy annotations. It iterates through each model in the\n    provided list of models, applying noise to the input image based on the\n    characteristics of each model.\n\n    Parameters:\n        img (EagerTensor): The input image to which noise will be added.\n        models (list): A list of annotator models used to generate noise.\n        model_shape: The shape of the model's output.\n        target_shape: The target shape of the output annotations.\n\n    Returns:\n        EagerTensor: A tensor representing the noisy annotations generated by\n        applying noise from multiple annotators to the input image.\n\n    \"\"\"\n    return tf.transpose([disturb_mask(model, img, model_shape=model_shape, target_shape=target_shape) for model in models], [2, 3, 1, 4, 0])\n\n\ndef map_dataset_MA(dataset, target_shape, model_shape, batch_size, num_annotators):\n    \"\"\"Preprocesses a dataset for multi-annotator segmentation tasks.\n\n    This function performs a series of mapping operations on the input dataset\n    to prepare it for training or evaluation in a multi-annotator segmentation\n    scenario. It resizes images and masks, adds noisy annotations, reshapes masks,\n    mixes channels, and batches the data.\n\n    Parameters:\n        dataset (tf.data.Dataset): Input dataset containing images, masks, labels, and image IDs.\n        target_shape (tuple): Desired shape for the images and masks after resizing.\n        model_shape (tuple): Shape required by the segmentation model.\n        batch_size (int): Size of the batches to create.\n        num_annotators (int): Number of annotators providing annotations for each image.\n\n    Returns:\n        A preprocessed dataset ready for training or evaluation.\n\n    \"\"\"\n    dataset_ = dataset.map(lambda img, mask, label, id_img: (img, mask),\n                           num_parallel_calls=tf.data.AUTOTUNE)\n\n    dataset_ = dataset_.map(lambda img, mask: (tf.image.resize(img, target_shape),\n                                                tf.image.resize(mask, target_shape)),\n                             num_parallel_calls=tf.data.AUTOTUNE)\n\n    dataset_ = dataset_.map(lambda img, mask: (img, add_noisy_annotators(tf.expand_dims(img, 0),\n                                                                         disturbance_models,\n                                                                         model_shape=model_shape,\n                                                                         target_shape=target_shape)),\n                             num_parallel_calls=tf.data.AUTOTUNE)\n\n    dataset_ = dataset_.map(lambda img, mask: (img, tf.reshape(mask, (mask.shape[0], mask.shape[1], 1, mask.shape[-1]))),\n                             num_parallel_calls=tf.data.AUTOTUNE)\n\n    dataset_ = dataset_.map(lambda img, mask: (img, mix_channels(mask, num_annotators)),\n                             num_parallel_calls=tf.data.AUTOTUNE)\n\n    dataset_ = dataset_.map(lambda img, mask: (img, tf.squeeze(mask, axis=2)),\n                             num_parallel_calls=tf.data.AUTOTUNE)\n\n    dataset_ = dataset_.batch(batch_size)\n    return dataset_\n\n\n\nsynthetic_train = map_dataset_MA(\n    train_dataset,\n    target_shape=TARGET_SHAPE,\n    model_shape=ORIGINAL_MODEL_SHAPE,\n    batch_size=BATCH_SIZE,\n    num_annotators=NUM_ANNOTATORS)\nsynthetic_val = map_dataset_MA(\n    val_dataset,\n    target_shape=TARGET_SHAPE,\n    model_shape=ORIGINAL_MODEL_SHAPE,\n    batch_size=BATCH_SIZE,\n    num_annotators=NUM_ANNOTATORS)\n\nsynthetic_test = map_dataset_MA(\n    test_dataset,\n    target_shape=TARGET_SHAPE,\n    model_shape=ORIGINAL_MODEL_SHAPE,\n    batch_size=BATCH_SIZE,\n    num_annotators=NUM_ANNOTATORS)","metadata":{"id":"a_53f05lN3xh","papermill":{"duration":2.79368,"end_time":"2024-06-14T22:23:14.329343","exception":false,"start_time":"2024-06-14T22:23:11.535663","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting the different perturbations to a sample and the resulting dimensions\n\nfor img,mask in synthetic_train.take(1):\n  print(f\"Mask shape: {mask.shape} (batch_size * h * w * k * r) Img shape {img.shape}\")\n  fig, axes = plt.subplots(2,NUM_ANNOTATORS)\n  fig.set_size_inches(16,7)\n  for i in range(NUM_ANNOTATORS):\n    axes[0][i].imshow((mask)[0,:,:,0,i])\n    axes[0][i].set_title(f\"Mask for annotator {i}\")\n    axes[0][i].axis('off')\n    axes[1][i].imshow((mask)[0,:,:,-1,i])\n    axes[1][i].axis('off')","metadata":{"id":"jSHwxW7CKNm4","outputId":"9b01f669-ce65-438f-873f-57f654d7b36e","papermill":{"duration":16.400942,"end_time":"2024-06-14T22:23:30.781859","exception":false,"start_time":"2024-06-14T22:23:14.380917","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Loading of the different parts of the dataset","metadata":{"id":"vEVUXnbe0jL4","papermill":{"duration":0.049743,"end_time":"2024-06-14T22:23:30.882664","exception":false,"start_time":"2024-06-14T22:23:30.832921","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Loading of the training part of the database in a tensor manner\n\nsynthetic_y_train = []\n\nfor img, mask in synthetic_train.take(8):\n    mask_1 = tf.reshape(mask,[BATCH_SIZE, TARGET_SHAPE[0], TARGET_SHAPE[1], NUM_ANNOTATORS*2])\n    synthetic_y_train.append(tf.where(mask_1> 0.5, tf.ones_like(mask_1), tf.zeros_like(mask_1)))\n\nsynthetic_y_train = tf.concat(synthetic_y_train, axis=0)\ny_train = tf.concat([synthetic_y_train,original_y_train], axis=-1)\nprint(f'Tensor dimensions with training images: {X_train.shape} \\nTensor dimensions with training masks: {y_train.shape}')","metadata":{"id":"JhLPCIzLsyu_","papermill":{"duration":109.153096,"end_time":"2024-06-14T22:25:20.086538","exception":false,"start_time":"2024-06-14T22:23:30.933442","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading of the validation part of the database in a tensor manner\n\nsynthetic_y_val = []\n\nfor img, mask in synthetic_val.take(2):\n    mask_1 = tf.reshape(mask,[BATCH_SIZE, TARGET_SHAPE[0], TARGET_SHAPE[1], NUM_ANNOTATORS*2])\n    synthetic_y_val.append(tf.where(mask_1> 0.5, tf.ones_like(mask_1), tf.zeros_like(mask_1)))\n\nsynthetic_y_val = tf.concat(synthetic_y_val, axis=0)\ny_val = tf.concat([synthetic_y_val,original_y_val], axis=-1)\nprint(f'Tensor dimensions with validation images: {X_val.shape}\\nTensor dimensions with validation masks: {y_val.shape}')","metadata":{"id":"5rhZCU_fsyu_","papermill":{"duration":29.01667,"end_time":"2024-06-14T22:25:49.153725","exception":false,"start_time":"2024-06-14T22:25:20.137055","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading of the testing part of the database in a tensor manner\n\nsynthetic_y_test = []\n\nfor img, mask in synthetic_test.take(2):\n    mask_1 = tf.reshape(mask,[BATCH_SIZE, TARGET_SHAPE[0], TARGET_SHAPE[1], NUM_ANNOTATORS*2])\n    synthetic_y_test.append(tf.where(mask_1> 0.5, tf.ones_like(mask_1), tf.zeros_like(mask_1)))\n\nsynthetic_y_test = tf.concat(synthetic_y_test, axis=0)\ny_test = tf.concat([synthetic_y_test,original_y_test], axis=-1)\nprint(f'Tensor dimensions with test images: {X_test.shape}\\nTensor dimensions with test masks: {y_test.shape}')","metadata":{"id":"K-D-SCq3syvA","papermill":{"duration":28.751601,"end_time":"2024-06-14T22:26:17.957026","exception":false,"start_time":"2024-06-14T22:25:49.205425","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loss for Tuned Gradual Class-wise Ensemble for Semi-supervised Learning","metadata":{"id":"kd-YM7Ej0jL4","papermill":{"duration":0.049968,"end_time":"2024-06-14T22:26:18.058803","exception":false,"start_time":"2024-06-14T22:26:18.008835","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"$$TGCE_{SS}(\\mathbf{Y}_r,f(\\mathbf X;\\theta) | \\mathbf{\\Lambda}_r (\\mathbf X;\\theta)) =\\mathbb E_{r} \\left\\{ \\mathbb E_{w,h} \\left\\{  \\Lambda_r (\\mathbf X; \\theta) \\circ \\mathbb E_k \\bigg\\{    \\mathbf Y_r \\circ \\bigg( \\frac{\\mathbf 1 _{W\\times H \\times K} - f(\\mathbf X;\\theta) ^{\\circ q }}{q} \\bigg); k \\in K  \\bigg\\}  + \\\\ \\left(\\mathbf 1 _{W \\times H } - \\Lambda _r (\\mathbf X;\\theta) \\right) \\circ \\bigg(   \\frac{\\mathbf 1_{W\\times H} - (\\frac {1}{k} \\mathbf 1_{W\\times H})^{\\circ q}}{q} \\bigg); w \\in W, h \\in H \\right\\};r\\in R\\right\\} $$","metadata":{"id":"kQf1wiJB0jL4","papermill":{"duration":0.051003,"end_time":"2024-06-14T22:26:18.160420","exception":false,"start_time":"2024-06-14T22:26:18.109417","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Custom loss function: TGCE SS\n\nclass TGCE_SS(Loss):\n    \"\"\"Tuned Gradual Class-wise Ensemble for Semi-supervised Learning Loss.\n\n    This loss function implements the Tuned Gradual Class-wise Ensemble (TGCE) loss for\n    semi-supervised learning tasks. It is designed to improve the robustness of models\n    against noisy annotations by considering the reliability of annotators. The loss function\n    penalizes the disagreement between the model predictions and the ground truth labels,\n    taking into account the reliability of annotators.\n\n    Methods\n    ----------\n    call(y_true, y_pred)\n    get_config()\n\n    \"\"\"\n\n    def __init__(self, q=0.1, name='TGCE_SS', R=3, K_=2, smooth=1e-5, fill_channels=1, **kwargs):\n        \"\"\"Initializes the TGCE_SS loss object.\n\n        Parameters:\n            q (float): The tuning parameter for controlling the smoothness of the ensemble.\n                Defaults to 0.1.\n            name (str): Name of the loss function. Defaults to 'TGCE_SS'.\n            R (int): Number of annotators. Defaults to NUM_ANNOTATORS.\n            K_ (int): Number of classes. Defaults to 2.\n            smooth (float): Smoothing parameter to avoid division by zero. Defaults to 1e-5.\n            fill_channels (int): Number of filler channels. Defaults to 1.\n            **kwargs: Additional arguments passed to the parent class.\n\n        \"\"\"\n        self.q = q\n        self.R = R\n        self.K_ = K_\n        self.smooth = smooth\n        self.fill_channels = fill_channels\n        super().__init__(name=name, **kwargs)\n\n    def call(self, y_true, y_pred):\n        \"\"\"Computes the TGCE_SS loss.\n\n        Parameters:\n            y_true (tensor): Ground truth labels.\n            y_pred (tensor): Predicted probabilities.\n\n        Returns:\n            Containing the loss value.\n        \"\"\"\n        y_pred = y_pred[..., :-self.fill_channels] # Disregarding backfill channels\n        y_true = y_true[...,:-1]\n        y_true = tf.reshape(y_true, tuple(y_true.shape[:-1])+(self.K_,self.R))\n        Lambda_r = y_pred[..., self.K_:]  # Annotators reliability -> extra cnn upsampling layer\n        y_pred_ = y_pred[..., :self.K_]  # Segmented images from unet\n        N, W, H, _ = y_pred_.shape\n        y_pred_ = y_pred_[..., tf.newaxis]\n        y_pred_ = tf.repeat(y_pred_, repeats=[self.R], axis=-1)  # Repeat f(x)\n        \n        epsilon = 1e-8  # Small constant to avoid divisions by zero\n        y_pred_ = tf.clip_by_value(y_pred_, epsilon, 1.0 - epsilon)  # Limiting values between epsilon and 1 - epsilon\n        \n        term_r = tf.math.reduce_mean(tf.math.multiply(y_true, (tf.ones([N, W, H, self.K_, self.R]) - tf.pow(y_pred_, self.q)) / (self.q + epsilon + self.smooth)), axis=-2)\n        term_c = tf.math.multiply(tf.ones([N, W, H, self.R]) - Lambda_r, (tf.ones([N, W, H, self.R]) - tf.pow((1 / self.K_ + self.smooth) * tf.ones([N, W, H, self.R]), self.q)) / (self.q + epsilon + self.smooth))\n        \n        # Avoid NaN in final loss function\n        TGCE_SS = tf.math.reduce_mean(tf.math.multiply(Lambda_r, term_r) + term_c)\n        if tf.math.is_nan(TGCE_SS):\n            TGCE_SS = tf.where(tf.math.is_nan(TGCE_SS), tf.constant(1e-8), TGCE_SS)  # Replace NaN with 1e-8\n            print(\"\\nInitializing TGCE_SS \\n\")\n        \n        return TGCE_SS #/ tf.constant(0.5857603) # Divided by the highest possible loss value        \n\n    def get_config(self):\n        \"\"\"Gets the configuration of the loss function.\n\n        Returns:\n            A dictionary containing the configuration parameters of the loss function.\n\n        \"\"\"\n        base_config = super().get_config()\n        return {**base_config, \"q\": self.q}","metadata":{"id":"9uvuW24WpDtR","papermill":{"duration":0.071457,"end_time":"2024-06-14T22:26:18.282086","exception":false,"start_time":"2024-06-14T22:26:18.210629","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Definition of performance metrics","metadata":{"id":"v3wYQJGH0jL5","papermill":{"duration":0.051113,"end_time":"2024-06-14T22:26:20.598757","exception":false,"start_time":"2024-06-14T22:26:20.547644","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### DICE metric","metadata":{"id":"z0Gtcw3m0jL5","papermill":{"duration":0.050743,"end_time":"2024-06-14T22:26:20.700381","exception":false,"start_time":"2024-06-14T22:26:20.649638","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"$$\\text{Dice} = {2 \\cdot |\\text{Intersection}| + \\text{smooth} \\over |\\text{Union}| + \\text{smooth}}$$\n\nWhere:\n\n$|\\text{Intersection}| = \\sum_{i=1}^{N} y\\_{true\\_i} \\cdot y\\_{pred\\_i}$, $|\\text{Union}| = \\sum_{i=1}^{N} y\\_{true\\_i} + \\sum_{i=1}^{N} y\\_{pred\\_i}$\n\n\n- $N$ is the total number of elements in the segmentation masks.\n- $y\\_{true\\_i}$ and $y\\_{pred\\_i}$ represent the value of the i-th element in the ground truth and predicted segmentation masks, respectively.\n- $\\text{smooth}$ is a smoothing parameter to avoid division by zero.","metadata":{"id":"fnX2NgqG0jL5","papermill":{"duration":0.050022,"end_time":"2024-06-14T22:26:20.800816","exception":false,"start_time":"2024-06-14T22:26:20.750794","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Definition of the DiceCoefficientMetric\n\ndef dice_metric(y_true, y_pred, axis=(1, 2), smooth=1e-5, num_annotators=3):\n    \"\"\"Computes the Dice coefficient metric for evaluating semantic segmentation.\n\n    This function calculates the Dice coefficient metric, which measures the similarity\n    between ground truth and predicted segmentation masks.\n\n    Parameters:\n        y_true (tensor): Ground truth segmentation masks.\n        y_pred (tensor): Predicted segmentation masks.\n        axis (tuple of int): Axis along which to compute sums. Defaults to (1, 2).\n        smooth (float): A smoothing parameter to avoid division by zero. Defaults to 1e-5.\n        num_annotators (int): Number of annotators. Defaults to 3.\n\n    Returns:\n        A scalar value representing the average Dice coefficient metric.\n    \"\"\"\n    y_true = y_true[...,-1]\n    y_pred = tf.squeeze(y_pred[...,:1],axis=-1)  \n    y_pred = tf.where(y_pred> 0.5, tf.ones_like(y_pred), tf.zeros_like(y_pred))\n    intersection = tf.reduce_sum(y_true * y_pred, axis=axis)\n    union = tf.reduce_sum(y_true, axis=axis) + tf.reduce_sum(y_pred, axis=axis)\n    dice = (2. * intersection + smooth) / (union + smooth)\n    return tf.reduce_mean(dice)","metadata":{"id":"uYjk3Vj20jL5","papermill":{"duration":0.060715,"end_time":"2024-06-14T22:26:20.911909","exception":false,"start_time":"2024-06-14T22:26:20.851194","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Jaccard metric","metadata":{"id":"rRNiKRbB0jL5","papermill":{"duration":0.051593,"end_time":"2024-06-14T22:26:21.014806","exception":false,"start_time":"2024-06-14T22:26:20.963213","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"$$\\text{Jaccard} = {|\\text{Intersection}| + \\text{smooth} \\over |\\text{Union}| + \\text{smooth}}$$\n\nWhere:\n\n$|\\text{Intersection}| = \\sum_{i=1}^{N} y\\_{true\\_i} \\cdot y\\_{pred\\_i}$, $|\\text{Union}| = \\sum_{i=1}^{N} y\\_{true\\_i} + \\sum_{i=1}^{N} y\\_{pred\\_i} - |\\text{Intersection}|$\n- $N$ is the total number of elements in the segmentation masks.\n- $y\\_{true\\_i}$ and $y\\_{pred\\_i}$ represent the value of the i-th element in the ground truth and predicted segmentation masks, respectively.\n- $\\text{smooth}$ is a small smoothing parameter to avoid division by zero.","metadata":{"id":"4oITKb_v0jL5","papermill":{"duration":0.053652,"end_time":"2024-06-14T22:26:21.120269","exception":false,"start_time":"2024-06-14T22:26:21.066617","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Definition of the JaccardMetric\n\ndef jaccard_metric(y_true, y_pred, axis=(1, 2), smooth=1e-5, num_annotators=3):\n    \"\"\"Computes the Jaccard similarity coefficient as a metric for semantic segmentation.\n\n    The Jaccard similarity coefficient, also known as the Intersection over Union (IoU),\n    measures the similarity between two sets by comparing their intersection to their union.\n    In the context of semantic segmentation, it quantifies the overlap between the ground\n    truth segmentation masks and the predicted segmentation masks.\n\n    Parameters:\n        y_true (tensor): Ground truth segmentation masks.\n        y_pred (tensor): Predicted segmentation masks.\n        axis (tuple of int): Axes along which to compute sums. Defaults to (1, 2).\n        smooth (float): A small smoothing parameter to avoid division by zero. Defaults to 1e-5.\n        num_annotators (int): Number of annotators. Defaults to 3.\n\n    Returns:\n        A tensor representing the mean Jaccard similarity coefficient.\n\n    \"\"\"\n    y_true = y_true[...,-1]\n    y_pred = tf.squeeze(y_pred[...,:1],axis=-1) \n    y_pred = tf.where(y_pred> 0.5, tf.ones_like(y_pred), tf.zeros_like(y_pred))\n    intersection = tf.reduce_sum(y_true * y_pred, axis=axis)\n    union = tf.reduce_sum(y_true, axis=axis) + tf.reduce_sum(y_pred, axis=axis) - intersection\n    jaccard = (intersection + smooth) / (union + smooth)\n    return tf.reduce_mean(jaccard)","metadata":{"id":"0F8KNADG0jL5","papermill":{"duration":0.061982,"end_time":"2024-06-14T22:26:21.236628","exception":false,"start_time":"2024-06-14T22:26:21.174646","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Sensitivity metric","metadata":{"id":"bqmOPI3P0jL5","papermill":{"duration":0.050033,"end_time":"2024-06-14T22:26:21.336830","exception":false,"start_time":"2024-06-14T22:26:21.286797","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"$$\\text{Sensitivity} = {\\text{True Positives} \\over \\text{Actual Positives} + \\text{smooth}}$$\n\nWhere:\n\n$\\text{True Positives} = \\sum_{i=1}^{N} y\\_{true\\_i} \\cdot y\\_{pred\\_i}$, $\\text{Actual Positives} = \\sum_{i=1}^{N} y\\_{true\\_i}$\n\n\n- $N$ is the total number of elements in the labels.\n- $y\\_{true\\_i}$ and $y\\_{pred\\_i}$ represent the value of the i-th element in the ground truth and predicted labels, respectively.\n- $\\text{smooth}$ is a small value added to the denominator to avoid division by zero.","metadata":{"id":"xfvJITFI0jL5","papermill":{"duration":0.050271,"end_time":"2024-06-14T22:26:21.438148","exception":false,"start_time":"2024-06-14T22:26:21.387877","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Definition of the SensitivityMetric\n\ndef sensitivity_metric(y_true, y_pred, axis=(1, 2), smooth=1e-5, num_annotators=3):\n    \"\"\"Computes the sensitivity as a metric for semantic segmentation.\n\n    Sensitivity, also known as true positive rate or recall, measures the proportion\n    of actual positives that are correctly identified by the model. It is computed\n    as the ratio of true positives to the sum of true positives and false negatives.\n\n    Parameters:\n        y_true (tensor): Ground truth labels.\n        y_pred (tensor): Predicted probabilities or labels.\n        axis (tuple): Axes over which to perform the reduction. Defaults to (1, 2).\n        smooth (float): A small value added to the denominator to avoid division by zero. Defaults to 1e-5.\n        num_annotators (int): Number of annotators. Defaults to 3.\n\n    Returns:\n        The sensitivity metric averaged over the specified axes.\n\n    \"\"\"\n    y_true = y_true[...,-1]\n    y_pred = tf.squeeze(y_pred[...,:1],axis=-1) \n    y_pred = tf.where(y_pred> 0.5, tf.ones_like(y_pred), tf.zeros_like(y_pred))\n    true_positives = tf.reduce_sum(y_true * y_pred, axis=axis)\n    actual_positives = tf.reduce_sum(y_true, axis=axis)\n    sensitivity = true_positives / (actual_positives + smooth)\n    return tf.reduce_mean(sensitivity)","metadata":{"id":"OOHnQM4S0jL5","papermill":{"duration":0.062947,"end_time":"2024-06-14T22:26:21.554380","exception":false,"start_time":"2024-06-14T22:26:21.491433","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Specificity metric","metadata":{"id":"479663ou0jL6","papermill":{"duration":0.050162,"end_time":"2024-06-14T22:26:21.654744","exception":false,"start_time":"2024-06-14T22:26:21.604582","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"$$\\text{Specificity} = {\\text{True Negatives} \\over \\text{Actual Negatives} + \\text{smooth}}$$\n\nWhere:\n\n$\\text{True Negatives} = \\sum_{i=1}^{N} (1 - y\\_{true\\_i}) \\cdot (1 - y\\_{pred\\_i})$, $\\text{Actual Negatives} = \\sum_{i=1}^{N} (1 - y\\_{true\\_i})$\n\n- $N$ is the total number of samples.\n- $y\\_{true\\_i}$ and $y\\_{pred\\_i}$ represent the ground truth label and predicted probability (or binary prediction) for the i-th sample, respectively.\n- $\\text{smooth}$ is a smoothing term to avoid division by zero.","metadata":{"id":"QMxbXz880jL6","papermill":{"duration":0.051065,"end_time":"2024-06-14T22:26:21.757178","exception":false,"start_time":"2024-06-14T22:26:21.706113","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Definition of the SpecificityMetric\n\ndef specificity_metric(y_true, y_pred, axis=(1, 2), smooth=1e-5, num_annotators=3):\n    \"\"\"Computes the specificity as a metric for semantic segmentation.\n\n    Specificity measures the proportion of actual negative cases that were correctly\n    identified as such. It is complementary to sensitivity (recall).\n\n    Parameters:\n        y_true (tensor): Ground truth binary labels.\n        y_pred (tensor): Predicted probabilities or binary predictions.\n        axis (tuple): Axes over which to perform reduction. Defaults to (1, 2).\n        smooth (float): Smoothing term to avoid division by zero. Defaults to 1e-5.\n        num_annotators (int): Number of annotators. Defaults to 3.\n\n    Returns:\n        A tensor representing the specificity metric.\n\n    \"\"\"\n    y_true = y_true[...,-1]\n    y_pred = tf.squeeze(y_pred[...,:1],axis=-1)   \n    y_pred = tf.where(y_pred> 0.5, tf.ones_like(y_pred), tf.zeros_like(y_pred))\n    true_negatives = tf.reduce_sum((1 - y_true) * (1 - y_pred), axis=axis)\n    actual_negatives = tf.reduce_sum(1 - y_true, axis=axis)\n    specificity = true_negatives / (actual_negatives + smooth)\n    return tf.reduce_mean(specificity)","metadata":{"id":"umqjCkUA0jL6","papermill":{"duration":0.060138,"end_time":"2024-06-14T22:26:21.868309","exception":false,"start_time":"2024-06-14T22:26:21.808171","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Layers definition","metadata":{"id":"Ha_vo22D0jL6","papermill":{"duration":0.050267,"end_time":"2024-06-14T22:26:21.969017","exception":false,"start_time":"2024-06-14T22:26:21.918750","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Definition of layers for the neural network structure\n\nDefaultConv2D = partial(layers.Conv2D,\n                        kernel_size=3, activation='relu', padding=\"same\")\n\nDilatedConv = partial(layers.Conv2D,\n                        kernel_size=3, activation='relu', padding=\"same\", dilation_rate=10, name=\"DilatedConv\")\n\nDefaultPooling = partial(layers.MaxPool2D,\n                        pool_size=2)\n\nupsample = partial(layers.UpSampling2D, (2,2))","metadata":{"id":"jUz819YKVclC","papermill":{"duration":0.058921,"end_time":"2024-06-14T22:26:22.078737","exception":false,"start_time":"2024-06-14T22:26:22.019816","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Kernel initializers","metadata":{"id":"7s29maMe0jL6","papermill":{"duration":0.051185,"end_time":"2024-06-14T22:26:22.180280","exception":false,"start_time":"2024-06-14T22:26:22.129095","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def kernel_initializer(seed):\n    \"\"\"Returns a Glorot uniform initializer for kernel weights.\n\n    Glorot uniform initializer, also known as Xavier uniform initializer,\n    is commonly used to initialize the weights of kernels in neural network layers.\n    It draws samples from a uniform distribution within a certain range,\n    calculated to keep the variance of the weights constant across layers.\n    This initializer is useful for training deep neural networks.\n\n    Parameters:\n        seed (int): Random seed for reproducibility.\n\n    Returns:\n        A Glorot uniform initializer for kernel weights.\n\n    \"\"\"\n    return tf.keras.initializers.GlorotUniform(seed=seed)","metadata":{"id":"t5mUEaWR0jL6","papermill":{"duration":0.059057,"end_time":"2024-06-14T22:26:22.290541","exception":false,"start_time":"2024-06-14T22:26:22.231484","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Activation functions","metadata":{"id":"NwHge1Hu0jMC","papermill":{"duration":0.050457,"end_time":"2024-06-14T22:26:22.392106","exception":false,"start_time":"2024-06-14T22:26:22.341649","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### SparseSoftmax","metadata":{"id":"73n0TOla0jMC","papermill":{"duration":0.050881,"end_time":"2024-06-14T22:26:22.493646","exception":false,"start_time":"2024-06-14T22:26:22.442765","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"$$\\text{SparseSoftmax}(x) = \\frac{\\exp(x - \\text{max}(x))}{\\text{sum}(\\exp(x - \\text{max}(x)))}$$\n\nWhere:\n\n- $x$ is the input tensor.\n- $\\text{max}(x)$ is the maximum value in the tensor $x$.\n- $\\text{sum}$ is the sum of the exponential values of $x - \\text{max}(x)$.","metadata":{"id":"_p8uYjqo0jMC","papermill":{"duration":0.050774,"end_time":"2024-06-14T22:26:22.596617","exception":false,"start_time":"2024-06-14T22:26:22.545843","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class SparseSoftmax(Layer):\n    \"\"\"Custom layer implementing the sparse softmax activation function.\n\n    This layer computes the softmax activation function for a given input tensor,\n    handling sparse input efficiently.\n\n    Methods\n    ----------\n    build(input_shape)\n    call(x)\n    compute_output_shape(input_shape)\n\n    \"\"\"\n\n    def __init__(self, name='SparseSoftmax', **kwargs):\n        \"\"\"Initializes the SparseSoftmax layer.\n\n        Parameters:\n            **kwargs: Additional arguments to be passed to the parent class.\n\n        \"\"\"\n        super(SparseSoftmax, self).__init__(name=name,**kwargs)\n\n    def build(self, input_shape):\n        \"\"\"Builds the layer.\n\n        Parameters:\n            input_shape (tuple): Shape of the input tensor.\n\n        \"\"\"\n        super(SparseSoftmax, self).build(input_shape)\n\n    def call(self, x):\n        \"\"\"Computes the output of the layer.\n\n        Parameters:\n            x (tensor): Input tensor.\n\n        Returns:\n            A tensor representing the output of the softmax activation function.\n\n        \"\"\"\n        e_x = K.exp(x - K.max(x, axis=-1, keepdims=True))\n        sum_e_x = K.sum(e_x, axis=-1, keepdims=True)\n        output = e_x / (sum_e_x + K.epsilon())\n        return output\n\n    def compute_output_shape(self, input_shape):\n        \"\"\"Computes the output shape of the layer.\n\n        Parameters:\n            input_shape (tuple): Shape of the input tensor.\n\n        Returns:\n            The same shape as the input tensor.\n\n        \"\"\"\n        return input_shape","metadata":{"id":"BbCdFp0-0jMD","papermill":{"duration":0.062273,"end_time":"2024-06-14T22:26:22.708940","exception":false,"start_time":"2024-06-14T22:26:22.646667","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.get_custom_objects()['sparse_softmax'] = SparseSoftmax()","metadata":{"id":"gD0GEXmT0jMD","papermill":{"duration":0.057437,"end_time":"2024-06-14T22:26:22.924222","exception":false,"start_time":"2024-06-14T22:26:22.866785","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualization function","metadata":{}},{"cell_type":"code","source":"def plot_prediction(prediction,images,masks,num_img,num_annotators):\n    \"\"\"\n    Plots the original image, its corresponding masks, and the predicted masks and confidence maps.\n\n    This function helps in visualizing the performance of a model that predicts masks and confidence maps\n    for a given image. It creates a 4x4 subplot grid, where the first row displays the original image, the second\n    row shows the original and predicted masks for two channels, the third row displays the masks of each\n    annotator, and the fourth row shows the confidence maps of each annotator.\n\n    Parameters:\n    prediction (tensor): A tensor of shape (B, H, W, C) containing the predicted masks and confidence maps,\n                           where B is the batch size, H and W are the height and width of the image, and C is\n                           the number of channels (2 channels for masks and C-2 channels for confidence maps).\n    images (tensor): A tensor of shape (B, H, W, 3) containing the original RGB images.\n    masks (tensor): A tensor of shape (B, H, W, C) containing the original masks annotated by C annotators.\n    num_img (int): The index of the image in the batch to be plotted.\n    num_annotators (int): The number of annotators.\n\n    Returns:\n    None. This function only displays the plots.\n    \"\"\"\n    \n    \n    masks_annotators = masks\n    channels = [0, num_annotators]\n    masks = tf.gather(masks, channels, axis=-1)\n    rows, columns = 4, 4\n    fig, axes = plt.subplots(rows, columns, figsize=(20,10))\n    \n    # First row: original image\n    axes[0, 0].imshow(images[num_img,...])\n    axes[0, 0].set_title('Original image')\n    \n    # Second row: original masks and predicted masks\n    axes[1, 0].imshow(masks[num_img,...,0])\n    axes[1, 0].set_title('Original mask: Channel 0')\n    axes[1, 1].imshow(masks[num_img,...,1])\n    axes[1, 1].set_title('Original mask: Channel 1')\n    axes[1, 2].imshow(prediction[0,...,0])\n    axes[1, 2].set_title('Predicted mask: Channel 0')\n    axes[1, 3].imshow(prediction[0,...,1])\n    axes[1, 3].set_title('Predicted mask: Channel 1')\n    \n    # Third row: mask of each annotator for the image\n    axes[2, 0].imshow(masks_annotators[num_img,...,0])\n    axes[2, 0].set_title('Annotator 1 mask for image')\n    axes[2, 1].imshow(masks_annotators[num_img,...,1])\n    axes[2, 1].set_title('Annotator 2 mask for image')\n    axes[2, 2].imshow(masks_annotators[num_img,...,2])\n    axes[2, 2].set_title('Annotator 3 mask for image')\n    \n    # Fourth row: confidence maps of each annotator\n    axes[3, 0].imshow(prediction[0,...,2],vmin=0.0,vmax=1.0)\n    axes[3, 0].set_title('Confidence map annotator 1')\n    axes[3, 1].imshow(prediction[0,...,3],vmin=0.0,vmax=1.0)\n    axes[3, 1].set_title('Confidence map annotator 2')\n    axes[3, 2].imshow(prediction[0,...,4],vmin=0.0,vmax=1.0)\n    axes[3, 2].set_title('Confidence map annotator 3')\n    \n    # Off all axis\n    [axes[i, k].axis('off') for i in range(rows) for k in range(columns)]","metadata":{"papermill":{"duration":0.067904,"end_time":"2024-06-14T22:26:23.491629","exception":false,"start_time":"2024-06-14T22:26:23.423725","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Bayesian optimization of hyperparameters with Keras Tuner","metadata":{"papermill":{"duration":0.050026,"end_time":"2024-06-14T22:26:23.799926","exception":false,"start_time":"2024-06-14T22:26:23.749900","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class MetricsCallback(tf.keras.callbacks.Callback):\n    \"\"\"\n    Custom Keras callback to log metrics at the end of training epochs.\n\n    This callback logs metrics such as loss and accuracy at the end of each epoch\n    during training. It stores the metrics in a global list `global_metrics` upon\n    completion of the final epoch.\n\n    Parameters:\n        epochs (int): Total number of epochs for training.\n\n    Methods:\n        on_epoch_end(epoch, logs=None):\n            Called at the end of each epoch. Logs metrics and stores them in\n            `global_metrics` upon completion of the final epoch.\n    \"\"\"\n\n    def __init__(self, epochs):\n        \"\"\"\n        Initializes the MetricsCallback instance.\n\n        Parameters:\n            epochs (int): Total number of epochs for training.\n        \"\"\"\n        super().__init__()\n        self.epochs = epochs\n\n    def on_epoch_end(self, epoch, logs=None):\n        \"\"\"\n        Callback function called at the end of each epoch.\n\n        Logs metrics such as loss and accuracy at the end of each epoch.\n        Stores the metrics in `global_metrics` upon completion of the final epoch.\n\n        Parameters:\n            epoch (int): Current epoch number (0-indexed).\n            logs (dict): Dictionary containing the metrics to log.\n                Typically contains keys like 'loss' and 'accuracy'.\n        \"\"\"\n        if epoch == self.epochs - 1:\n            print(f\"Final Epoch {epoch + 1}:\")\n            metrics_dict = {'epoch': epoch + 1}\n            for key, value in logs.items():\n                print(f\"{key}: {value:.4f}\")\n                metrics_dict[key] = value\n            global_metrics.append(metrics_dict)","metadata":{"papermill":{"duration":0.060075,"end_time":"2024-06-14T22:26:23.910228","exception":false,"start_time":"2024-06-14T22:26:23.850153","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![Unet-TGCELoss](https://lh3.googleusercontent.com/fife/ALs6j_FQpKDYXh1PSuK4B7zca2t-aLNDOGiHv3Cx4wnqrj5tDZp7LFygbkh3Pw0t3sIu7Fly0fC7BrL5I8HME3fXUAFPgKIkzPuTL8W1m6kBgaqQf8yMOt89R7PUO4PNgy0dG5DbixazOdTRquMcEzJ1WiYWf33c1gIK9_8eJhDmmhAYutY-HkMQBerxetLXEdSc2euRjPf7yKb44U9MwhfhX7UWt4Scndk6Pn6FQIczftrSXrAkpm1wQ3KqEuvtfRyffpoYAc48uZ2SYu9G22X9ZIyWBiGhQbuOe1MpW27lJ8SAtXcG5BIt6jy55NTFSWeh0AhuVM-3gwKhTOYqwy4C0lcbXQJ8XobTaWAQWxSyCPHkGfsccxwkILRi9-13j7Bd22ZReapWyu7w0Yrn4I2qxpvJ7kiVjElZWw8foKitEHsELCqLOKcJXrGQGr0qo9maTPzU_aLVYObrDrvndmdOSfkThd2hYswinp75TaoDSMOa3XJIK-02Rv9uS3S8w6VaWX3kggtxDeGlUyfLs8zBOj9SI0sZ8X8l0fd3nFUgOb1lTf29Eod57vzVICMMQvWO0sgabPavO7QxiMt5EvcJHrdZz0U1OYtC91SAszpd58JQSKPChrRaYjIiPNZPWdkFkztzFhClTMlgw8U6SUTSWHYJEDXFeb15Dnv4kNENTV0zpBT6PCdrdpqsNMnhGPGpaCeVPivt85Em8mz1I5YApYKKstjP-L5GG3cCSM3KSe2lu13cVZoBHDXAVUfmqlGgj37MGYsAi5V8U-S4G7Xtun40AYqxRC4OhptA2AdShQ6JOdPfKAlmhLy4rH9Ae1Euu3KrCcYuFY_-_x0POr45C7ouP5uyOAgtWDvM75yyAYkSBVqMYKV-VFnLW_piSQNsZiQXwqjiCGYTi4eYJVsn8TatuJW7o6L1MlO5qdtdgrWNN4lNTl1_CFB8b55wrzZgCxjuuUUGRKU1hA8cEtECtktWbUhfrwvgdCU7-fvuUMIZF7OBizlzGXQ-tYYbQjGpUzfSq19efU_9Wqj8Fep5fNRKuLRTLVP4d-GLy7ecoHbYR-bDx-Ln3oTiiEJ7n1_H6vrRskybpNN_eGhokwxNy3_9h2jWeEAxwWJn6tMf19lkpwdsYanf-VTp5SthSoWb7BA7_qGGXScorMXbbFlKGjAfYKLuXLapcO2wy1yvQe87fggoTkyECfNg_3ie_TdEZlmhdvjAX0oNNOFH6KOsck3WbRdB3cOyICd872hkS0QNB8TxdPVRAR_2ifRxBKOygqBYA0WKEVl2VPCnDDfmwJp401ORkk6AFEZssaKzWinP9Kt3Xo436vo_983b8oNLowlhfeJpHR9iWl9iaHxwU_LShYvCrG8L_eLeaPFAVVXgpPF31KGgeKOSFoaEDvO5OpMrPJ0HrDZPZSXw6KMNtf0B0t_1R6OJoD9OpJSAvUVrVd9xxnsVJxZGSGFr28MNrSJ1gl9PRPyLalFiakDhxYO2PwshZJrzf-GaUANkRx-lJpnzPkqlqcMxl1HNDYSnY73A3sb3kLJe7m_eq0EfCpJkTK4oNPF6-fS1YcQguS8BKnlMXdWfTTywv6Cjhb7ROe1qB83tCtU7syhVEuTzMohgcm-5jT-AOtGJX-WGQny3mboNKZU0j_y9sHAJD7UmTihwutj4Hr8eF1yFUbOTdw=w1632-h928)","metadata":{"id":"-R8yTj9Z0jME","papermill":{"duration":0.050365,"end_time":"2024-06-14T22:26:23.699283","exception":false,"start_time":"2024-06-14T22:26:23.648918","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def build_model_kt(hp: HyperParameters, input_shape=(128,128,3), name='UNET', out_channels=2, out_filler_channels=2,\n                   out_ActFunction='sparse_softmax', n_scorers=3, out_ActFunction_Lambda='sparse_softmax'):\n    \"\"\"\n    Builds a U-Net style model for image segmentation with hyperparameters tuned using Keras Tuner.\n\n    This function constructs a U-Net architecture for semantic segmentation. It includes both the\n    encoder and decoder parts with skip connections, and allows for hyperparameter tuning using\n    Keras Tuner.\n\n    Parameters:\n        hp (HyperParameters): Keras Tuner HyperParameters object for defining tunable hyperparameters.\n        input_shape (tuple): Shape of the input image tensor. Defaults to (128, 128, 3).\n        name (str): Name of the model. Defaults to 'UNET'.\n        out_channels (int): Number of output channels/classes for segmentation. Defaults to 2.\n        out_filler_channels (int): Number of output filler channels. Defaults to 1.\n        out_ActFunction (str): Activation function for the output layer. Defaults to 'sparse_softmax'.\n        n_scorers (int): Number of scorers. Defaults to 3.\n        out_ActFunction_Lambda (str): Activation function for the lambda output layer. Defaults to 'sparse_softmax'.\n\n    Returns:\n        tf.keras.Model: Compiled U-Net model configured with the specified hyperparameters.\n    \"\"\"\n    # Encoder\n    input = layers.Input(shape=input_shape)\n\n    x = layers.BatchNormalization(name='Batch00')(input)\n    x = DefaultConv2D(8, kernel_initializer=kernel_initializer(34), name='Conv10')(x)\n    #x = layers.Dropout(0.1)(x)\n    x = layers.BatchNormalization(name='Batch10')(x)\n    x = level_1 = DefaultConv2D(8, kernel_initializer=kernel_initializer(4), name='Conv11')(x)\n    #x = layers.Dropout(0.1)(x)\n    x = layers.BatchNormalization(name='Batch11')(x)\n    x = DefaultPooling(name='Pool10')(x)  # 128x128 -> 64x64\n\n    x = DefaultConv2D(16, kernel_initializer=kernel_initializer(56), name='Conv20')(x)\n    #x = layers.Dropout(0.1)(x)\n    x = layers.BatchNormalization(name='Batch20')(x)\n    x = level_2 = DefaultConv2D(16, kernel_initializer=kernel_initializer(32), name='Conv21')(x)\n    #x = layers.Dropout(0.1)(x)\n    x = layers.BatchNormalization(name='Batch22')(x)\n    x = DefaultPooling(name='Pool20')(x)  # 64x64 -> 32x32\n\n    x = DefaultConv2D(32, kernel_initializer=kernel_initializer(87), name='Conv30')(x)\n    #x = layers.Dropout(0.1)(x)\n    x = layers.BatchNormalization(name='Batch30')(x)\n    x = level_3 = DefaultConv2D(32, kernel_initializer=kernel_initializer(30), name='Conv31')(x)\n    #x = layers.Dropout(0.1)(x)\n    x = layers.BatchNormalization(name='Batch31')(x)\n    x = DefaultPooling(name='Pool30')(x)  # 32x32 -> 16x16\n\n    x = DefaultConv2D(64, kernel_initializer=kernel_initializer(79), name='Conv40')(x)\n    #x = layers.Dropout(0.1)(x)\n    x = layers.BatchNormalization(name='Batch40')(x)\n    x = level_4 = DefaultConv2D(64, kernel_initializer=kernel_initializer(81), name='Conv41')(x)\n    #x = layers.Dropout(0.1)(x)\n    x = layers.BatchNormalization(name='Batch41')(x)\n    x = DefaultPooling(name='Pool40')(x)  # 16x16 -> 8x8\n\n    # Decoder\n    x = DefaultConv2D(128, kernel_initializer=kernel_initializer(89), name='Conv50')(x)\n    #x = layers.Dropout(0.1)(x)\n    x = layers.BatchNormalization(name='Batch50')(x)\n    x = DefaultConv2D(128, kernel_initializer=kernel_initializer(42), name='Conv51')(x)\n    x = layers.Dropout(0.1)(x)\n    x = layers.BatchNormalization(name='Batch51')(x)\n\n    x = upsample(name='Up60')(x)  # 8x8 -> 16x16\n    x = layers.Concatenate(name='Concat60')([level_4, x])\n    x = DefaultConv2D(64, kernel_initializer=kernel_initializer(91), name='Conv60')(x)\n    #x = layers.Dropout(0.1)(x)\n    x = layers.BatchNormalization(name='Batch60')(x)\n    x = DefaultConv2D(64, kernel_initializer=kernel_initializer(47), name='Conv61')(x)\n    #x = layers.Dropout(0.1)(x)\n    x = layers.BatchNormalization(name='Batch61')(x)\n\n    x = upsample(name='Up70')(x)  # 16x16 -> 32x32\n    x = layers.Concatenate(name='Concat70')([level_3, x])\n    x = DefaultConv2D(32, kernel_initializer=kernel_initializer(21), name='Conv70')(x)\n    #x = layers.Dropout(0.1)(x)\n    x = layers.BatchNormalization(name='Batch70')(x)\n    x = DefaultConv2D(32, kernel_initializer=kernel_initializer(96), name='Conv71')(x)\n    #x = layers.Dropout(0.1)(x)\n    x = layers.BatchNormalization(name='Batch71')(x)\n\n    x = upsample(name='Up80')(x)  # 32x32 -> 64x64\n    x = layers.Concatenate(name='Concat80')([level_2, x])\n    x = DefaultConv2D(16, kernel_initializer=kernel_initializer(96), name='Conv80')(x)\n    #x = layers.Dropout(0.1)(x)\n    x = layers.BatchNormalization(name='Batch80')(x)\n    x = DefaultConv2D(16, kernel_initializer=kernel_initializer(98), name='Conv81')(x)\n    #x = layers.Dropout(0.1)(x)\n    x = layers.BatchNormalization(name='Batch81')(x)\n\n    x = upsample(name='Up90')(x)  # 64x64 -> 128x128\n    x = layers.Concatenate(name='Concat90')([level_1, x])\n    x = DefaultConv2D(8, kernel_initializer=kernel_initializer(35), name='Conv90')(x)\n    #x = layers.Dropout(0.1)(x)\n    x = layers.BatchNormalization(name='Batch90')(x)\n    x = DefaultConv2D(8, kernel_initializer=kernel_initializer(7), name='Conv91')(x)\n    #x = layers.Dropout(0.1)(x)\n    x = layers.BatchNormalization(name='Batch91')(x)\n\n    xy = DefaultConv2D(out_channels, kernel_size=(1, 1), activation=out_ActFunction,\n                      kernel_initializer=kernel_initializer(42), name='Conv100')(x)\n    x_lambda = DilatedConv(n_scorers, kernel_size=(1, 1), activation=out_ActFunction_Lambda,\n                        kernel_initializer=kernel_initializer(42), name='DilatedConv101-Lambda')(x)\n    xyy = DefaultConv2D(out_filler_channels, kernel_size=(1, 1),\n                      kernel_initializer=kernel_initializer(42),\n                      name='Conv101')(x)\n\n    y = layers.Concatenate(name='Concat100')([xy, x_lambda, xyy])\n\n    model = Model(input, y, name=name)\n\n    model.compile(optimizer=Adam(hp.Float('learning_rate', min_value=1e-4, max_value=1e-1, sampling='log')),\n              loss=TGCE_SS(q=hp.Float('q', min_value=0.1, max_value=0.9, sampling='linear'), R=NUM_ANNOTATORS, K_=2, fill_channels=2),\n              metrics=[dice_metric, jaccard_metric, sensitivity_metric, specificity_metric])\n\n    return model","metadata":{"papermill":{"duration":0.083166,"end_time":"2024-06-14T22:26:24.044180","exception":false,"start_time":"2024-06-14T22:26:23.961014","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Definition number of epochs and initialization of the global variable with metrics for callback\n\nnum_epochs = 15\nglobal_metrics = []\ncallbacks = [MetricsCallback(num_epochs)]","metadata":{"papermill":{"duration":0.057711,"end_time":"2024-06-14T22:26:24.152632","exception":false,"start_time":"2024-06-14T22:26:24.094921","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!rm -rf /kaggle/working/my_dir","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define HyperModel and tuner\ntuner = BayesianOptimization(\n    build_model_kt,\n    objective=Objective('val_dice_metric',direction='max'),\n    max_trials=80,\n    executions_per_trial=1,\n    directory='my_dir',\n    project_name='val_dice')","metadata":{"papermill":{"duration":0.655412,"end_time":"2024-06-14T22:26:24.858665","exception":false,"start_time":"2024-06-14T22:26:24.203253","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Execute the search\ntuner.search(X_train, y_train, epochs=num_epochs, batch_size=64, validation_data=(X_val, y_val), callbacks=callbacks)","metadata":{"papermill":{"duration":4941.612072,"end_time":"2024-06-14T23:48:46.527578","exception":false,"start_time":"2024-06-14T22:26:24.915506","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## GPR obtained from the optimization with Keras Tuner","metadata":{}},{"cell_type":"code","source":"gp = tuner.oracle._gpr_trained()\nprint(f\"Kernel parameters before training: {gp.kernel}\\nKernel parameters after training: {gp.kernel_}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Obtaining optimization results","metadata":{}},{"cell_type":"code","source":"# Convert to a pandas data frame the metrics of each epoch of each trial\ndf_metrics = pd.DataFrame(global_metrics)\n\n# List to store the results\ntrial_results = []\n\n# Iterate over all trials\nfor trial in tuner.oracle.get_best_trials(num_trials=tuner.oracle.max_trials):\n    trial_info = {\n        'trial_id': trial.trial_id,\n        'hyperparameters': trial.hyperparameters.values,\n        'dice': trial.metrics.get_best_value('dice_metric')\n    }\n    trial_results.append(trial_info)\n\n# Convert the results list to a DataFrame\ndf_trials = pd.DataFrame(trial_results)\n\n# Apply pd.json_normalize to expand the dictionary into new columns\ndf_hyperparameters = pd.json_normalize(df_trials['hyperparameters'])\n\n# Combine the original DataFrame with the new hyperparameter columns\nhp_df = pd.concat([df_trials['trial_id'], df_hyperparameters, df_trials['dice']], axis=1)\n\n# Display the combined DataFrame\n#hp_df = hp_df.sort_values(by='trial_id')\nhp_df.to_json('hp_df.json')\nhp_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualize results of optimization","metadata":{"papermill":{"duration":0.051347,"end_time":"2024-06-14T23:48:47.032537","exception":false,"start_time":"2024-06-14T23:48:46.981190","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Plots the distributions\nplt.figure(figsize=(12, 8))\nfor i, column in enumerate(hp_df.columns[1:]):\n    plt.subplot(2, 2, i+1)\n    sns.histplot(hp_df[column], kde=True)\n    plt.title(column)\n\nplt.tight_layout()\nplt.show()","metadata":{"papermill":{"duration":2.377322,"end_time":"2024-06-14T23:48:49.461153","exception":false,"start_time":"2024-06-14T23:48:47.083831","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns_to_log_scale = ['learning_rate']\n# Plot hyperparameters values vs loss value\nfig,axes = plt.subplots(1, 2, figsize=(10, 6))\n\n# Iterate over each subplot and the corresponding scatterplot is created.\nfor (column, ax) in zip(hp_df.iloc[:, 1:4], axes.flatten()):\n    sns.scatterplot(x=hp_df[column], y=hp_df['dice'], hue=hp_df['dice'], palette='viridis', ax=ax, legend=False)\n    ax.set_xlabel(column)\n    if hp_df[column].name in columns_to_log_scale:\n        ax.set_xscale('log')\n    ax.set_ylabel('Dice value')\n    ax.set_title(f'{column} vs Dice value')\n    \n# Adjust the design and show the figure\nplt.tight_layout()\nplt.show()","metadata":{"papermill":{"duration":0.980757,"end_time":"2024-06-14T23:48:50.496626","exception":false,"start_time":"2024-06-14T23:48:49.515869","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot a heatmap of the learning rate and q hyperparameters\nplt.pcolormesh(hp_df[['learning_rate', 'q']].values)\nplt.colorbar()  # Add a colorbar \nplt.show()\n\n# Plot the dice metric values over trials\nhp_df_1 = hp_df.sort_values(by='trial_id')\nplt.plot(hp_df_1['dice'].values, label='Dice Metric', linestyle='-', color='orange')\n\n# Add a legend to differentiate between the loss and dice metric plots\nplt.legend()\n\n# Display the line plots\nplt.show()","metadata":{"papermill":{"duration":0.554153,"end_time":"2024-06-14T23:48:52.176768","exception":false,"start_time":"2024-06-14T23:48:51.622615","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predictive GPR trained for the search space using embeddings as hyperparameter probabilities","metadata":{"papermill":{"duration":0.054348,"end_time":"2024-06-14T23:48:52.286828","exception":false,"start_time":"2024-06-14T23:48:52.232480","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Call the _vectorize_trials method from the tuner oracle\n# This method converts the hyperparameter configurations of trials into vector representations (X)\n# and pairs these vectors with their corresponding performance scores (y)\nX, y = tuner.oracle._vectorize_trials()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate 300 points evenly spaced between the minimum and maximum values of the first feature in X\n# Add a small buffer (0.00003) to the minimum and maximum to avoid edge effects\nhp_1 = np.linspace(X[:,0].min() - 0.00003, X[:,0].max() + 0.00003, 300)\n\n# Generate 300 points evenly spaced between the minimum and maximum values of the second feature in X\n# Add a small buffer (0.00003) to the minimum and maximum to avoid edge effects\nhp_2 = np.linspace(X[:,1].min() - 0.00003, X[:,1].max() + 0.00003, 300)","metadata":{"papermill":{"duration":0.065298,"end_time":"2024-06-14T23:48:54.137380","exception":false,"start_time":"2024-06-14T23:48:54.072082","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a meshgrid for the input space\nXx, Yy = np.meshgrid(hp_1, hp_2)\n\n# Stack the meshgrid arrays to create the input for prediction\nXY = np.vstack([Xx.ravel(), Yy.ravel()]).T\n\n# Make predictions using the Gaussian Process model\nmean, std = gp.predict(XY, return_std=True)  \n\n# Reshape the predictions to match the shape of the meshgrid\nmean = mean.reshape(Xx.shape)\nstd = std.reshape(Xx.shape)","metadata":{"papermill":{"duration":0.832511,"end_time":"2024-06-14T23:48:55.085711","exception":false,"start_time":"2024-06-14T23:48:54.253200","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the results\nplt.figure(figsize=(16, 7))\n\n# Heatmap of the mean prediction\nplt.subplot(1, 2, 1)\nplt.title('Mean Prediction') \nplt.xlabel('Learning rate')\nplt.ylabel('q')  \n# Plot the heatmap of the mean prediction using contourf\nheatmap1 = plt.contourf(hp_1, hp_2, mean, cmap='viridis', aspect='auto')  # Adjust the axis scaling\nplt.colorbar(heatmap1)  # Add a colorbar \nplt.scatter(X[:, 0], X[:, 1], c='white', s=50, edgecolors='k') # Scatter plot of the data points in white with black edges\n\n# Heatmap of the uncertainty (standard deviation)\nplt.subplot(1, 2, 2)\nplt.title('Uncertainty (Standard Deviation)')\nplt.xlabel('Learning rate')\nplt.ylabel('q')\n# Plot the heatmap of the standard deviation using contourf\nheatmap2 = plt.contourf(hp_1, hp_2, std, cmap='viridis', aspect='auto')  # Adjust the axis scaling\nplt.colorbar(heatmap2)  # Add a colorbar\nplt.scatter(X[:, 0], X[:, 1], c='white', s=50, edgecolors='k') # Scatter plot of the data points in white with black edges\n\n# Adjust the spacing between subplots to prevent overlap\nplt.tight_layout(pad=3.0)\n# Display the plots\nplt.show()","metadata":{"papermill":{"duration":0.838222,"end_time":"2024-06-14T23:48:56.032488","exception":false,"start_time":"2024-06-14T23:48:55.194266","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualizing the performance of the best model in a test image","metadata":{}},{"cell_type":"code","source":"# Get the best model found\nbest_model = tuner.get_best_models(num_models=1)[0]\n\n# Print the best hyperparameters\ntop_trials = tuner.oracle.get_best_trials(1)\n\nfor trial in top_trials:\n    print(f\"Trial {trial.trial_id}: Learning rate: {trial.hyperparameters.get('learning_rate')}, Power of q: {trial.hyperparameters.get('q')}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get model's prediction on test image\nbest_model_bayesian = tuner.get_best_models(num_models=1)[0]\nprediction = best_model_bayesian.predict(X_test[123:124])\nprediction = tf.where(prediction> 0.5, tf.ones_like(prediction), tf.zeros_like(prediction))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize the prediction\nplot_prediction(prediction,X_test,y_test,123,3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save model\nbest_model_bayesian.save('best_model_bayesian_sparsesoftmax.keras')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}