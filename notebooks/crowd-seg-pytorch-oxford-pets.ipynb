{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-07-12T20:07:24.506865Z","iopub.status.busy":"2024-07-12T20:07:24.505993Z","iopub.status.idle":"2024-07-12T20:07:51.615544Z","shell.execute_reply":"2024-07-12T20:07:51.614471Z","shell.execute_reply.started":"2024-07-12T20:07:24.506828Z"},"trusted":true},"outputs":[],"source":["%%capture\n","!pip install segmentation-models-pytorch==0.2.1 mlflow"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-07-12T20:07:51.617887Z","iopub.status.busy":"2024-07-12T20:07:51.617585Z","iopub.status.idle":"2024-07-12T20:08:06.280915Z","shell.execute_reply":"2024-07-12T20:08:06.280003Z","shell.execute_reply.started":"2024-07-12T20:07:51.617859Z"},"trusted":true},"outputs":[],"source":["# General Libraries\n","import os\n","import sys\n","import csv\n","import cv2\n","import yaml\n","import h5py\n","import errno\n","import shutil\n","import random\n","import mlflow\n","import pickle\n","import imageio\n","import inspect\n","import zipfile\n","import warnings\n","import argparse\n","import functools\n","import matplotlib\n","import collections\n","from collections.abc import Mapping \n","import numpy as np\n","import pandas as pd\n","matplotlib.use('Agg')\n","from PIL import Image\n","from typing import Dict\n","import albumentations as albu\n","from enum import auto, Enum\n","import matplotlib.pyplot as plt\n","from IPython.display import display, Image\n","\n","# Sklearn metrics Libraries\n","from sklearn.metrics import accuracy_score, jaccard_score\n","\n","# Pytorch Libraries\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils import data\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","from torchvision.utils import save_image\n","from torch.distributions import Normal, Independent, kl\n","from torch.utils.data.sampler import SubsetRandomSampler\n","from torch.utils.data import Dataset, DataLoader,TensorDataset\n","\n","# Segmentation Models on Pytorch Libraries\n","from segmentation_models_pytorch.losses import DiceLoss, FocalLoss\n","from segmentation_models_pytorch.encoders import get_preprocessing_fn"]},{"cell_type":"markdown","metadata":{},"source":["## Probalistic Unet in Pytorch library functions"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-07-12T20:08:06.282477Z","iopub.status.busy":"2024-07-12T20:08:06.282175Z","iopub.status.idle":"2024-07-12T20:08:06.298448Z","shell.execute_reply":"2024-07-12T20:08:06.297464Z","shell.execute_reply.started":"2024-07-12T20:08:06.282450Z"},"trusted":true},"outputs":[],"source":["class LIDC_IDRI(Dataset):\n","    images = []\n","    labels = []\n","    series_uid = []\n","\n","    def __init__(self, dataset_location, transform=None):\n","        self.transform = transform\n","        max_bytes = 2**31 - 1\n","        data = {}\n","        for file in os.listdir(dataset_location):\n","            filename = os.fsdecode(file)\n","            if '.pickle' in filename:\n","                print(\"Loading file\", filename)\n","                file_path = dataset_location + filename\n","                bytes_in = bytearray(0)\n","                input_size = os.path.getsize(file_path)\n","                with open(file_path, 'rb') as f_in:\n","                    for _ in range(0, input_size, max_bytes):\n","                        bytes_in += f_in.read(max_bytes)\n","                new_data = pickle.loads(bytes_in)\n","                data.update(new_data)\n","        \n","        for key, value in data.items():\n","            self.images.append(value['image'].astype(float))\n","            self.labels.append(value['masks'])\n","            self.series_uid.append(value['series_uid'])\n","\n","        assert (len(self.images) == len(self.labels) == len(self.series_uid))\n","\n","        for img in self.images:\n","            assert np.max(img) <= 1 and np.min(img) >= 0\n","        for label in self.labels:\n","            assert np.max(label) <= 1 and np.min(label) >= 0\n","\n","        del new_data\n","        del data\n","\n","    def __getitem__(self, index):\n","        image = np.expand_dims(self.images[index], axis=0)\n","\n","        #Randomly select one of the four labels for this image\n","        label = self.labels[index][random.randint(0,3)].astype(float)\n","        if self.transform is not None:\n","            image = self.transform(image)\n","\n","        series_uid = self.series_uid[index]\n","\n","        # Convert image and label to torch tensors\n","        image = torch.from_numpy(image)\n","        label = torch.from_numpy(label)\n","\n","        #Convert uint8 to float tensors\n","        image = image.type(torch.FloatTensor)\n","        label = label.type(torch.FloatTensor)\n","\n","        return image, label, series_uid\n","\n","    # Override to give PyTorch size of dataset\n","    def __len__(self):\n","        return len(self.images)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-07-12T20:08:06.302599Z","iopub.status.busy":"2024-07-12T20:08:06.301950Z","iopub.status.idle":"2024-07-12T20:08:06.437624Z","shell.execute_reply":"2024-07-12T20:08:06.436701Z","shell.execute_reply.started":"2024-07-12T20:08:06.302561Z"},"trusted":true},"outputs":[],"source":["\n","class Encoder(nn.Module):\n","    \"\"\"\n","    A convolutional neural network, consisting of len(num_filters) times a block of no_convs_per_block convolutional layers,\n","    after each block a pooling operation is performed. And after each convolutional layer a non-linear (ReLU) activation function is applied.\n","    \"\"\"\n","    def __init__(self, input_channels, num_filters, no_convs_per_block, initializers, padding=True, posterior=False):\n","        super(Encoder, self).__init__()\n","        self.contracting_path = nn.ModuleList()\n","        self.input_channels = input_channels\n","        self.num_filters = num_filters\n","\n","        if posterior:\n","            #To accomodate for the mask that is concatenated at the channel axis, we increase the input_channels.\n","            self.input_channels += 1\n","\n","        layers = []\n","        for i in range(len(self.num_filters)):\n","            \"\"\"\n","            Determine input_dim and output_dim of conv layers in this block. The first layer is input x output,\n","            All the subsequent layers are output x output.\n","            \"\"\"\n","            input_dim = self.input_channels if i == 0 else output_dim\n","            output_dim = num_filters[i]\n","            \n","            if i != 0:\n","                layers.append(nn.AvgPool2d(kernel_size=2, stride=2, padding=0, ceil_mode=True))\n","            \n","            layers.append(nn.Conv2d(input_dim, output_dim, kernel_size=3, padding=int(padding)))\n","            layers.append(nn.ReLU(inplace=True))\n","\n","            for _ in range(no_convs_per_block-1):\n","                layers.append(nn.Conv2d(output_dim, output_dim, kernel_size=3, padding=int(padding)))\n","                layers.append(nn.ReLU(inplace=True))\n","\n","        self.layers = nn.Sequential(*layers)\n","\n","        self.layers.apply(init_weights)\n","\n","    def forward(self, input):\n","        output = self.layers(input)\n","        return output\n","\n","class AxisAlignedConvGaussian(nn.Module):\n","    \"\"\"\n","    A convolutional net that parametrizes a Gaussian distribution with axis aligned covariance matrix.\n","    \"\"\"\n","    def __init__(self, input_channels, num_filters, no_convs_per_block, latent_dim, initializers, posterior=False):\n","        super(AxisAlignedConvGaussian, self).__init__()\n","        self.input_channels = input_channels\n","        self.channel_axis = 1\n","        self.num_filters = num_filters\n","        self.no_convs_per_block = no_convs_per_block\n","        self.latent_dim = latent_dim\n","        self.posterior = posterior\n","        if self.posterior:\n","            self.name = 'Posterior'\n","        else:\n","            self.name = 'Prior'\n","        self.encoder = Encoder(self.input_channels, self.num_filters, self.no_convs_per_block, initializers, posterior=self.posterior)\n","        self.conv_layer = nn.Conv2d(num_filters[-1], 2 * self.latent_dim, (1,1), stride=1)\n","        self.show_img = 0\n","        self.show_seg = 0\n","        self.show_concat = 0\n","        self.show_enc = 0\n","        self.sum_input = 0\n","\n","        nn.init.kaiming_normal_(self.conv_layer.weight, mode='fan_in', nonlinearity='relu')\n","        nn.init.normal_(self.conv_layer.bias)\n","\n","    def forward(self, input, segm=None):\n","\n","        #If segmentation is not none, concatenate the mask to the channel axis of the input\n","        if segm is not None:\n","            self.show_img = input\n","            self.show_seg = segm\n","            input = torch.cat((input, segm), dim=1)\n","            self.show_concat = input\n","            self.sum_input = torch.sum(input)\n","\n","        encoding = self.encoder(input)\n","        self.show_enc = encoding\n","\n","        #We only want the mean of the resulting hxw image\n","        encoding = torch.mean(encoding, dim=2, keepdim=True)\n","        encoding = torch.mean(encoding, dim=3, keepdim=True)\n","\n","        #Convert encoding to 2 x latent dim and split up for mu and log_sigma\n","        mu_log_sigma = self.conv_layer(encoding)\n","\n","        #We squeeze the second dimension twice, since otherwise it won't work when batch size is equal to 1\n","        mu_log_sigma = torch.squeeze(mu_log_sigma, dim=2)\n","        mu_log_sigma = torch.squeeze(mu_log_sigma, dim=2)\n","\n","        mu = mu_log_sigma[:,:self.latent_dim]\n","        log_sigma = mu_log_sigma[:,self.latent_dim:]\n","\n","        #This is a multivariate normal with diagonal covariance matrix sigma\n","        #https://github.com/pytorch/pytorch/pull/11178\n","        dist = Independent(Normal(loc=mu, scale=torch.exp(log_sigma)),1)\n","        return dist\n","\n","class Fcomb(nn.Module):\n","    \"\"\"\n","    A function composed of no_convs_fcomb times a 1x1 convolution that combines the sample taken from the latent space,\n","    and output of the UNet (the feature map) by concatenating them along their channel axis.\n","    \"\"\"\n","    def __init__(self, num_filters, latent_dim, num_output_channels, num_classes, no_convs_fcomb, initializers, use_tile=True):\n","        super(Fcomb, self).__init__()\n","        self.num_channels = num_output_channels #output channels\n","        self.num_classes = num_classes\n","        self.channel_axis = 1\n","        self.spatial_axes = [2,3]\n","        self.num_filters = num_filters\n","        self.latent_dim = latent_dim\n","        self.use_tile = use_tile\n","        self.no_convs_fcomb = no_convs_fcomb \n","        self.name = 'Fcomb'\n","\n","        if self.use_tile:\n","            layers = []\n","\n","            #Decoder of N x a 1x1 convolution followed by a ReLU activation function except for the last layer\n","            layers.append(nn.Conv2d(self.num_filters[0]+self.latent_dim, self.num_filters[0], kernel_size=1))\n","            layers.append(nn.ReLU(inplace=True))\n","\n","            for _ in range(no_convs_fcomb-2):\n","                layers.append(nn.Conv2d(self.num_filters[0], self.num_filters[0], kernel_size=1))\n","                layers.append(nn.ReLU(inplace=True))\n","\n","            self.layers = nn.Sequential(*layers)\n","\n","            self.last_layer = nn.Conv2d(self.num_filters[0], self.num_classes, kernel_size=1)\n","\n","            if initializers['w'] == 'orthogonal':\n","                self.layers.apply(init_weights_orthogonal_normal)\n","                self.last_layer.apply(init_weights_orthogonal_normal)\n","            else:\n","                self.layers.apply(init_weights)\n","                self.last_layer.apply(init_weights)\n","\n","    def tile(self, a, dim, n_tile):\n","        \"\"\"\n","        This function is taken form PyTorch forum and mimics the behavior of tf.tile.\n","        Source: https://discuss.pytorch.org/t/how-to-tile-a-tensor/13853/3\n","        \"\"\"\n","        init_dim = a.size(dim)\n","        repeat_idx = [1] * a.dim()\n","        repeat_idx[dim] = n_tile\n","        a = a.repeat(*(repeat_idx))\n","        order_index = torch.LongTensor(np.concatenate([init_dim * np.arange(n_tile) + i for i in range(init_dim)])).to(device)\n","        return torch.index_select(a, dim, order_index)\n","\n","    def forward(self, feature_map, z):\n","        \"\"\"\n","        Z is batch_sizexlatent_dim and feature_map is batch_sizexno_channelsxHxW.\n","        So broadcast Z to batch_sizexlatent_dimxHxW. Behavior is exactly the same as tf.tile (verified)\n","        \"\"\"\n","        if self.use_tile:\n","            z = torch.unsqueeze(z,2)\n","            z = self.tile(z, 2, feature_map.shape[self.spatial_axes[0]])\n","            z = torch.unsqueeze(z,3)\n","            z = self.tile(z, 3, feature_map.shape[self.spatial_axes[1]])\n","\n","            #Concatenate the feature map (output of the UNet) and the sample taken from the latent space\n","            feature_map = torch.cat((feature_map, z), dim=self.channel_axis)\n","            output = self.layers(feature_map)\n","            return self.last_layer(output)\n","\n","\n","class ProbabilisticUnet(nn.Module):\n","    \"\"\"\n","    A probabilistic UNet (https://arxiv.org/abs/1806.05034) implementation.\n","    input_channels: the number of channels in the image (1 for greyscale and 3 for RGB)\n","    num_classes: the number of classes to predict\n","    num_filters: is a list consisint of the amount of filters layer\n","    latent_dim: dimension of the latent space\n","    no_cons_per_block: no convs per block in the (convolutional) encoder of prior and posterior\n","    \"\"\"\n","\n","    def __init__(self, input_channels=1, num_classes=1, num_filters=[32,64,128,192], latent_dim=6, no_convs_fcomb=4, beta=10.0):\n","        super(ProbabilisticUnet, self).__init__()\n","        self.input_channels = input_channels\n","        self.num_classes = num_classes\n","        self.num_filters = num_filters\n","        self.latent_dim = latent_dim\n","        self.no_convs_per_block = 3\n","        self.no_convs_fcomb = no_convs_fcomb\n","        self.initializers = {'w':'he_normal', 'b':'normal'}\n","        self.beta = beta\n","        self.z_prior_sample = 0\n","\n","        self.unet = Unet(self.input_channels, self.num_classes, self.num_filters, self.initializers, apply_last_layer=False, padding=True).to(device)\n","        self.prior = AxisAlignedConvGaussian(self.input_channels, self.num_filters, self.no_convs_per_block, self.latent_dim,  self.initializers,).to(device)\n","        self.posterior = AxisAlignedConvGaussian(self.input_channels, self.num_filters, self.no_convs_per_block, self.latent_dim, self.initializers, posterior=True).to(device)\n","        self.fcomb = Fcomb(self.num_filters, self.latent_dim, self.input_channels, self.num_classes, self.no_convs_fcomb, {'w':'orthogonal', 'b':'normal'}, use_tile=True).to(device)\n","\n","    def forward(self, patch, segm, training=True):\n","        \"\"\"\n","        Construct prior latent space for patch and run patch through UNet,\n","        in case training is True also construct posterior latent space\n","        \"\"\"\n","        if training:\n","            self.posterior_latent_space = self.posterior.forward(patch, segm)\n","        self.prior_latent_space = self.prior.forward(patch)\n","        self.unet_features = self.unet.forward(patch,False)\n","\n","    def sample(self, testing=False):\n","        \"\"\"\n","        Sample a segmentation by reconstructing from a prior sample\n","        and combining this with UNet features\n","        \"\"\"\n","        if testing == False:\n","            z_prior = self.prior_latent_space.rsample()\n","            self.z_prior_sample = z_prior\n","        else:\n","            #You can choose whether you mean a sample or the mean here. For the GED it is important to take a sample.\n","            #z_prior = self.prior_latent_space.base_dist.loc \n","            z_prior = self.prior_latent_space.sample()\n","            self.z_prior_sample = z_prior\n","        return self.fcomb.forward(self.unet_features,z_prior)\n","\n","\n","    def reconstruct(self, use_posterior_mean=False, calculate_posterior=False, z_posterior=None):\n","        \"\"\"\n","        Reconstruct a segmentation from a posterior sample (decoding a posterior sample) and UNet feature map\n","        use_posterior_mean: use posterior_mean instead of sampling z_q\n","        calculate_posterior: use a provided sample or sample from posterior latent space\n","        \"\"\"\n","        if use_posterior_mean:\n","            z_posterior = self.posterior_latent_space.loc\n","        else:\n","            if calculate_posterior:\n","                z_posterior = self.posterior_latent_space.rsample()\n","        return self.fcomb.forward(self.unet_features, z_posterior)\n","\n","    def kl_divergence(self, analytic=True, calculate_posterior=False, z_posterior=None):\n","        \"\"\"\n","        Calculate the KL divergence between the posterior and prior KL(Q||P)\n","        analytic: calculate KL analytically or via sampling from the posterior\n","        calculate_posterior: if we use samapling to approximate KL we can sample here or supply a sample\n","        \"\"\"\n","        if analytic:\n","            #Neeed to add this to torch source code, see: https://github.com/pytorch/pytorch/issues/13545\n","            kl_div = kl.kl_divergence(self.posterior_latent_space, self.prior_latent_space)\n","        else:\n","            if calculate_posterior:\n","                z_posterior = self.posterior_latent_space.rsample()\n","            log_posterior_prob = self.posterior_latent_space.log_prob(z_posterior)\n","            log_prior_prob = self.prior_latent_space.log_prob(z_posterior)\n","            kl_div = log_posterior_prob - log_prior_prob\n","        return kl_div\n","\n","    def elbo(self, segm, analytic_kl=True, reconstruct_posterior_mean=False):\n","        \"\"\"\n","        Calculate the evidence lower bound of the log-likelihood of P(Y|X)\n","        \"\"\"\n","\n","        criterion = nn.BCEWithLogitsLoss(size_average = False, reduce=False, reduction=None)\n","        z_posterior = self.posterior_latent_space.rsample()\n","        \n","        self.kl = torch.mean(self.kl_divergence(analytic=analytic_kl, calculate_posterior=False, z_posterior=z_posterior))\n","\n","        #Here we use the posterior sample sampled above\n","        self.reconstruction = self.reconstruct(use_posterior_mean=reconstruct_posterior_mean, calculate_posterior=False, z_posterior=z_posterior)\n","        \n","        reconstruction_loss = criterion(input=self.reconstruction, target=segm)\n","        self.reconstruction_loss = torch.sum(reconstruction_loss)\n","        self.mean_reconstruction_loss = torch.mean(reconstruction_loss)\n","\n","        return -(self.reconstruction_loss + self.beta * self.kl)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-07-12T20:08:06.439661Z","iopub.status.busy":"2024-07-12T20:08:06.439311Z","iopub.status.idle":"2024-07-12T20:08:06.456131Z","shell.execute_reply":"2024-07-12T20:08:06.455253Z","shell.execute_reply.started":"2024-07-12T20:08:06.439630Z"},"trusted":true},"outputs":[],"source":["class Unet(nn.Module):\n","    \"\"\"\n","    A UNet (https://arxiv.org/abs/1505.04597) implementation.\n","    input_channels: the number of channels in the image (1 for greyscale and 3 for RGB)\n","    num_classes: the number of classes to predict\n","    num_filters: list with the amount of filters per layer\n","    apply_last_layer: boolean to apply last layer or not (not used in Probabilistic UNet)\n","    padidng: Boolean, if true we pad the images with 1 so that we keep the same dimensions\n","    \"\"\"\n","\n","    def __init__(self, input_channels, num_classes, num_filters, initializers, apply_last_layer=True, padding=True):\n","        super(Unet, self).__init__()\n","        self.input_channels = input_channels\n","        self.num_classes = num_classes\n","        self.num_filters = num_filters\n","        self.padding = padding\n","        self.activation_maps = []\n","        self.apply_last_layer = apply_last_layer\n","        self.contracting_path = nn.ModuleList()\n","\n","        for i in range(len(self.num_filters)):\n","            input = self.input_channels if i == 0 else output\n","            output = self.num_filters[i]\n","\n","            if i == 0:\n","                pool = False\n","            else:\n","                pool = True\n","\n","            self.contracting_path.append(DownConvBlock(input, output, initializers, padding, pool=pool))\n","\n","        self.upsampling_path = nn.ModuleList()\n","\n","        n = len(self.num_filters) - 2\n","        for i in range(n, -1, -1):\n","            input = output + self.num_filters[i]\n","            output = self.num_filters[i]\n","            self.upsampling_path.append(UpConvBlock(input, output, initializers, padding))\n","\n","        if self.apply_last_layer:\n","            self.last_layer = nn.Conv2d(output, num_classes, kernel_size=1)\n","            #nn.init.kaiming_normal_(self.last_layer.weight, mode='fan_in',nonlinearity='relu')\n","            #nn.init.normal_(self.last_layer.bias)\n","\n","\n","    def forward(self, x, val):\n","        blocks = []\n","        for i, down in enumerate(self.contracting_path):\n","            x = down(x)\n","            if i != len(self.contracting_path)-1:\n","                blocks.append(x)\n","\n","        for i, up in enumerate(self.upsampling_path):\n","            x = up(x, blocks[-i-1])\n","\n","        del blocks\n","\n","        #Used for saving the activations and plotting\n","        if val:\n","            self.activation_maps.append(x)\n","        \n","        if self.apply_last_layer:\n","            x =  self.last_layer(x)\n","\n","        return x"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-07-12T20:08:06.457674Z","iopub.status.busy":"2024-07-12T20:08:06.457331Z","iopub.status.idle":"2024-07-12T20:08:06.472946Z","shell.execute_reply":"2024-07-12T20:08:06.471865Z","shell.execute_reply.started":"2024-07-12T20:08:06.457643Z"},"trusted":true},"outputs":[],"source":["class DownConvBlock(nn.Module):\n","    \"\"\"\n","    A block of three convolutional layers where each layer is followed by a non-linear activation function\n","    Between each block we add a pooling operation.\n","    \"\"\"\n","    def __init__(self, input_dim, output_dim, initializers, padding, pool=True):\n","        super(DownConvBlock, self).__init__()\n","        layers = []\n","\n","        if pool:\n","            layers.append(nn.AvgPool2d(kernel_size=2, stride=2, padding=0, ceil_mode=True))\n","\n","        layers.append(nn.Conv2d(input_dim, output_dim, kernel_size=3, stride=1, padding=int(padding)))\n","        layers.append(nn.ReLU(inplace=True))\n","        layers.append(nn.Conv2d(output_dim, output_dim, kernel_size=3, stride=1, padding=int(padding)))\n","        layers.append(nn.ReLU(inplace=True))\n","        layers.append(nn.Conv2d(output_dim, output_dim, kernel_size=3, stride=1, padding=int(padding)))\n","        layers.append(nn.ReLU(inplace=True))\n","\n","        self.layers = nn.Sequential(*layers)\n","\n","        self.layers.apply(init_weights)\n","\n","    def forward(self, patch):\n","        return self.layers(patch)\n","\n","\n","class UpConvBlock(nn.Module):\n","    \"\"\"\n","    A block consists of an upsampling layer followed by a convolutional layer to reduce the amount of channels and then a DownConvBlock\n","    If bilinear is set to false, we do a transposed convolution instead of upsampling\n","    \"\"\"\n","    def __init__(self, input_dim, output_dim, initializers, padding, bilinear=True):\n","        super(UpConvBlock, self).__init__()\n","        self.bilinear = bilinear\n","\n","        if not self.bilinear:\n","            self.upconv_layer = nn.ConvTranspose2d(input_dim, output_dim, kernel_size=2, stride=2)\n","            self.upconv_layer.apply(init_weights)\n","\n","        self.conv_block = DownConvBlock(input_dim, output_dim, initializers, padding, pool=False)\n","\n","    def forward(self, x, bridge):\n","        if self.bilinear:\n","            up = nn.functional.interpolate(x, mode='bilinear', scale_factor=2, align_corners=True)\n","        else:\n","            up = self.upconv_layer(x)\n","        \n","        assert up.shape[3] == bridge.shape[3]\n","        out = torch.cat([up, bridge], 1)\n","        out =  self.conv_block(out)\n","\n","        return out"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-07-12T20:08:06.475017Z","iopub.status.busy":"2024-07-12T20:08:06.474278Z","iopub.status.idle":"2024-07-12T20:08:06.488725Z","shell.execute_reply":"2024-07-12T20:08:06.487739Z","shell.execute_reply.started":"2024-07-12T20:08:06.474982Z"},"trusted":true},"outputs":[],"source":["def truncated_normal_(tensor, mean=0, std=1):\n","    size = tensor.shape\n","    tmp = tensor.new_empty(size + (4,)).normal_()\n","    valid = (tmp < 2) & (tmp > -2)\n","    ind = valid.max(-1, keepdim=True)[1]\n","    tensor.data.copy_(tmp.gather(-1, ind).squeeze(-1))\n","    tensor.data.mul_(std).add_(mean)\n","\n","def init_weights(m):\n","    if type(m) == nn.Conv2d or type(m) == nn.ConvTranspose2d:\n","        nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n","        #nn.init.normal_(m.weight, std=0.001)\n","        #nn.init.normal_(m.bias, std=0.001)\n","        truncated_normal_(m.bias, mean=0, std=0.001)\n","\n","def init_weights_orthogonal_normal(m):\n","    if type(m) == nn.Conv2d or type(m) == nn.ConvTranspose2d:\n","        nn.init.orthogonal_(m.weight)\n","        truncated_normal_(m.bias, mean=0, std=0.001)\n","        #nn.init.normal_(m.bias, std=0.001)\n","\n","def l2_regularisation(m):\n","    l2_reg = None\n","\n","    for W in m.parameters():\n","        if l2_reg is None:\n","            l2_reg = W.norm(2)\n","        else:\n","            l2_reg = l2_reg + W.norm(2)\n","    return l2_reg\n","\n","def save_mask_prediction_example(mask, pred, iter):\n","    plt.imshow(pred[0,:,:],cmap='Greys')\n","    plt.savefig('images/'+str(iter)+\"_prediction.png\")\n","    plt.imshow(mask[0,:,:],cmap='Greys')\n","    plt.savefig('images/'+str(iter)+\"_mask.png\")"]},{"cell_type":"markdown","metadata":{},"source":["## Crowds for automated histopathological image segmentation functions"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-07-12T20:08:06.490249Z","iopub.status.busy":"2024-07-12T20:08:06.489973Z","iopub.status.idle":"2024-07-12T20:08:06.537462Z","shell.execute_reply":"2024-07-12T20:08:06.536673Z","shell.execute_reply.started":"2024-07-12T20:08:06.490214Z"},"trusted":true},"outputs":[],"source":["def double_conv(in_channels, out_channels, step, norm):\n","    # ===========================================\n","    # in_channels: dimension of input\n","    # out_channels: dimension of output\n","    # step: stride\n","    # ===========================================\n","    if norm == 'in':\n","        return torch.nn.Sequential(\n","            torch.nn.Conv2d(in_channels, out_channels, 3, stride=step, padding=1, groups=1, bias=False),\n","            torch.nn.InstanceNorm2d(out_channels, affine=True),\n","            torch.nn.PReLU(),\n","            torch.nn.Conv2d(out_channels, out_channels, 3, stride=1, padding=1, groups=1, bias=False),\n","            torch.nn.InstanceNorm2d(out_channels, affine=True),\n","            torch.nn.PReLU()\n","        )\n","    elif norm == 'bn':\n","        return torch.nn.Sequential(\n","            torch.nn.Conv2d(in_channels, out_channels, 3, stride=step, padding=1, groups=1, bias=False),\n","            torch.nn.BatchNorm2d(out_channels, affine=True),\n","            torch.nn.PReLU(),\n","            torch.nn.Conv2d(out_channels, out_channels, 3, stride=1, padding=1, groups=1, bias=False),\n","            torch.nn.BatchNorm2d(out_channels, affine=True),\n","            torch.nn.PReLU()\n","        )\n","    elif norm == 'ln':\n","        return torch.nn.Sequential(\n","            torch.nn.Conv2d(in_channels, out_channels, 3, stride=step, padding=1, groups=1, bias=False),\n","            torch.nn.GroupNorm(out_channels, out_channels, affine=True),\n","            torch.nn.PReLU(),\n","            torch.nn.Conv2d(out_channels, out_channels, 3, stride=1, padding=1, groups=1, bias=False),\n","            torch.nn.GroupNorm(out_channels, out_channels, affine=True),\n","            torch.nn.PReLU()\n","        )\n","    elif norm == 'gn':\n","        return torch.nn.Sequential(\n","            torch.nn.Conv2d(in_channels, out_channels, 3, stride=step, padding=1, groups=1, bias=False),\n","            torch.nn.GroupNorm(out_channels // 8, out_channels, affine=True),\n","            torch.nn.PReLU(),\n","            torch.nn.Conv2d(out_channels, out_channels, 3, stride=1, padding=1, groups=1, bias=False),\n","            torch.nn.GroupNorm(out_channels // 8, out_channels, affine=True),\n","            torch.nn.PReLU()\n","        )\n","\n","\n","\n","class global_CM(torch.nn.Module):\n","    \"\"\" This defines the annotator network (CR Global)\n","    \"\"\"\n","\n","    def __init__(self, class_no, input_height, input_width, noisy_labels_no):\n","        super(global_CM, self).__init__()\n","        self.class_no = class_no\n","        self.noisy_labels_no = noisy_labels_no\n","        self.input_height = input_height\n","        self.input_width = input_width\n","        self.noisy_labels_no = noisy_labels_no\n","        self.dense_output = torch.nn.Linear(noisy_labels_no, class_no ** 2)\n","        self.act = torch.nn.Softplus()\n","        # self.relu = torch.nn.ReLU()\n","\n","    def forward(self, A_id, x=None):\n","        output = self.act(self.dense_output(A_id))\n","        all_weights = output.unsqueeze(-1).unsqueeze(-1).repeat(1, 1, 1, 512, 512)\n","        y = all_weights.view(-1, self.class_no**2, self.input_height, self.input_width)\n","\n","\n","\n","        return y\n","\n","\n","class conv_layers_image(torch.nn.Module):\n","    def __init__(self, in_channels):\n","        super(conv_layers_image, self).__init__()\n","        self.conv = torch.nn.Conv2d(in_channels=in_channels, out_channels=8, kernel_size=3, stride=1, padding=1)\n","        self.conv2 = torch.nn.Conv2d(in_channels=8, out_channels=4, kernel_size=3, stride=1, padding=1)\n","        self.conv3 = torch.nn.Conv2d(in_channels=4, out_channels=4, kernel_size=3, stride=1, padding=1)\n","        self.relu = torch.nn.ReLU()\n","        self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.conv_bn = torch.nn.BatchNorm2d(8)\n","        self.conv_bn2 = torch.nn.BatchNorm2d(4)\n","        self.fc_bn = torch.nn.BatchNorm1d(128)\n","        self.flatten = torch.nn.Flatten()\n","        self.fc1 = torch.nn.Linear(in_features=4096, out_features=128)\n","        self.fc2 = torch.nn.Linear(in_features=128, out_features=64)\n","\n","    def forward(self, x):\n","        x = self.pool(self.relu(self.conv_bn(self.conv(x))))\n","        x = self.pool(self.relu(self.conv_bn2(self.conv2(x))))\n","        x = self.pool(self.relu(self.conv_bn2(self.conv3(x))))\n","        x = self.pool(self.relu(self.conv_bn2(self.conv3(x))))\n","        x = self.flatten(x)\n","\n","        x = self.relu(self.fc_bn(self.fc1(x)))\n","        y = self.fc2(x)\n","\n","        return y\n","\n","\n","class image_CM(torch.nn.Module):\n","    \"\"\" This defines the annotator network (CR Image)\n","    \"\"\"\n","\n","    def __init__(self, class_no, input_height, input_width, noisy_labels_no):\n","        super(image_CM, self).__init__()\n","        self.class_no = class_no\n","        self.noisy_labels_no = noisy_labels_no\n","        self.input_height = input_height\n","        self.input_width = input_width\n","        self.noisy_labels_no = noisy_labels_no\n","        self.conv_layers = conv_layers_image(16)\n","        self.dense_annotator = torch.nn.Linear(noisy_labels_no, 64)\n","        self.dense_output = torch.nn.Linear(128, class_no ** 2)\n","        self.norm = torch.nn.BatchNorm1d(class_no ** 2)\n","        self.act = torch.nn.Softplus()\n","\n","    def forward(self, A_id, x):\n","        A_feat = self.dense_annotator(A_id)  # B, F_A\n","        x = self.conv_layers(x)\n","        output = self.dense_output(torch.hstack((A_feat, x)))\n","        output = self.norm(output)\n","        output = self.act(output.view(-1, self.class_no, self.class_no))\n","        all_weights = output.unsqueeze(-1).unsqueeze(-1).repeat(1, 1, 1, self.input_height, self.input_width)\n","        y = all_weights.view(-1, self.class_no**2, self.input_height, self.input_width)\n","\n","        return y\n","\n","\n","\n","class cm_layers(torch.nn.Module):\n","    \"\"\" This defines the annotator network (CR Pixel)\n","    \"\"\"\n","\n","    def __init__(self, in_channels, norm, class_no, noisy_labels_no):\n","        super(cm_layers, self).__init__()\n","        self.conv_1 = double_conv(in_channels=in_channels, out_channels=in_channels, norm=norm, step=1)\n","        self.conv_2 = double_conv(in_channels=in_channels, out_channels=in_channels, norm=norm, step=1)\n","        # self.conv_last = torch.nn.Conv2d(in_channels, class_no ** 2, 1, bias=True)\n","        self.class_no = class_no\n","        self.dense = torch.nn.Linear(80, 25)\n","        self.dense2 = torch.nn.Linear(25, 25)\n","        self.dense_annotator = torch.nn.Linear(noisy_labels_no, 64)\n","        # self.dense_classes = torch.nn.Linear(noisy_labels_no, 50)\n","        self.norm = torch.nn.BatchNorm2d(80, affine=True)\n","        self.relu = torch.nn.Softplus()\n","        self.act = torch.nn.Softmax(dim=3)\n","\n","    def forward(self, A_id, x):\n","        print('################################################4\\n')\n","        y = self.conv_2(self.conv_1(x))\n","        print(f'y shape: {y.shape}\\n')\n","        A_id = self.relu(self.dense_annotator(A_id))  # B, F_A\n","        print(f'A_id shape: {A_id.shape}\\n')\n","        A_id = A_id.unsqueeze(-1).unsqueeze(-1).repeat(1, 1, 512, 512)\n","        print(f'A_id shape: {A_id.shape}\\n')\n","        \n","        y = torch.cat((A_id, y), dim=1)\n","        print(f'y shape: {y.shape}\\n')\n","        y = self.norm(y)\n","        print(f'y shape: {y.shape}\\n')\n","        y = y.permute(0, 2, 3, 1)\n","        print(f'y shape: {y.shape}\\n')\n","        y = self.relu((self.dense(y)))\n","        print(f'y shape: {y.shape}\\n')\n","        y = self.dense2(y)\n","        print(f'y shape: {y.shape}\\n')\n","        \n","        # Verificar el tamaÃ±o actual del tensor\n","        print(f\"Current tensor size before view: {y.size()}\")\n","\n","        # Calcular la nueva forma\n","        print(self.class_no)\n","        expected_shape = (-1, 512, 512, 5 * self.class_no)  # Adjust shape to match actual tensor size\n","        expected_size = torch.prod(torch.tensor(expected_shape[1:])).item()  # Calculate expected size\n","\n","        if y.numel() == expected_size:\n","            y = self.relu(y.view(expected_shape))\n","        else:\n","            raise RuntimeError(f\"Expected size {expected_size} but got {y.numel()}\")\n","\n","        y = y.view(-1, 512, 512, self.class_no ** 2).permute(0, 3, 1, 2)\n","\n","        return y\n","    \n","class Crowd_segmentationModel(torch.nn.Module):\n","    \"\"\" This defines the architecture of the chosen CR method\n","    \"\"\"\n","    def __init__(self, noisy_labels):\n","        super().__init__()\n","        self.seg_model = create_segmentation_backbone()\n","        self.activation = torch.nn.Softmax(dim=1)\n","        self.noisy_labels_no = len(noisy_labels)\n","        print(\"Number of annotators (model): \", self.noisy_labels_no)\n","        self.class_no = config['data']['class_no']\n","        self.crowd_type = config['model']['crowd_type']\n","        if self.crowd_type == 'global':\n","            print(\"Global crowdsourcing\")\n","            self.crowd_layers = global_CM(self.class_no, 512, 512, self.noisy_labels_no)\n","        elif self.crowd_type == 'image': \n","            print(\"Image dependent crowdsourcing\")\n","            self.crowd_layers = image_CM(self.class_no, 512, 512, self.noisy_labels_no)\n","        elif self.crowd_type == 'pixel':\n","            print(\"Pixel dependent crowdsourcing\")\n","            self.crowd_layers = cm_layers(in_channels=16, norm='in',\n","                                                  class_no=config['data']['class_no'], noisy_labels_no=self.noisy_labels_no)  \n","        self.activation = torch.nn.Softmax(dim=1)\n","\n","    def forward(self, x, A_id=None):\n","        cm = None\n","        #print('################################################3\\n')\n","        #print(f'1nd x shape: {x.shape} \\n')\n","        x = self.seg_model.encoder(x)\n","        #print(f'2nd x len: {len(x)}\\nshape 0 {x[0].shape}\\nshape 1 {x[1].shape}\\nshape 2 {x[2].shape}\\nshape 3 {x[3].shape}\\nshape 4 {x[4].shape}\\nshape 5 {x[5].shape}')\n","        x = self.seg_model.decoder(*x)\n","        if A_id is not None:\n","            #print(f'A_id shape: {A_id.shape}, 3nd x shape: {x.shape} \\n')\n","            cm = self.crowd_layers(A_id, x)\n","        x = self.seg_model.segmentation_head(x)\n","        y = self.activation(x)\n","        return y, cm"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-07-12T20:08:06.538819Z","iopub.status.busy":"2024-07-12T20:08:06.538563Z","iopub.status.idle":"2024-07-12T20:08:06.554389Z","shell.execute_reply":"2024-07-12T20:08:06.553651Z","shell.execute_reply.started":"2024-07-12T20:08:06.538797Z"},"trusted":true},"outputs":[],"source":["config = {}\n","\n","def config_update(orig_dict, new_dict):\n","    for key, val in new_dict.items():\n","        if isinstance(val, Mapping):\n","            tmp = config_update(orig_dict.get(key, { }), val)\n","            orig_dict[key] = tmp\n","        elif isinstance(val, list):\n","            orig_dict[key] = val\n","        else:\n","            orig_dict[key] = new_dict[key]\n","    return orig_dict\n","\n","def init_global_config(args):\n","    global config\n","\n","    # load default config\n","    with open(args.default_config) as file:\n","        config = yaml.full_load(file)\n","\n","    # load dataset config, overwrite parameters if double\n","    with open(config[\"data\"][\"dataset_config\"]) as file:\n","        config_data_dependent = yaml.full_load(file)\n","    config = config_update(config, config_data_dependent)\n","\n","    # load experiment config, overwrite parameters if double\n","    if args.experiment_folder != 'None':\n","        experiment_config = os.path.join(args.experiment_folder, 'exp_config.yaml')\n","        if os.path.exists(experiment_config):\n","            with open(experiment_config) as file:\n","                exp_config = yaml.full_load(file)\n","            config = config_update(config, exp_config)\n","        config['logging']['experiment_folder'] = args.experiment_folder\n","        if config['data']['crowd']:\n","            exp_fold = args.experiment_folder.split(\"/\")[-3:]\n","        else:\n","            exp_fold = args.experiment_folder.split(\"/\")[-2:]\n","        exp_fold = \"_\".join(exp_fold)\n","        config['logging']['run_name'] = exp_fold\n","    else:\n","        out_dir = './output/'\n","        os.makedirs(out_dir, exist_ok=True)\n","        warnings.warn(\"No experiment folder was given. Use ./output folder to store experiment results.\")\n","        config['logging']['experiment_folder'] = out_dir\n","        config['logging']['run_name'] = 'default'"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-07-12T20:08:06.557504Z","iopub.status.busy":"2024-07-12T20:08:06.557134Z","iopub.status.idle":"2024-07-12T20:08:06.572220Z","shell.execute_reply":"2024-07-12T20:08:06.571554Z","shell.execute_reply.started":"2024-07-12T20:08:06.557480Z"},"trusted":true},"outputs":[],"source":["def start_logging():\n","    mlflow.set_tracking_uri(config[\"logging\"][\"mlruns_folder\"])\n","\n","    data_config_log = config['data'].copy()\n","    data_config_log.pop('visualize_images') # drop this because it is often to long to be logged\n","\n","    # experiment = mlflow.set_experiment(experiment_name=config[\"data\"][\"dataset_name\"])\n","    mlflow.set_experiment(experiment_name=config[\"data\"][\"dataset_name\"])\n","    # with mlflow.start_run(experiment_id=experiment.experiment_id, run_name='test') as run:\n","    # mlflow.start_run(experiment_id=experiment.experiment_id, run_name='test')\n","    mlflow.start_run(run_name=config['logging']['run_name'])\n","    print('tracking uri:', mlflow.get_tracking_uri())\n","    print('artifact uri:', mlflow.get_artifact_uri())\n","    mlflow.log_params(config['model'])\n","    mlflow.log_params(data_config_log)\n","    mlflow.log_artifact('/kaggle/working/OxfordPet/config.yaml')\n","\n","\n","def log_results(results, mode, step=None):\n","\n","    formatted_results = {}\n","\n","    for key in results.keys():\n","        new_key = mode + '_' + key\n","        formatted_results[new_key] = results[key]\n","\n","    mlflow.log_metrics(formatted_results, step=step)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-07-12T20:08:06.573554Z","iopub.status.busy":"2024-07-12T20:08:06.573268Z","iopub.status.idle":"2024-07-12T20:08:06.590062Z","shell.execute_reply":"2024-07-12T20:08:06.589276Z","shell.execute_reply.started":"2024-07-12T20:08:06.573526Z"},"trusted":true},"outputs":[],"source":["torch.backends.cudnn.deterministic = True\n","eps=1e-7\n","\n","def noisy_label_loss(pred, cms, labels, ignore_index, min_trace = False, alpha=0.1, loss_mode=None):\n","    \"\"\" Loss for the crowdsourcing methods\n","    \"\"\"\n","    b, c, h, w = pred.size()\n","\n","    #\n","    pred_norm = pred.view(b, c, h*w).permute(0, 2, 1).contiguous().view(b*h*w, c, 1)\n","    cm = cms.view(b, c ** 2, h * w).permute(0, 2, 1).contiguous().view(b * h * w, c * c).view(b * h * w, c, c)\n","    cm = cm / cm.sum(1, keepdim=True)\n","\n","    pred_noisy = torch.bmm(cm, pred_norm).view(b*h*w, c)\n","    pred_noisy = pred_noisy.view(b, h*w, c).permute(0, 2, 1).contiguous().view(b, c, h, w)\n","\n","    if loss_mode == 'ce':\n","        loss_ce = nn.NLLLoss(reduction='mean', ignore_index=ignore_index)(torch.log(pred_noisy+eps), labels.view(b, h, w).long())\n","    elif loss_mode == 'dice':\n","        loss_ce = DiceLoss(ignore_index=ignore_index, from_logits=False, mode='multiclass')(pred_noisy, labels.view(b, h, w).long())\n","    elif loss_mode == 'focal':\n","        loss_ce = FocalLoss(reduction='mean', ignore_index=ignore_index, mode='multiclass')(pred_noisy, labels.view(b, h, w).long())\n","\n","    # regularization\n","    regularisation = torch.trace(torch.transpose(torch.sum(cm, dim=0), 0, 1)).sum() / (b * h * w)\n","    regularisation = alpha * regularisation\n","\n","    if min_trace:\n","        loss = loss_ce + regularisation\n","    else:\n","        loss = loss_ce - regularisation\n","\n","    return loss, loss_ce, regularisation"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-07-12T20:08:06.591359Z","iopub.status.busy":"2024-07-12T20:08:06.591066Z","iopub.status.idle":"2024-07-12T20:08:06.605875Z","shell.execute_reply":"2024-07-12T20:08:06.605103Z","shell.execute_reply.started":"2024-07-12T20:08:06.591326Z"},"trusted":true},"outputs":[],"source":["import segmentation_models_pytorch as smp\n","\n","\n","def create_segmentation_backbone():\n","    class_no = config['data']['class_no']\n","\n","    if config['model']['backbone'] == 'unet':\n","        seg_model = smp.Unet(\n","            encoder_name=config['model']['encoder']['backbone'],  # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n","            encoder_weights=config['model']['encoder']['weights'],\n","            # use `imagenet` pre-trained weights for encoder initialization\n","            in_channels=3,  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n","            classes=class_no,  # model output channels (number of classes in your dataset)           \n","        )\n","        \n","    elif config['model']['backbone'] == 'linknet':\n","        seg_model = smp.Linknet(\n","            encoder_name=config['model']['encoder']['backbone'],  # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n","            encoder_weights=config['model']['encoder']['weights'],\n","            # use `imagenet` pre-trained weights for encoder initialization\n","            in_channels=3,  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n","            classes=class_no  # model output channels (number of classes in your dataset)\n","        )\n","    else:\n","        raise Exception('Choose valid model backbone!')\n","    return seg_model\n","\n","\n","class SegmentationModel(torch.nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.seg_model = create_segmentation_backbone()\n","        self.activation = torch.nn.Softmax(dim=1)\n","\n","    def forward(self, x):\n","        x = self.seg_model(x)\n","        y = self.activation(x)\n","        return y"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-07-12T20:08:06.607148Z","iopub.status.busy":"2024-07-12T20:08:06.606885Z","iopub.status.idle":"2024-07-12T20:08:06.620473Z","shell.execute_reply":"2024-07-12T20:08:06.619807Z","shell.execute_reply.started":"2024-07-12T20:08:06.607125Z"},"trusted":true},"outputs":[],"source":["def preprocess_input(\n","    x, mean=None, std=None, input_space=\"RGB\", input_range=None, **kwargs\n","):\n","\n","    if input_space == \"BGR\":\n","        x = x[..., ::-1].copy()\n","\n","    if input_range is not None:\n","        if x.max() > 1 and input_range[1] == 1:\n","            x = x / 255.0\n","\n","    if mean is not None:\n","        mean = np.array(mean)\n","        x = x - mean\n","\n","    if std is not None:\n","        std = np.array(std)\n","        x = x / std\n","\n","    return x\n","\n","\n","def get_preprocessing_params():\n","    formatted_settings = {}\n","    formatted_settings[\"input_range\"] = (0,1)\n","    formatted_settings[\"input_space\"] = \"RGB\"\n","    formatted_settings[\"mean\"] = None\n","    formatted_settings[\"std\"] = None\n","    return formatted_settings\n","\n","\n","def get_preprocessing_fn_without_normalization():\n","    params = get_preprocessing_params()\n","    return functools.partial(preprocess_input, **params)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-07-12T20:08:06.622287Z","iopub.status.busy":"2024-07-12T20:08:06.621882Z","iopub.status.idle":"2024-07-12T20:08:06.646620Z","shell.execute_reply":"2024-07-12T20:08:06.645851Z","shell.execute_reply.started":"2024-07-12T20:08:06.622257Z"},"trusted":true},"outputs":[],"source":["def save_model(model):\n","    model_dir = 'models'\n","    dir = os.path.join(config['logging']['experiment_folder'], model_dir)\n","    os.makedirs(dir, exist_ok=True)\n","    out_path = os.path.join(dir, 'best_model.pth')\n","    torch.save(model, out_path)\n","    print('Best Model saved!')\n","\n","def save_test_images(test_imgs:torch.Tensor, test_preds: np.array, test_labels: np.array, test_name: np.array, mode: str):\n","    visual_dir = 'qualitative_results/' + mode\n","    dir = os.path.join(config['logging']['experiment_folder'], visual_dir)\n","    os.makedirs(dir, exist_ok=True)\n","\n","    h, w = np.shape(test_labels)\n","\n","    test_preds = np.asarray(test_preds, dtype=np.uint8)\n","    test_labels = np.asarray(test_labels, dtype=np.uint8)\n","\n","    # print(\"test name \", test_name)\n","    out_path = os.path.join(dir, 'img_' + test_name)\n","    save_image(test_imgs, out_path)\n","\n","    test_pred_rgb = convert_classes_to_rgb(test_preds, h, w)\n","    out_path = os.path.join(dir, 'pred_' + test_name)\n","    imageio.imsave(out_path, test_pred_rgb)\n","\n","    test_label_rgb = convert_classes_to_rgb(test_labels, h, w)\n","    out_path = os.path.join(dir, 'gt_' + test_name)\n","    imageio.imsave(out_path, test_label_rgb)\n","    mlflow.log_artifacts(dir, visual_dir)\n","\n","# TODO: funcion que guarde bien el crowdsourcing\n","def save_crowd_images(test_imgs:torch.Tensor, gt_pred: np.array, test_preds: np.array, test_labels: np.array, test_name: np.array, annotator, cm):\n","    visual_dir = 'qualitative_results/' + \"train_crowd\"\n","    dir = os.path.join(config['logging']['experiment_folder'], visual_dir)\n","    os.makedirs(dir, exist_ok=True)\n","\n","    h, w = np.shape(test_labels)\n","\n","    test_preds = np.asarray(test_preds, dtype=np.uint8)\n","    test_labels = np.asarray(test_labels, dtype=np.uint8)\n","\n","    # print(\"test name \", test_name)\n","    out_path = os.path.join(dir, 'img_' + test_name)\n","    save_image(test_imgs, out_path)\n","\n","    test_pred_rgb = convert_classes_to_rgb(test_preds, h, w)\n","    out_path = os.path.join(dir, annotator + '_pred_' + test_name)\n","    imageio.imsave(out_path, test_pred_rgb)\n","\n","    gt_pred_rgb = convert_classes_to_rgb(gt_pred, h, w)\n","    out_path = os.path.join(dir, 'gt_pred_' + test_name)\n","    imageio.imsave(out_path, gt_pred_rgb)\n","\n","    test_label_rgb = convert_classes_to_rgb(test_labels, h, w)\n","    out_path = os.path.join(dir, annotator + '_gt_' + test_name)\n","    imageio.imsave(out_path, test_label_rgb)\n","\n","    cm = cm.detach().cpu().numpy()\n","    plt.matshow(cm)\n","    out_path = os.path.join(dir, annotator + '_matrix_' + test_name)\n","    plt.savefig(out_path)\n","\n","    mlflow.log_artifacts(dir, visual_dir)\n","\n","\n","def save_image_color_legend():\n","    visual_dir = 'qualitative_results/'\n","    dir = os.path.join(config['logging']['experiment_folder'], visual_dir)\n","    os.makedirs(dir, exist_ok=True)\n","    class_no = config['data']['class_no']\n","    class_names = config['data']['class_names']\n","\n","    fig = plt.figure()\n","\n","    size = 100\n","\n","    for class_id in range(class_no):\n","        # out_img[size*class_id:size*(class_id+1),:,:] = convert_classes_to_rgb(np.ones(size,size,3)*class_id, size,size)\n","        out_img = convert_classes_to_rgb(np.ones(shape=[size,size])*class_id, size,size)\n","        ax = fig.add_subplot(1, class_no, class_id+1)\n","        ax.imshow(out_img)\n","        ax.set_title(class_names[class_id])\n","        ax.axis('off')\n","    plt.savefig(dir + 'legend.png')\n","    mlflow.log_artifact(dir + 'legend.png', 'qualitative_results')\n","\n","\n","def convert_classes_to_rgb(seg_classes, h, w):\n","\n","    seg_rgb = np.zeros((h, w, 3), dtype=np.uint8)\n","    class_no = config['data']['class_no']\n","\n","    colors = [[0,179,255], [153,0,0], [255,102,204], [0,153,51], [153,0,204]]\n","\n","    for class_id in range(class_no):\n","        seg_rgb[:, :, 0][seg_classes == class_id] = colors[class_id][0]\n","        seg_rgb[:, :, 1][seg_classes == class_id] = colors[class_id][1]\n","        seg_rgb[:, :, 2][seg_classes == class_id] = colors[class_id][2]\n","\n","    return seg_rgb\n","\n","\n","def save_results(results):\n","    results_dir = 'quantitative_results'\n","    dir = os.path.join(config['logging']['experiment_folder'], results_dir)\n","    os.makedirs(dir, exist_ok=True)\n","    out_path = os.path.join(dir, 'results.csv')\n","\n","    with open(out_path, 'w') as csv_file:\n","        writer = csv.writer(csv_file)\n","        for key, value in results.items():\n","            writer.writerow([key, value])"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-07-12T20:08:06.648028Z","iopub.status.busy":"2024-07-12T20:08:06.647671Z","iopub.status.idle":"2024-07-12T20:08:06.662586Z","shell.execute_reply":"2024-07-12T20:08:06.661701Z","shell.execute_reply.started":"2024-07-12T20:08:06.647995Z"},"trusted":true},"outputs":[],"source":["def dice_coef_binary(y_true, y_pred):\n","    intersection = np.sum(y_true * y_pred)\n","    smooth = 0.0001\n","    return (2. * intersection + smooth) / (np.sum(y_true) + np.sum(y_pred) + smooth)\n","\n","def dice_coef_multilabel(y_true, y_pred):\n","    class_no = config['data']['class_no']\n","    dice_per_class = []\n","    for index in range(class_no):\n","        dice_per_class.append(dice_coef_binary(y_true == index, y_pred == index))\n","\n","    return np.array(dice_per_class)\n","\n","def segmentation_scores(label_trues, label_preds, metric_names):\n","    '''\n","    :param label_trues:\n","    :param label_preds:\n","    :param n_class:\n","    :return:\n","    '''\n","    results = {}\n","    class_no = config['data']['class_no']\n","    class_names = config['data']['class_names']\n","    ignore_last_class = config['data']['ignore_last_class']\n","\n","    assert len(label_trues) == len(label_preds)\n","\n","    label_preds = np.array(label_preds, dtype='int8')\n","    label_trues = np.array(label_trues, dtype='int8')\n","\n","    if ignore_last_class:\n","        label_preds = label_preds[label_trues!=class_no]\n","        label_trues = label_trues[label_trues!=class_no]\n","\n","    dice_per_class = dice_coef_multilabel(label_trues, label_preds)\n","\n","    results['macro_dice'] = dice_per_class.mean()\n","\n","    intersection = (label_preds == label_trues).sum(axis=None)\n","    sum_ = 2 * np.prod(label_preds.shape)\n","    results['micro_dice'] = ((2 * intersection + 1e-6) / (sum_ + 1e-6))\n","\n","    for class_id in range(class_no):\n","        results['dice_class_' + str(class_id) + '_' + class_names[class_id]] = dice_per_class[class_id]\n","\n","    results['accuracy'] = accuracy_score(label_trues, label_preds)\n","    results['miou'] = jaccard_score(label_trues, label_preds, average=\"macro\") # same as IoU!\n","\n","    for metric in metric_names:\n","        assert metric in results.keys()\n","\n","    return results"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-07-12T20:08:06.664372Z","iopub.status.busy":"2024-07-12T20:08:06.664073Z","iopub.status.idle":"2024-07-12T20:08:06.704998Z","shell.execute_reply":"2024-07-12T20:08:06.704341Z","shell.execute_reply.started":"2024-07-12T20:08:06.664339Z"},"trusted":true},"outputs":[],"source":["def get_training_augmentation():\n","    aug_config = config['data']['augmentation']\n","    if aug_config['use_augmentation']:\n","        train_transform = [\n","            albu.HorizontalFlip(p=0.5),\n","            albu.VerticalFlip(p=0.5),\n","            albu.RandomRotate90(p=0.5),\n","\n","            albu.Blur(blur_limit=aug_config['gaussian_blur_kernel'], p=0.5),\n","            albu.RandomBrightnessContrast(brightness_limit=aug_config['brightness_limit'],\n","                                          contrast_limit=aug_config['contrast_limit'],\n","                                          p=0.5),\n","            albu.HueSaturationValue(hue_shift_limit=aug_config['hue_shift_limit'],\n","                                    sat_shift_limit=aug_config['sat_shift_limit'],\n","                                    p=0.5)\n","        ]\n","        composed_transform = albu.Compose(train_transform)\n","    else:\n","        composed_transform = None\n","    return composed_transform\n","\n","\n","def get_validation_augmentation():\n","    \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n","    test_transform = [\n","        albu.PadIfNeeded(384, 480)\n","    ]\n","    return albu.Compose(test_transform)\n","\n","\n","def to_tensor(x, **kwargs):\n","    return x.transpose(2, 0, 1).astype('float32')\n","\n","\n","def get_preprocessing(preprocessing_fn):\n","    \"\"\"Construct preprocessing transform\n","    Args:\n","        preprocessing_fn (callbale): data normalization function\n","            (can be specific for each pretrained neural network)\n","    Return:\n","        transform: albumentations.Compose\n","    \"\"\"\n","\n","    _transform = [\n","        albu.Lambda(image=preprocessing_fn),\n","        albu.Lambda(image=to_tensor, mask=to_tensor),\n","    ]\n","    return albu.Compose(_transform)\n","\n","# =============================================\n","\n","class CustomDataset(torch.utils.data.Dataset):\n","    \"\"\"Custom Dataset. Read images, apply augmentation and preprocessing transformations.\n","    Args:\n","        images_dir (str): path to images folder\n","        masks_dir (str): path to segmentation masks folder\n","        class_values (list): values of classes to extract from segmentation mask\n","        augmentation (albumentations.Compose): data transfromation pipeline\n","            (e.g. flip, scale, etc.)\n","        preprocessing (albumentations.Compose): data preprocessing\n","            (e.g. normalization, shape manipulation, etc.)\n","    \"\"\"\n","    def __init__(\n","            self,\n","            images_dir,\n","            masks_dir,\n","            augmentation=None,\n","            preprocessing=None\n","    ):\n","        #if config['data']['sr_experiment']:\n","        #    names = pd.read_csv(images_dir + config['data']['sr_path'] + 'test.csv').values.tolist()\n","        #    self.ids = [x[0] for x in names]\n","        #else:\n","            #self.ids = os.listdir(images_dir)\n","        self.ids = os.listdir(images_dir)\n","        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]\n","        self.masks_fps = [os.path.join(masks_dir, image_id) for image_id in self.ids]\n","        self.class_no = config['data']['class_no']\n","        self.class_values = self.set_class_values(self.class_no)\n","        self.augmentation = augmentation\n","        self.preprocessing = preprocessing\n","\n","    def __getitem__(self, i):\n","\n","        # read data\n","        image = cv2.imread(self.images_fps[i])\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        mask = cv2.imread(self.masks_fps[i], 0)\n","        \n","        #print(f'Image directory: {self.images_fps[i]}, mask directory: {self.masks_fps[i]}\\n')\n","        \n","        # check dimensions\n","        if image.shape[:2] != mask.shape[:2]:\n","            raise ValueError(f\"Inconsistent dimensions: image {image.shape[:2]} vs mask {mask.shape[:2]}\\n\")\n","        \n","        # extract certain classes from mask (e.g. cars)\n","        masks = [(mask == v) for v in self.class_values]\n","        mask = np.stack(masks, axis=-1).astype('float')\n","\n","        # apply augmentations\n","        if self.augmentation:\n","            sample = self.augmentation(image=image, mask=mask)\n","            image, mask = sample['image'], sample['mask']\n","\n","        # apply preprocessing\n","        if self.preprocessing:\n","            sample = self.preprocessing(image=image, mask=mask)\n","            image, mask = sample['image'], sample['mask']\n","\n","        return image, mask, self.ids[i], 0\n","\n","    def __len__(self):\n","        return len(self.ids)\n","\n","    def set_class_values(self, class_no):\n","        if config['data']['ignore_last_class']:\n","            class_values = list(range(class_no + 1))\n","        else:\n","            class_values = list(range(class_no))\n","        return class_values\n","\n","\n","class Crowdsourced_Dataset(torch.utils.data.Dataset):\n","    \"\"\"Crowdsourced_Dataset Dataset. Read images, apply augmentation and preprocessing transformations.\n","    Args:\n","        images_dir (str): path to images folder\n","        masks_dir (str): path to segmentation masks folder\n","        class_values (list): values of classes to extract from segmentation mask\n","        augmentation (albumentations.Compose): data transfromation pipeline\n","            (e.g. flip, scale, etc.)\n","        preprocessing (albumentations.Compose): data preprocessing\n","            (e.g. noralization, shape manipulation, etc.)\n","    \"\"\"\n","    def __init__(\n","            self,\n","            images_dir,\n","            masks_dir,\n","            augmentation=None,\n","            preprocessing=None,\n","            _set = None\n","    ):\n","        #if config['data']['sr_experiment']:\n","        #    names = pd.read_csv(images_dir + config['data']['sr_path'] + 'train.csv').values.tolist()\n","        #    self.ids = [x[0] for x in names]\n","        #else:\n","        #    self.ids = os.listdir(images_dir)\n","        self.ids = os.listdir(images_dir)\n","        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]\n","        \n","        annotators = os.listdir(masks_dir)\n","        self.annotators = [e for e in annotators if e not in ('expert', 'MV', 'STAPLE')]\n","        self.annotators_fps = [os.path.join(masks_dir, annotator) for annotator in self.annotators]\n","        self.masks_dir = masks_dir\n","        self.annotators_no = len(self.annotators)\n","        print(\"Images: \", self.ids)\n","        print(\"Annotators: \")\n","        print(*self.annotators, sep = \"\\n\")\n","        print(\"Number of annotators: \", self.annotators_no)\n","        print(\"Paths of annotators \", *self.annotators_fps)\n","        self.class_no = config['data']['class_no']\n","        self.class_values = self.set_class_values(self.class_no)\n","        self.augmentation = augmentation\n","        self.preprocessing = preprocessing\n","\n","        if config['data']['ignore_last_class']:\n","            self.ignore_index = int(self.class_no) # deleted class is always set to the last index\n","        else:\n","            self.ignore_index = -100 # this means no index ignored\n","\n","\n","\n","    def __getitem__(self, i):\n","\n","        # read data\n","        image = cv2.imread(self.images_fps[i])\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        size_image, _, _ = image.shape\n","        indexes = np.random.permutation(self.annotators_no)\n","        for ann_index in indexes:\n","            ann_path = self.annotators_fps[ann_index]\n","            #print('ANN_PATH',ann_path,end='\\n')\n","            mask_path = os.path.join(ann_path, self.ids[i])\n","            #print('MASK_PATH',mask_path,end='\\n')\n","            if os.path.exists(mask_path):\n","                mask = cv2.imread(mask_path, 0)\n","                # extract certain classes from mask (e.g. cars)\n","                mask = [(mask == v) for v in self.class_values]\n","                mask = np.stack(mask, axis=-1).astype('float')\n","                annotator_id = torch.zeros(len(self.annotators_fps))\n","                annotator_id[self.annotators_fps.index(ann_path)] = 1\n","                break\n","\n","                #print(\"Exist esta wea\", mask_path)\n","            else:\n","                #print(\"Not exist \", mask_path)\n","                continue\n","\n","        # apply augmentations\n","        if self.augmentation:\n","            # print(\"Augmentation!\")\n","            sample = self.augmentation(image=image, mask=mask)\n","            image = sample['image']\n","            mask = sample['mask']\n","\n","        # apply preprocessing\n","        if self.preprocessing:\n","            # print(\"Preprocessing!\")\n","            sample = self.preprocessing(image=image, mask=mask)\n","            image = sample['image']\n","            mask = sample['mask']\n","        # print(\"Return \", len(masks), \"masks\")\n","        # print(masks.shape)\n","        return image, mask, self.ids[i], annotator_id\n","\n","    def __len__(self):\n","        return len(self.ids)\n","\n","    def set_class_values(self, class_no):\n","        if config['data']['ignore_last_class']:\n","            class_values = list(range(class_no + 1))\n","        else:\n","            class_values = list(range(class_no))\n","        return class_values\n","\n","\n","def get_data_supervised():\n","    batch_size = config['model']['batch_size']\n","    normalization = config['data']['normalization']\n","    crowd = config['data']['crowd']\n","\n","    train_image_folder = os.path.join(config['data']['path'], config['data']['train']['images'])\n","    train_label_folder = os.path.join(config['data']['path'], config['data']['train']['masks'])\n","    val_image_folder = os.path.join(config['data']['path'], config['data']['val']['images'])\n","    val_label_folder = os.path.join(config['data']['path'], config['data']['val']['masks'])\n","    print(f'val_image_folder: {val_image_folder}, val_label_folder: {val_label_folder}')\n","    test_image_folder = os.path.join(config['data']['path'], config['data']['test']['images'])\n","    test_label_folder = os.path.join(config['data']['path'], config['data']['test']['masks'])\n","\n","    if normalization:\n","        encoder_name = config['model']['encoder']['backbone']\n","        encoder_weights = config['model']['encoder']['weights']\n","        preprocessing_fn = get_preprocessing_fn(encoder_name, pretrained=encoder_weights)\n","    else:\n","        preprocessing_fn = get_preprocessing_fn_without_normalization()\n","\n","    preprocessing = get_preprocessing(preprocessing_fn)\n","\n","    annotators = []\n","\n","    if crowd:\n","        train_dataset = Crowdsourced_Dataset(train_image_folder, train_label_folder, augmentation=get_training_augmentation(),\n","                                      preprocessing = preprocessing)\n","        annotators = train_dataset.annotators\n","\n","    else:\n","        train_dataset = CustomDataset(train_image_folder, train_label_folder, augmentation=get_training_augmentation(),\n","                                      preprocessing = preprocessing)\n","    validate_dataset = CustomDataset(val_image_folder, val_label_folder, preprocessing = preprocessing)\n","    \n","    test_dataset = CustomDataset(test_image_folder, test_label_folder, preprocessing = preprocessing)\n","\n","    \n","    trainloader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8, drop_last=True)\n","    \n","    validateloader = data.DataLoader(validate_dataset, batch_size=batch_size, shuffle=False, num_workers=batch_size,\n","                                     drop_last=False)\n","    testloader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=batch_size,\n","                                 drop_last=False)\n","\n","    return trainloader, validateloader, testloader, annotators"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-07-12T20:08:06.706688Z","iopub.status.busy":"2024-07-12T20:08:06.706377Z","iopub.status.idle":"2024-07-12T20:08:06.767115Z","shell.execute_reply":"2024-07-12T20:08:06.766236Z","shell.execute_reply.started":"2024-07-12T20:08:06.706665Z"},"trusted":true},"outputs":[],"source":["eps=1e-7\n","\n","\n","class ModelHandler():\n","    def __init__(self, annotators):\n","\n","        # architecture\n","        if config['model']['crowd_type'] == 'prob-unet':\n","            self.model = ProbabilisticUnet(3, config['data']['class_no'])\n","        elif config['data']['crowd']:\n","            self.model = Crowd_segmentationModel(annotators)\n","            self.alpha = 1\n","            self.annotators = annotators\n","        else:\n","            self.model = SegmentationModel()\n","\n","        # loss\n","        self.loss_mode = config['model']['loss']\n","\n","        #GPU\n","        self.model.cuda()\n","        if torch.cuda.is_available():\n","            print('Running on GPU')\n","            self.device = torch.device('cuda')\n","        else:\n","            warnings.warn(\"Running on CPU because no GPU was found!\")\n","            self.device = torch.device('cpu')\n","\n","    def train(self, trainloader, validateloader):\n","        model = self.model\n","        device = self.device\n","        max_score = 0\n","        c_weights = config['data']['class_weights']\n","        class_weights = torch.FloatTensor(c_weights).cuda()\n","\n","        class_no = config['data']['class_no']\n","        epochs = config['model']['epochs']\n","        learning_rate = config['model']['learning_rate']\n","        batch_s = config['model']['batch_size']\n","        vis_train_images = config['data']['visualize_images']['train']\n","        save_image_color_legend()\n","\n","        # Optimizer\n","        if config['data']['crowd'] and config['model']['crowd_type']!='prob-unet':\n","            optimizer = torch.optim.Adam([\n","                {'params': model.seg_model.parameters()},\n","                {'params': model.crowd_layers.parameters(), 'lr': 1e-3}\n","            ], lr=learning_rate)\n","        elif config['model']['optimizer'] == 'adam':\n","            optimizer = torch.optim.Adam([\n","                dict(params=model.parameters(), lr=learning_rate),\n","            ])\n","        elif config['model']['optimizer'] == 'sgd_mom':\n","            optimizer = torch.optim.SGD([\n","                dict(params=model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True),\n","            ])\n","        else:\n","            raise Exception('Choose valid optimizer!')\n","\n","        min_trace = config['model']['min_trace']\n","\n","        # Training loop\n","        for i in range(0, epochs):\n","\n","            print('\\nEpoch: {}'.format(i))\n","            model.train()\n","\n","            # Stop of the warm-up period\n","            if i == 5: #10 for cr_image_dice // 5 rest of the methods\n","                print(\"Minimize trace activated!\")\n","                min_trace = True\n","                self.alpha = config['model']['alpha']\n","                print(\"Alpha updated\", self.alpha)\n","\n","                if config['data']['crowd'] and config['model']['crowd_type']!='prob-unet':\n","                    optimizer = torch.optim.Adam([\n","                        {'params': model.seg_model.parameters()},\n","                        {'params': model.crowd_layers.parameters(), 'lr': 1e-4}\n","                    ], lr=learning_rate)\n","\n","            # Training in batches\n","            for j, (images, labels, imagename, ann_ids) in enumerate(trainloader):\n","                # Loading data to GPU\n","                images = images.cuda().float()\n","                labels = labels.cuda().long()\n","                ann_ids = ann_ids.cuda().float()\n","                \n","                #print(f'Images shape: {images.shape}, Labels shape: {labels.shape}, ANN_ids shape: {ann_ids.shape}')\n","\n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","\n","                if config['data']['ignore_last_class']:\n","                    ignore_index = int(config['data']['class_no'])  # deleted class is always set to the last index\n","                else:\n","                    ignore_index = -100  # this means no index ignored\n","                self.ignore_index = ignore_index\n","\n","                # Foward+loss (crowd or not)\n","                if config['model']['crowd_type'] == 'prob-unet':\n","                    _, labels = torch.max(labels, dim=1)\n","                    labels = labels[:,None,:,:]\n","                    model.forward(images, labels, training=True)\n","                    elbo = model.elbo(labels)\n","                    reg_loss = l2_regularisation(model.posterior) + l2_regularisation(model.prior) + l2_regularisation(\n","                        model.fcomb.layers)\n","                    loss = -elbo + 1e-5 * reg_loss\n","                elif config['data']['crowd']:\n","                    _, labels = torch.max(labels, dim=1)\n","                    #print('################################################1\\n')\n","                    y_pred, cms = model(images, ann_ids)\n","                    #print(f'y_pred shape: {y_pred.shape}')\n","                    loss, loss_ce, loss_trace = noisy_label_loss(y_pred, cms, labels, ignore_index,\n","                                                                 min_trace, self.alpha, self.loss_mode)\n","                else:\n","                    _, labels = torch.max(labels, dim=1)\n","                    y_pred = model(images)\n","\n","                    if self.loss_mode == 'ce':\n","                        loss = torch.nn.NLLLoss(reduction='mean', ignore_index=ignore_index, weight=class_weights)(\n","                            torch.log(y_pred+eps), labels)\n","                    elif self.loss_mode == 'dice':\n","                        loss = DiceLoss(ignore_index=ignore_index, from_logits=False, mode='multiclass')(\n","                            y_pred, labels)\n","                    elif self.loss_mode == 'focal':\n","                        loss = FocalLoss(reduction='mean', ignore_index=ignore_index, mode='multiclass')(\n","                            y_pred, labels)\n","\n","                # Final prediction\n","                if not config['data']['crowd']:\n","                    _, y_pred_max = torch.max(y_pred[:, 0:class_no], dim=1)\n","\n","                # Backprop\n","                if not torch.isnan(loss):\n","                    loss.backward()\n","                    optimizer.step()\n","\n","                # Save results in training (only save for not crowd methods)\n","                if j % int(config['logging']['interval']) == 0:\n","                    print(\"Iter {}/{} - batch loss : {:.4f}\".format(j, len(trainloader), loss))\n","                    if not config['data']['crowd']:\n","                        train_results = self.get_results(y_pred_max, labels)\n","                        log_results(train_results, mode='train', step=(i * len(trainloader) * batch_s + j))\n","                        for k in range(len(imagename)):\n","                            if imagename[k] in vis_train_images:\n","                                labels_save = labels[k].cpu().detach().numpy()\n","                                y_pred_max_save = y_pred_max[k].cpu().detach().numpy()\n","                                images_save = images[k]  # .cpu().detach().numpy()\n","                                save_test_images(images_save, y_pred_max_save, labels_save, imagename[k], 'train')\n","\n","            # Save validation results\n","            val_results = self.evaluate(validateloader, mode='val')  # TODO: validate crowd\n","            log_results(val_results, mode='val', step=int((i + 1) * len(trainloader) * batch_s))\n","            mlflow.log_metric('finished_epochs', i + 1, int((i + 1) * len(trainloader) * batch_s))\n","\n","            # Save model\n","            metric_for_saving = val_results['macro_dice']\n","            if max_score < metric_for_saving and i > 9:\n","                save_model(model)\n","                max_score = metric_for_saving\n","\n","            # LR decay\n","            if i > config['model']['lr_decay_after_epoch']:\n","                for g in optimizer.param_groups:\n","                    g['lr'] = g['lr'] / (1 + config['model']['lr_decay_param'])\n","\n","            # Show annotator matrix\n","            if config['data']['crowd'] and config['model']['crowd_type']!='prob-unet':\n","                _,  ann_id = torch.max(ann_ids, dim=1)\n","                for ann_ix, cm in enumerate(cms):\n","                    print(f'ann_id: {ann_id}, ann_ix:{ann_ix}, cm shape: {cm.shape}')\n","                    if cm.shape[0] == 4:\n","                    # adjust the shape of the `cm` tensor for subsequent operations.\n","                        cm = cm.view(2, 2, 512, 512)\n","                    else:\n","                        cm = cm.view(5,5,512,512)\n","                    cm_ = cm[:,:,100,100]\n","                    cm_ = cm_/cm_.sum(0)\n","                    print(\"Annotators\", ann_id)\n","                    print(\"CM \", ann_id[ann_ix].cpu().detach().numpy()+1, \": \", cm_.cpu().detach().numpy())\n","\n","\n","        # Final evaluation of crowd\n","        if config['data']['crowd'] and config['model']['crowd_type']!='prob-unet':\n","            self.evaluate_crowd(trainloader, mode='train')\n","\n","    def test(self, testloader):\n","        save_image_color_legend()\n","        results = self.evaluate(testloader)\n","        log_results(results, mode='test', step=None)\n","        save_results(results)\n","\n","    def evaluate(self, evaluatedata, mode='test'):\n","        class_no = config['data']['class_no']\n","        vis_images = config['data']['visualize_images'][mode]\n","\n","        if mode=='test':\n","            print(\"Testing the best model\")\n","            model_dir = 'models'\n","            dir = os.path.join(config['logging']['experiment_folder'], model_dir)\n","            model_path = os.path.join(dir, 'best_model.pth')\n","            model = torch.load(model_path)\n","        else:\n","            model = self.model\n","\n","        device = self.device\n","        model.eval()\n","\n","        labels = []\n","        preds = []\n","\n","        with torch.no_grad():\n","            for j, (test_img, test_label, test_name, _) in enumerate(evaluatedata):\n","                test_img = test_img.to(device=device, dtype=torch.float32)\n","                if config['model']['crowd_type'] == 'prob-unet':\n","                    model.forward(test_img, None, training=False)\n","                    test_pred = model.sample(testing=True)\n","                elif config['data']['crowd']:\n","                    test_pred, _ = model(test_img)\n","                else:\n","                    test_pred = model(test_img)\n","                _, test_pred = torch.max(test_pred[:, 0:class_no], dim=1)\n","                test_pred_np = test_pred.cpu().detach().numpy()\n","                test_label = test_label.cpu().detach().numpy()\n","                test_label = np.argmax(test_label, axis=1)\n","\n","                preds.append(test_pred_np.astype(np.int8).copy().flatten())\n","                labels.append(test_label.astype(np.int8).copy().flatten())\n","\n","                for k in range(len(test_name)):\n","                    if test_name[k] in vis_images or vis_images == 'all':\n","                        img = test_img[k]\n","                        save_test_images(img, test_pred_np[k], test_label[k], test_name[k], mode)\n","\n","            preds = np.concatenate(preds, axis=0, dtype=np.int8).flatten()\n","            labels = np.concatenate(labels, axis=0, dtype=np.int8).flatten()\n","\n","            results = self.get_results(preds, labels)\n","\n","            print('RESULTS for ' + mode)\n","            print(results)\n","            return results\n","\n","    def evaluate_crowd(self, evaluatedata, mode='train'):\n","        class_no = config['data']['class_no']\n","        vis_images = config['data']['visualize_images'][mode]\n","        print(\"Testing the best model for crowds\")\n","        model_dir = 'models'\n","        dir = os.path.join(config['logging']['experiment_folder'], model_dir)\n","        model_path = os.path.join(dir, 'best_model.pth')\n","        model = torch.load(model_path)\n","\n","        device = self.device\n","        model.eval()\n","\n","        with torch.no_grad():\n","            for j, (test_img, test_label, test_name, ann_id) in enumerate(evaluatedata):\n","                test_img = test_img.to(device=device, dtype=torch.float32)\n","                ann_id = ann_id.to(device=device)\n","                pred_noisy_list = []\n","                test_pred, cm = model(test_img, ann_id)\n","\n","                test_pred_np = test_pred.cpu().detach().numpy()\n","                test_pred_np = np.argmax(test_pred_np, axis=1)\n","\n","                _, test_label = torch.max(test_label, dim=1)\n","                test_label = test_label.cpu().detach().numpy()\n","\n","                b, c, h, w = test_pred.size()\n","\n","                pred_noisy = test_pred.view(b, c, h * w).permute(0, 2, 1).contiguous().view(b * h * w, c, 1)\n","\n","                cm = cm.view(b, c ** 2, h * w).permute(0, 2, 1).contiguous().view(b * h * w, c * c).view(\n","                    b * h * w, c, c)\n","                cm = cm / cm.sum(1, keepdim=True) # normalize cm\n","\n","                pred_noisy = torch.bmm(cm, pred_noisy).view(b * h * w, c) # prediction annotator\n","                pred_noisy = pred_noisy.view(b, h * w, c).permute(0, 2, 1).contiguous().view(b, c, h, w)\n","\n","                _, pred_noisy = torch.max(pred_noisy[:, 0:class_no], dim=1)\n","                pred_noisy_np = pred_noisy.cpu().detach().numpy()\n","\n","                pred_noisy_list.append(pred_noisy.cpu().detach().numpy().astype(np.int8).copy().flatten())\n","\n","                cm = cm.view(b, h*w, c, c)\n","\n","                if config['model']['crowd_type'] == 'pixel':\n","                    cm = cm.mean(1)\n","                    cm = cm/cm.sum(1, keepdim=True)\n","\n","                else:\n","                    cm = cm[:,0,:,:]\n","                _, ann = torch.max(ann_id, dim=1)\n","                ann = ann.cpu().detach().numpy()\n","                for k in range(len(test_name)):\n","                    if test_name[k] in vis_images or vis_images == 'all':\n","                        img = test_img[k]\n","                        save_crowd_images(img, test_pred_np[k], pred_noisy_np[k], test_label[k],\n","                                          test_name[k], self.annotators[ann[k]], cm[k])\n","\n","    def get_results(self, pred, label):\n","        class_no = config['data']['class_no']\n","        class_names = config['data']['class_names']\n","\n","        metrics_names = ['macro_dice', 'micro_dice', 'miou', 'accuracy']\n","        for class_id in range(class_no):\n","            metrics_names.append('dice_class_' + str(class_id) + '_' + class_names[class_id])\n","\n","        if torch.is_tensor(pred):\n","            pred = pred.cpu().detach().numpy().copy().flatten()\n","        if torch.is_tensor(label):\n","            label = label.cpu().detach().numpy().copy().flatten()\n","\n","        results = segmentation_scores(label, pred, metrics_names)\n","\n","        return results"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-07-12T20:08:06.783659Z","iopub.status.busy":"2024-07-12T20:08:06.783350Z","iopub.status.idle":"2024-07-12T20:09:34.262511Z","shell.execute_reply":"2024-07-12T20:09:34.261315Z","shell.execute_reply.started":"2024-07-12T20:08:06.783619Z"},"trusted":true},"outputs":[],"source":["if not os.path.exists('/kaggle/working/OxfordPet'):\n","    os.makedirs('/kaggle/working/OxfordPet')\n","\n","!cp -r /kaggle/input/oxfordpets /kaggle/working\n","\n","if os.path.exists('/kaggle/working/oxfordpets'):\n","    os.rename('/kaggle/working/oxfordpets', '/kaggle/working/OxfordPet')\n","    \n","if not os.path.exists('/kaggle/working/OxfordPet/experiments'):\n","    os.makedirs('/kaggle/working/OxfordPet/experiments')\n","    \n","!cp -r /kaggle/input/crowd-seg-model-training-results/models /kaggle/working/OxfordPet\n","!cp -r /kaggle/input/crowd-seg-model-training-results/qualitative_results /kaggle/working/OxfordPet\n","!cp -r /kaggle/input/crowd-seg-model-training-results/quantitative_results /kaggle/working/OxfordPet"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-11T15:32:58.367673Z","iopub.status.busy":"2024-07-11T15:32:58.367355Z","iopub.status.idle":"2024-07-11T15:32:58.393868Z","shell.execute_reply":"2024-07-11T15:32:58.393009Z","shell.execute_reply.started":"2024-07-11T15:32:58.367643Z"},"trusted":true},"outputs":[],"source":["def main():\n","    print(os.curdir)\n","    #os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  # see issue #152\n","    #os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(3)\n","    #os.environ['CUDA_LAUNCH_BLOCKING'] = str(1)\n","\n","    start_logging()\n","\n","    # load data\n","    trainloader, validateloader, testloader, annotators = get_data_supervised()\n","\n","    # load and train the model\n","    model_handler = ModelHandler(annotators)\n","    model_handler.train(trainloader, validateloader)\n","    model_handler.test(testloader)\n","\n","# Simulate argument parsing for notebook execution\n","args = argparse.Namespace(\n","    default_config=\"/kaggle/working/OxfordPet/config.yaml\",\n","    experiment_folder= \"/kaggle/working/OxfordPet/\"\n",")\n","init_global_config(args)\n","torch.manual_seed(config['model']['seed'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-11T15:32:58.396132Z","iopub.status.busy":"2024-07-11T15:32:58.395768Z","iopub.status.idle":"2024-07-11T16:02:06.451844Z","shell.execute_reply":"2024-07-11T16:02:06.450697Z","shell.execute_reply.started":"2024-07-11T15:32:58.396101Z"},"trusted":true},"outputs":[],"source":["# Call the main function to execute the workflow\n","main()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-12T20:10:01.358153Z","iopub.status.busy":"2024-07-12T20:10:01.357224Z","iopub.status.idle":"2024-07-12T20:10:01.761530Z","shell.execute_reply":"2024-07-12T20:10:01.760601Z","shell.execute_reply.started":"2024-07-12T20:10:01.358114Z"},"trusted":true},"outputs":[],"source":["model = torch.load('/kaggle/working/OxfordPet/models/best_model.pth')\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-07-12T20:10:18.262043Z","iopub.status.busy":"2024-07-12T20:10:18.261654Z","iopub.status.idle":"2024-07-12T20:10:18.268076Z","shell.execute_reply":"2024-07-12T20:10:18.267159Z","shell.execute_reply.started":"2024-07-12T20:10:18.262000Z"},"trusted":true},"outputs":[],"source":["def load_chunk_from_hdf5(filename, dataset_name, start_index, end_index):\n","    \"\"\"\n","    Load a chunk of data from an HDF5 file.\n","\n","    This function loads a chunk of data from an HDF5 file, given the filename,\n","    the name of the dataset in the file, and the start and end indices of the chunk\n","    in the dataset. The function uses the h5py library to open the file in read mode\n","    and read the chunk data from the specified dataset.\n","\n","    Args:\n","        filename (str): The name of the HDF5 file to load the chunk from.\n","        dataset_name (str): The name of the dataset in the HDF5 file to load the chunk from.\n","        start_index (int): The start index of the chunk in the dataset.\n","        end_index (int): The end index of the chunk in the dataset.\n","\n","    Returns:\n","        chunk_data (numpy.ndarray): The chunk of data loaded from the HDF5 file.\n","    \"\"\"\n","    with h5py.File(filename, 'r') as f:\n","        dataset = f[dataset_name]\n","        chunk_data = dataset[start_index:end_index, :, :, :]\n","    return chunk_data"]},{"cell_type":"markdown","metadata":{},"source":["## Sample of images and mask in Oxford Pets dataset in h5 file"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-12T20:10:44.011273Z","iopub.status.busy":"2024-07-12T20:10:44.010433Z","iopub.status.idle":"2024-07-12T20:10:44.277173Z","shell.execute_reply":"2024-07-12T20:10:44.276361Z","shell.execute_reply.started":"2024-07-12T20:10:44.011241Z"},"trusted":true},"outputs":[],"source":["chunk_data_images = load_chunk_from_hdf5('/kaggle/working/OxfordPet/images_train.h5', 'dataset', 105, 110)\n","\n","chunk_data_masks = load_chunk_from_hdf5('/kaggle/working/OxfordPet/masks_train.h5', 'dataset', 105, 110)\n","\n","print(f'Images shape: {chunk_data_images.shape}, Masks shape: {chunk_data_masks.shape}')\n","plt.imsave('./image_example_train.png', chunk_data_images[0], format='png')\n","plt.imsave('./mask_example_train.png', chunk_data_masks[0,:,:,0], format='png')\n","display(Image(filename='/kaggle/working/image_example_train.png'))\n","display(Image(filename='/kaggle/working/mask_example_train.png'))"]},{"cell_type":"markdown","metadata":{},"source":["## Definition of performance metrics"]},{"cell_type":"markdown","metadata":{},"source":["### DICE metric"]},{"cell_type":"markdown","metadata":{},"source":["$$\\text{Dice} = {2 \\cdot |\\text{Intersection}| + \\text{smooth} \\over |\\text{Union}| + \\text{smooth}}$$\n","\n","Where:\n","\n","$|\\text{Intersection}| = \\sum_{i=1}^{N} y\\_{true\\_i} \\cdot y\\_{pred\\_i}$, $|\\text{Union}| = \\sum_{i=1}^{N} y\\_{true\\_i} + \\sum_{i=1}^{N} y\\_{pred\\_i}$\n","\n","\n","- $N$ is the total number of elements in the segmentation masks.\n","- $y\\_{true\\_i}$ and $y\\_{pred\\_i}$ represent the value of the i-th element in the ground truth and predicted segmentation masks, respectively.\n","- $\\text{smooth}$ is a smoothing parameter to avoid division by zero."]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-07-12T20:10:52.609286Z","iopub.status.busy":"2024-07-12T20:10:52.608912Z","iopub.status.idle":"2024-07-12T20:10:52.618567Z","shell.execute_reply":"2024-07-12T20:10:52.617470Z","shell.execute_reply.started":"2024-07-12T20:10:52.609255Z"},"trusted":true},"outputs":[],"source":["class DiceMetric:\n","    \"\"\"\n","    Class for calculating the Dice coefficient metric for image segmentation.\n","\n","    This class provides a method for calculating the Dicecoefficient between \n","    a ground truth image and a predicted image, measures their similarity,\n","    as well as a method for calculating the total Dice coefficient for a set of images.\n","\n","    Attributes:\n","        None\n","\n","    Methods:\n","        calculate: Calculates the Dice coefficient between a ground truth image\n","                   and a predicted image.\n","        total_dice: Calculates the total Dice coefficient for a set of images.\n","    \"\"\"\n","    \n","    def __init__(self):\n","        \"\"\"\n","        Initializes the dice_metric class.\n","        \"\"\"\n","        pass\n","    \n","    def calculate(self,y_true, y_pred, axis=(1, 2), smooth=1e-5):\n","        \"\"\"\n","        Calculates the Dice coefficient between a ground truth image and a\n","        predicted image.\n","\n","        Parameters:\n","            y_true: Ground truth image.\n","            y_pred: Predicted image.\n","            axis: Axes over which to sum the intersection and union of the images.\n","            smooth: Small value added to the numerator and denominator to avoid\n","                    division by zero.\n","\n","        Returns:\n","            None\n","        \"\"\"\n","        global dice_total\n","        y_true = np.squeeze(y_true, axis=-1).astype(np.float32)\n","        y_pred = y_pred.astype(np.float32)\n","        intersection = np.sum(y_true * y_pred, axis=axis)\n","        union = np.sum(y_true, axis=axis) + np.sum(y_pred, axis=axis)\n","        dice = (2. * intersection + smooth) / (union + smooth)\n","        dice_total.append(dice)\n","    \n","    def total_dice(self):\n","        \"\"\"\n","        Calculates the total Dice coefficient for a set of images.\n","\n","        Parameters:\n","            None\n","\n","        Returns:\n","            value: The total Dice coefficient for the set of images.\n","        \"\"\"\n","        value = np.concatenate(dice_total, axis=0)\n","        return np.mean(value)"]},{"cell_type":"markdown","metadata":{},"source":["### Jaccard metric"]},{"cell_type":"markdown","metadata":{},"source":["$$\\text{Jaccard} = {|\\text{Intersection}| + \\text{smooth} \\over |\\text{Union}| + \\text{smooth}}$$\n","\n","Where:\n","\n","$|\\text{Intersection}| = \\sum_{i=1}^{N} y\\_{true\\_i} \\cdot y\\_{pred\\_i}$, $|\\text{Union}| = \\sum_{i=1}^{N} y\\_{true\\_i} + \\sum_{i=1}^{N} y\\_{pred\\_i} - |\\text{Intersection}|$\n","- $N$ is the total number of elements in the segmentation masks.\n","- $y\\_{true\\_i}$ and $y\\_{pred\\_i}$ represent the value of the i-th element in the ground truth and predicted segmentation masks, respectively.\n","- $\\text{smooth}$ is a small smoothing parameter to avoid division by zero."]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-07-12T20:10:53.890836Z","iopub.status.busy":"2024-07-12T20:10:53.889948Z","iopub.status.idle":"2024-07-12T20:10:53.899968Z","shell.execute_reply":"2024-07-12T20:10:53.899205Z","shell.execute_reply.started":"2024-07-12T20:10:53.890799Z"},"trusted":true},"outputs":[],"source":["class JaccardMetric:\n","    \"\"\"\n","    Class for calculating the Jaccard similarity coefficient metric for image\n","    segmentation.\n","\n","    The Jaccard similarity coefficient, also known as the Intersection over Union (IoU),\n","    measures the similarity between two sets by comparing their intersection to their union.\n","    In the context of semantic segmentation, it quantifies the overlap between the ground\n","    truth segmentation masks and the predicted segmentation masks.\n","\n","    Attributes:\n","        None\n","\n","    Methods:\n","        calculate: Calculates the Jaccard similarity coefficient between a ground\n","                   truth image and a predicted image.\n","        total_jaccard: Calculates the total Jaccard similarity coefficient for a\n","                       set of images.\n","    \"\"\"\n","    def __init__(self):\n","        \"\"\"\n","        Initializes the JaccardMetric class.\n","        \"\"\"\n","        pass\n","    \n","    def calculate(self, y_true, y_pred, axis=(1, 2), smooth=1e-5):\n","        \"\"\"\n","        Calculates the Jaccard similarity coefficient between a ground truth image\n","        and a predicted image.\n","\n","        Parameters:\n","            y_true: Ground truth image.\n","            y_pred: Predicted image.\n","            axis: Axes over which to sum the intersection and union of the images.\n","            smooth: Small value added to the numerator and denominator to avoid\n","                    division by zero.\n","\n","        Returns:\n","            None\n","        \"\"\"\n","        global jaccard_total\n","        y_true = np.squeeze(y_true, axis=-1).astype(np.float32)\n","        y_pred = y_pred.astype(np.float32)\n","        intersection = np.sum(y_true * y_pred, axis=axis)\n","        union = np.sum(y_true, axis=axis) + np.sum(y_pred, axis=axis) - intersection\n","        jaccard = (intersection + smooth) / (union + smooth)\n","        jaccard_total.append(jaccard)\n","        \n","    def total_jaccard(self):\n","        \"\"\"\n","        Calculates the total Jaccard similarity coefficient for a set of images.\n","\n","        Parameters:\n","            None\n","\n","        Returns:\n","            value: The total Jaccard similarity coefficient for the set of images.\n","        \"\"\"\n","        value = np.concatenate(jaccard_total, axis=0)\n","        return np.mean(value)"]},{"cell_type":"markdown","metadata":{},"source":["### Sensitivity metric"]},{"cell_type":"markdown","metadata":{},"source":["$$\\text{Sensitivity} = {\\text{True Positives} \\over \\text{Actual Positives} + \\text{smooth}}$$\n","\n","Where:\n","\n","$\\text{True Positives} = \\sum_{i=1}^{N} y\\_{true\\_i} \\cdot y\\_{pred\\_i}$, $\\text{Actual Positives} = \\sum_{i=1}^{N} y\\_{true\\_i}$\n","\n","\n","- $N$ is the total number of elements in the labels.\n","- $y\\_{true\\_i}$ and $y\\_{pred\\_i}$ represent the value of the i-th element in the ground truth and predicted labels, respectively.\n","- $\\text{smooth}$ is a small value added to the denominator to avoid division by zero."]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-07-12T20:10:54.984060Z","iopub.status.busy":"2024-07-12T20:10:54.983177Z","iopub.status.idle":"2024-07-12T20:10:54.992643Z","shell.execute_reply":"2024-07-12T20:10:54.991764Z","shell.execute_reply.started":"2024-07-12T20:10:54.984024Z"},"trusted":true},"outputs":[],"source":["class SensitivityMetric:\n","    \"\"\"\n","    Class for calculating the sensitivity metric for image segmentation.\n","\n","    Sensitivity, also known as true positive rate or recall, measures the proportion\n","    of actual positives that are correctly identified by the model. It is computed\n","    as the ratio of true positives to the sum of true positives and false negatives.\n","\n","    Attributes:\n","        None\n","\n","    Methods:\n","        calculate: Calculates the sensitivity between a ground truth image and a\n","                   predicted image.\n","        total_sensitivity: Calculates the total sensitivity for a set of images.\n","    \"\"\"\n","    def __init__(self):\n","        \"\"\"\n","        Initializes the SensitivityMetric class.\n","        \"\"\"\n","        pass\n","    \n","    def calculate(self, y_true, y_pred, axis=(1, 2), smooth=1e-5):\n","        \"\"\"\n","        Calculates the sensitivity between a ground truth image and a predicted\n","        image.\n","\n","        Parameters:\n","            y_true: Ground truth image.\n","            y_pred: Predicted image.\n","            axis: Axes over which to sum the true positives and actual positives.\n","            smooth: Small value added to the denominator to avoid division by zero.\n","\n","        Returns:\n","            None\n","        \"\"\"\n","        global sensitivity_total\n","        y_true = np.squeeze(y_true, axis=-1).astype(np.float32)\n","        y_pred = y_pred.astype(np.float32)\n","        true_positives = np.sum(y_true * y_pred, axis=axis)\n","        actual_positives = np.sum(y_true, axis=axis)\n","        sensitivity = true_positives / (actual_positives + smooth)\n","        sensitivity_total.append(sensitivity)\n","        \n","    def total_sensitivity(self):\n","        \"\"\"\n","        Calculates the total sensitivity for a set of images.\n","\n","        Parameters:\n","            None\n","\n","        Returns:\n","            value: The total sensitivity for the set of images.\n","        \"\"\"\n","        value = np.concatenate(sensitivity_total, axis=0)\n","        return np.mean(value)"]},{"cell_type":"markdown","metadata":{},"source":["### Specificity metric"]},{"cell_type":"markdown","metadata":{},"source":["$$\\text{Specificity} = {\\text{True Negatives} \\over \\text{Actual Negatives} + \\text{smooth}}$$\n","\n","Where:\n","\n","$\\text{True Negatives} = \\sum_{i=1}^{N} (1 - y\\_{true\\_i}) \\cdot (1 - y\\_{pred\\_i})$, $\\text{Actual Negatives} = \\sum_{i=1}^{N} (1 - y\\_{true\\_i})$\n","\n","- $N$ is the total number of samples.\n","- $y\\_{true\\_i}$ and $y\\_{pred\\_i}$ represent the ground truth label and predicted probability (or binary prediction) for the i-th sample, respectively.\n","- $\\text{smooth}$ is a smoothing term to avoid division by zero."]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-07-12T20:10:56.070025Z","iopub.status.busy":"2024-07-12T20:10:56.069121Z","iopub.status.idle":"2024-07-12T20:10:56.079377Z","shell.execute_reply":"2024-07-12T20:10:56.078335Z","shell.execute_reply.started":"2024-07-12T20:10:56.069992Z"},"trusted":true},"outputs":[],"source":["class SpecificityMetric:\n","    \"\"\"\n","    Class for calculating the specificity metric for image segmentation.\n","\n","    Specificity measures the proportion of actual negative cases that were correctly\n","    identified as such. It is complementary to sensitivity (recall).\n","\n","    Attributes:\n","        None\n","\n","    Methods:\n","        calculate: Calculates the specificity between a ground truth image and a\n","                   predicted image.\n","        total_specificity: Calculates the total specificity for a set of images.\n","    \"\"\"\n","    def __init__(self):\n","        \"\"\"\n","        Initializes the SpecificityMetric class.\n","        \"\"\"\n","        pass\n","    \n","    def calculate(self, y_true, y_pred, axis=(1, 2), smooth=1e-5):\n","        \"\"\"\n","        Calculates the specificity between a ground truth image and a predicted\n","        image.\n","\n","        Parameters:\n","            y_true: Ground truth image.\n","            y_pred: Predicted image.\n","            axis: Axes over which to sum the true negatives and actual negatives.\n","            smooth: Small value added to the denominator to avoid division by zero.\n","\n","        Returns:\n","            None\n","        \"\"\"\n","        global specificity_total\n","        y_true = np.squeeze(y_true, axis=-1).astype(np.float32)\n","        y_pred = y_pred.astype(np.float32)\n","        true_negatives = np.sum((1 - y_true) * (1 - y_pred), axis=axis)\n","        actual_negatives = np.sum(1 - y_true, axis=axis)\n","        specificity = true_negatives / (actual_negatives + smooth)\n","        specificity_total.append(specificity)\n","        \n","    def total_specificity(self):\n","        \"\"\"\n","        Calculates the total specificity for a set of images.\n","\n","        Parameters:\n","            None\n","\n","        Returns:\n","            value: The total specificity for the set of images.\n","        \"\"\"\n","        value = np.concatenate(specificity_total, axis=0)\n","        return np.mean(value)"]},{"cell_type":"markdown","metadata":{},"source":["### Model performance for the training part of the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-12T20:13:43.308805Z","iopub.status.busy":"2024-07-12T20:13:43.307889Z","iopub.status.idle":"2024-07-12T20:13:49.462366Z","shell.execute_reply":"2024-07-12T20:13:49.461335Z","shell.execute_reply.started":"2024-07-12T20:13:43.308770Z"},"trusted":true},"outputs":[],"source":["dice_total = []\n","jaccard_total = []\n","sensitivity_total = []\n","specificity_total = []\n","\n","train_dice = DiceMetric()\n","train_jaccard = JaccardMetric()\n","train_sensitivity = SensitivityMetric()\n","train_specificity = SpecificityMetric()\n","\n","for batch in range(0, 206, 5):\n","    \n","    original_images = load_chunk_from_hdf5('/kaggle/working/OxfordPet/images_train.h5', 'dataset', batch, batch+5)\n","    original_masks = load_chunk_from_hdf5('/kaggle/working/OxfordPet/masks_train.h5', 'dataset', batch, batch+5)\n","    input_tensor = ((torch.from_numpy(original_images)).permute(0, 3, 1, 2)).to(device)\n","    A_id_tensor = torch.zeros(5, 3).to(device)\n","    with torch.no_grad():\n","        output, cm = model(input_tensor, A_id_tensor)\n","    output = output.detach().cpu()\n","    cm = cm.detach().cpu()\n","    predicted_masks = output[:,1,:,:].numpy() > 0.5\n","    \n","    train_dice.calculate(original_masks, predicted_masks)\n","    train_jaccard.calculate(original_masks, predicted_masks)\n","    train_sensitivity.calculate(original_masks, predicted_masks)\n","    train_specificity.calculate(original_masks, predicted_masks)\n","    \n","    del input_tensor, A_id_tensor, output, cm\n","    torch.cuda.empty_cache()\n","    \n","print(f'Dice: {train_dice.total_dice()}, Jaccard: {train_jaccard.total_jaccard()}, Sensitivity: {train_sensitivity.total_sensitivity()}, Specificity: {train_specificity.total_specificity()}')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-12T20:12:51.682014Z","iopub.status.busy":"2024-07-12T20:12:51.681377Z","iopub.status.idle":"2024-07-12T20:12:51.708230Z","shell.execute_reply":"2024-07-12T20:12:51.707367Z","shell.execute_reply.started":"2024-07-12T20:12:51.681979Z"},"trusted":true},"outputs":[],"source":["plt.imsave('./image_example_train_1.png', predicted_masks[4], format='png')\n","display(Image(filename='/kaggle/working/image_example_train_1.png'))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-12T20:15:03.925760Z","iopub.status.busy":"2024-07-12T20:15:03.924840Z","iopub.status.idle":"2024-07-12T20:15:10.010247Z","shell.execute_reply":"2024-07-12T20:15:10.009324Z","shell.execute_reply.started":"2024-07-12T20:15:03.925724Z"},"trusted":true},"outputs":[],"source":["dice_total = []\n","jaccard_total = []\n","sensitivity_total = []\n","specificity_total = []\n","\n","val_dice = DiceMetric()\n","val_jaccard = JaccardMetric()\n","val_sensitivity = SensitivityMetric()\n","val_specificity = SpecificityMetric()\n","\n","for batch in range(0, 206, 5):\n","    \n","    original_images = load_chunk_from_hdf5('/kaggle/working/OxfordPet/images_val.h5', 'dataset', batch, batch+5)\n","    original_masks = load_chunk_from_hdf5('/kaggle/working/OxfordPet/masks_val.h5', 'dataset', batch, batch+5)\n","    input_tensor = ((torch.from_numpy(original_images)).permute(0, 3, 1, 2)).to(device)\n","    A_id_tensor = torch.zeros(5, 3).to(device)\n","    with torch.no_grad():\n","        output, cm = model(input_tensor, A_id_tensor)\n","    output = output.detach().cpu()\n","    cm = cm.detach().cpu()\n","    predicted_masks = output[:,1,:,:].numpy() > 0.5\n","    \n","    train_dice.calculate(original_masks, predicted_masks)\n","    train_jaccard.calculate(original_masks, predicted_masks)\n","    train_sensitivity.calculate(original_masks, predicted_masks)\n","    train_specificity.calculate(original_masks, predicted_masks)\n","    \n","    del input_tensor, A_id_tensor, output, cm\n","    torch.cuda.empty_cache()\n","    \n","print(f'Dice: {val_dice.total_dice()}, Jaccard: {val_jaccard.total_jaccard()}, Sensitivity: {val_sensitivity.total_sensitivity()}, Specificity: {val_specificity.total_specificity()}')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-12T20:26:13.775094Z","iopub.status.busy":"2024-07-12T20:26:13.774722Z","iopub.status.idle":"2024-07-12T20:26:19.910149Z","shell.execute_reply":"2024-07-12T20:26:19.909159Z","shell.execute_reply.started":"2024-07-12T20:26:13.775063Z"},"trusted":true},"outputs":[],"source":["dice_total = []\n","jaccard_total = []\n","sensitivity_total = []\n","specificity_total = []\n","\n","test_dice = DiceMetric()\n","test_jaccard = JaccardMetric()\n","test_sensitivity = SensitivityMetric()\n","test_specificity = SpecificityMetric()\n","\n","for batch in range(0, 206, 5):\n","    \n","    original_images = load_chunk_from_hdf5('/kaggle/working/OxfordPet/images_test.h5', 'dataset', batch, batch+5)\n","    original_masks = load_chunk_from_hdf5('/kaggle/working/OxfordPet/masks_test.h5', 'dataset', batch, batch+5)\n","    input_tensor = ((torch.from_numpy(original_images)).permute(0, 3, 1, 2)).to(device)\n","    A_id_tensor = torch.zeros(5, 3).to(device)\n","    with torch.no_grad():\n","        output, cm = model(input_tensor, A_id_tensor)\n","    output = output.detach().cpu()\n","    cm = cm.detach().cpu()\n","    predicted_masks = output[:,1,:,:].numpy() > 0.5\n","    \n","    train_dice.calculate(original_masks, predicted_masks)\n","    train_jaccard.calculate(original_masks, predicted_masks)\n","    train_sensitivity.calculate(original_masks, predicted_masks)\n","    train_specificity.calculate(original_masks, predicted_masks)\n","    \n","    del input_tensor, A_id_tensor, output, cm\n","    torch.cuda.empty_cache()\n","    \n","print(f'Dice: {test_dice.total_dice()}, Jaccard: {test_jaccard.total_jaccard()}, Sensitivity: {test_sensitivity.total_sensitivity()}, Specificity: {test_specificity.total_specificity()}')"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5373655,"sourceId":8932391,"sourceType":"datasetVersion"},{"datasetId":5301916,"sourceId":8940156,"sourceType":"datasetVersion"}],"dockerImageVersionId":30732,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
